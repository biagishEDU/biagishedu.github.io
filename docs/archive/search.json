[
  {
    "objectID": "projects/wGAN-stock-predictions/index.html",
    "href": "projects/wGAN-stock-predictions/index.html",
    "title": "Deep Learning Equity Trading Model",
    "section": "",
    "text": "This project seeks to rely on recent deep learning and technical analysis advancements to generate predictions of short term price movements for the Apple stock, $AAPL.\nPlease check out the links below for the full paper and the project website.\nFull Paper\nProject Website"
  },
  {
    "objectID": "projects/wGAN-stock-predictions/index.html#problem-description",
    "href": "projects/wGAN-stock-predictions/index.html#problem-description",
    "title": "Deep Learning Equity Trading Model",
    "section": "Problem Description",
    "text": "Problem Description\nWith the New York Stock Exchange containing a total global market capitalization of $15 trillion, it is no surprise that both retail and institutional investors have relied on every possible advancement in the digital world to generate stronger returns [3]. As such, to remain a competitive investor in such a highly technologically optimized market, it is crucial that new innovative approaches to investing are made. Ultimately, this has led to an open market today where 90% of all short term trades and 50-70% of all trades are completed by stocking trading algorithms [3]. Primarily, these computer generated trades rely on technical analysis, which avoid traditional fundamental methodologies, and instead rely on past trading activity, price changes of a security, and patterns in charts to develop valuable indicators for a given security’s future price movements.\nOur solution proposal seeks to rely on recent deep learning and technical analysis advancements to generate predictions of short term price movements for a specific equity holding. Our model will rely on an ensemble approach by combining analysisfrom a Wasserstein Generative Adversarial Network (wGAN) and the pretrained Valence Aware Dictionary for Sentiment Reasoning (VADER) model to generate our prediction process. The wGAN serves as a stable learning model during gradient descent, while avoiding convergence failures and mode collapse. This model will serve as a generator that will utilize two key data points for prediction, correlated assets and technical analysis. This model will serve in conjunction with VADER, which will offer sentiment analysis of discussion of the chosen security, determining whether posts are positive, neutral, or negative. Ultimately, we seek to rely on each of these models to offer an alternative to human based decisionmaking for short-term trading and instead create automated and successful predictions to stock price movements."
  },
  {
    "objectID": "projects/wGAN-stock-predictions/index.html#motivation",
    "href": "projects/wGAN-stock-predictions/index.html#motivation",
    "title": "Deep Learning Equity Trading Model",
    "section": "Motivation",
    "text": "Motivation\nAccurate forecasting of the stock market can be greatly beneficial for a variety of persons. Politicians and government officials would be able to leverage this forecasting to predict the future health of the economy. And with such predictions, they would be able to enact policy to counteract problematic trends, leading to more stable economic growth on a national level. Investors would be able to better profit from trades, generating wealth and more stable income from the stock market. Businesses and corporations could even use this forecasting information for more accurate quarterly performance reviews to assess projected success.\nThese are just a few of the possible benefits to motivate a project around improving current state-of-the-art stock market prediction models. This just touches the surface of what is possible here. With this data, anyone could be able to achieve a greater understanding of the behavior of the stock market, which up until this point has been one of the largest mysteries in finance."
  },
  {
    "objectID": "projects/wGAN-stock-predictions/index.html#challenges-and-solutions",
    "href": "projects/wGAN-stock-predictions/index.html#challenges-and-solutions",
    "title": "Deep Learning Equity Trading Model",
    "section": "Challenges and Solutions",
    "text": "Challenges and Solutions\nThe primary question is: how can the currently available methods of forecasting the stock market be improved?\nEnter artificial intelligence, and more specifically, machine learning (ML) models. With innate abilities to identify patterns, trends, and mostly unnoticeable correlations between data points, ML models are the perfect candidate for such tasks. Recurrent Neural Network (RNN), Long Short-Term Memory (LSTM) [6], Gated Recurrent Unit (GRU) and models have all been used to accurately predict time series data before, so why not extend the ideas to predicting stock market prices? Unfortunately, the stock market is extremely volatile, and as a result, it is quite difficult to forecast. Because of this, searches for more complicated models which accurately assess the vast and obscure trends and patterns of the stock market have heightened in recent years, leading to improvements in GAN models and similar for this use case specifically [5].\nAlthough we have seen improvements in many of these more complicated stock price prediction models, there always remains a lack of assessment of sentiment directly from the public. In the most basic sense, people are the primary cause of stock market movements. With a public mass movement towards selling a specific stock, there is likely a fall in a stock price to follow. Similarly, if more people are willing to purchase a stock than sell it, then an increase in the price is likely to follow. Supply and demand are primary factors in stock price movement. Researchers attempt solving this challenge by scraping news articles from official sources, but oftentimes, sentiment received from official sources differs substantially from the actual sentiment of the public.\nTo rectify these issues, this project proposes the use of a wGANGP (Gradient Penalty) model fed by analyses of public sentiment gained from social media platforms along with basic and technical indicators associated with a specific stock ($APPL in our case). With data from posts and comments from social media, sentiment can be assessed directly from personal accounts rather than official news outlets, hopefully bypassing media biases which could skew the predicted stock prices. As people more directly influence the stock market, there is definitely a perceived benefit in this approach. Regarding the use of a wGAN-GP model specifically, it has been chosen for its improvements upon a traditional GAN, noting its advantages in training stability and higher likelihood of convergence. This is directly beneficial when using more complex models for the generator and discriminator in the GAN."
  },
  {
    "objectID": "projects/wGAN-stock-predictions/index.html#literature-survey",
    "href": "projects/wGAN-stock-predictions/index.html#literature-survey",
    "title": "Deep Learning Equity Trading Model",
    "section": "Literature Survey",
    "text": "Literature Survey\nThe article by Boris Banushev, Using the Latest Advancements in Deep Learning to Predict Stock Price Movements [1], provided us great insight into how a GAN can be applied to make time series predictions on a stock. In his project, Boris uses an Long-Short Term Memory Recurrent Neural Network for his generator and a Convolutional Neural Network as his discriminator. To supplement his data, he uses various techniques like Fourier transforms, stacked autoencoders, and eigen portfolios to assemble the most information possible on the stock. Although we will not be using most of these techniques, he does conduct sentiment analysis by gathering news articles and inputting their text into a pretrained BERT model. To tune the hyperparameters of the GAN model, Boris uses reinforcement learning and Bayesian optimization.\nIn Generative Adversarial Network for Stock Market Price Prediction by Ricardo Romero [4], the project’s main goal was to see if various deep learning models could predict whether a stock would increase or decrease. A GAN model, an ARIMA model, an LSTM model, and a Deep LSTM model were all trained and tested. The GAN model had the second highest accuracy of 72.68%, which was slightly under the LSTM model’s highest accuracy of 74.16%. For analyzing the GAN’s performance, 20k, 30k, and 50k epoch models were made, with the 50k epochs model performing the best.\nThe paper authored by Labiad, Benabbou, and Berrado titled Improving Stock Market Intraday Prediction by Generative Adversarial Neural Networks [2] attempted to use a GAN to predict intraday stock prices. The initial dataset of intraday prices had noisy variations and inconsistent distance between consecutive points. To remedy this, the team discretized the data to have points spaced by 10 minute intervals, which led to an overall reduction in the number of observations. Additionally, mode-specific normalization was utilized to better capture the complex distribution within the data. As a result, the synthetic data generated has a much closer distribution to the real data, leading to overall better training of the discriminator and improved prediction."
  },
  {
    "objectID": "projects/wGAN-stock-predictions/index.html#limitations-of-existing-approaches",
    "href": "projects/wGAN-stock-predictions/index.html#limitations-of-existing-approaches",
    "title": "Deep Learning Equity Trading Model",
    "section": "Limitations of Existing Approaches",
    "text": "Limitations of Existing Approaches\nCurrently, three primary solutions exist for predictions of time series stock market prices. They are the PCA-LSTM Model, Traditional GAN Model, and the Traditional GAN Model with BERT. Each of these have a series of limitations that our proposed approach seeks to imrpove upon and ultimately resolve entirely. The PCA-LSTM Model currently relies on an extremely trivial LSTM neural network for predictions solely based on technical indicators. We see this as an extreme limitation as the model is extremely simple and has no inclusion of fundamental analysis. To improve upon this simplicity, the Traditional GAN Model was created; however, is still limited by its lack of fundamental analysis and is typically prone to convergence failures and training instability. Finally, this traditional model has attempted to incorporate fundamental analysis by relying on BERT to offer fundamental analysis provided from biased news outlets, without any sentiment directly from the public. As such, we require that our model improves on these shortcomings by developing resistance to training instability and convergence failures as well as incorporate public sentiment from retail investors directly."
  },
  {
    "objectID": "projects/wGAN-stock-predictions/index.html#problem-definition",
    "href": "projects/wGAN-stock-predictions/index.html#problem-definition",
    "title": "Deep Learning Equity Trading Model",
    "section": "Problem Definition",
    "text": "Problem Definition\nLet \\(F={f_1, f_2, \\cdots , f_n}\\) be the set of \\(n\\) features which will serve as input to the model. These features will include basic indicators (high, low, open, close, volume), sentiment analysis scores, and technical indicators (moving averages, relative strength index, etc.).\nLet \\(W={w_1, w_2, \\cdots , w_n}\\) be the set of \\(n\\) weights which will be updated while training the model. The magnitude of \\(n\\) will be determined by the feature engineering described in the following section.\nThe prediction equation will be as follows:\n\\(stk\\_price = F \\odot W\\)\nWhere \\(stk\\_price\\) is the predicted closing stock price outputted by the model and \\(\\odot\\) denotes the Hadamard Product (more commonly described as component-wise multiplication).\n\nProject Pipeline Diagram\n\nPictured above is the complete data pipeline for this project. The data processing and analyses are divided into two subcomponents, one detailing the fundamental analysis and the other detailing the technical analysis. The fundamental analysis includes evaluation of social media sentiment regarding the stock. Posts are scraped from social media outlets, processed via the NLTK Python library, and finally given sentiment scores by the VADER model. The scores for all posts within a day are then averaged. On the technical analysis side, basic indicators are first pulled from TDAmeritrade and used in various ways to compute a large set of technical indicators, including moving averages and similar. The expanded set of indicators will then be fed into an autoencoder model to produce a set of compressed features. After this process, we will have a condensed set of indicators to merge back with the original technical indicator set to feed into the prediction model. Finally, the expanded indicator set and the daily averaged sentiment scores will be merged and fed to the wGAN-GP prediction model. This model will output the predicted closing prices for the stock."
  },
  {
    "objectID": "projects/wGAN-stock-predictions/index.html#the-dataset",
    "href": "projects/wGAN-stock-predictions/index.html#the-dataset",
    "title": "Deep Learning Equity Trading Model",
    "section": "The Dataset",
    "text": "The Dataset\nWith regards to the fundamental analysis side of the pipeline, tweets mentioning “$AAPL” and “AAPL” are scraped from Twitter. For the technical analysis portion, basic financial data for $AAPL and a number of comparative assets are pulled via the TDAmeritrade API. All data within our dataset spans from 05/09/2013 to 03/31/2023. For purposes of tuning the prediction model, the dataset was split 80:10:10, for the training, validation, and testing sets respectively."
  },
  {
    "objectID": "projects/wGAN-stock-predictions/index.html#fundamental-analysis",
    "href": "projects/wGAN-stock-predictions/index.html#fundamental-analysis",
    "title": "Deep Learning Equity Trading Model",
    "section": "Fundamental Analysis",
    "text": "Fundamental Analysis\nFor sentiment analysis the text extracted from the social media webscraper must be cleaned so that such analysis can be conducted smoothly. Using the Python NLTK library, the text is tokenized and converted into lowercase. Stop-words (unimportant words), such as “the” and “is”, as well as any non-alphanumeric characters are stripped. Finally, the tokens are lemmatized.\nThe pre-processed text from the Twitter posts is then given a sentiment score by the VADER model. Sentiment scores for each day are averaged, returning a final set of daily averaged sentiments from Twitter regarding."
  },
  {
    "objectID": "projects/wGAN-stock-predictions/index.html#technical-analysis",
    "href": "projects/wGAN-stock-predictions/index.html#technical-analysis",
    "title": "Deep Learning Equity Trading Model",
    "section": "Technical Analysis",
    "text": "Technical Analysis\nIn this project, we will generate new features from the few basic indicators and score the averaged public sentiment score for each day. The basic indicators - high, low, open, close, and volume - will initially be utilized to compute various technical indicators. These technical indicators may include items such as the moving average, Bollinger Band, relative strength index, average directional index, moving average convergence divergence, and many more. Next, these features will be input into a variational autoencoder in order to produce a compressed representation of the input features, denoted as compressed features. The compressed features will be added to the original features to extend the feature space. The variational autoencoder will not be used for dimensionality reduction, as these compressed features will be served to the final model in addition to the original features. This is done in hopes that the compressed features will better highlight stock price movement patterns."
  },
  {
    "objectID": "projects/wGAN-stock-predictions/index.html#model-architecture-descriptions",
    "href": "projects/wGAN-stock-predictions/index.html#model-architecture-descriptions",
    "title": "Deep Learning Equity Trading Model",
    "section": "Model Architecture Descriptions",
    "text": "Model Architecture Descriptions\n\nSocial Media Webscraper\nA python script was be developed to scrape data from Twitter. The is script was developed using the Scweet Python library. It searches tweets that mention the keywords “$AAPL” or “AAPL” and collects up to 100 of them for each day. Afterwards, the sentiment of each tweet is determined using VADER. The average sentiment amongst all tweets per day is calculated to get the sentiment of one day.\n\n\nVADER for Sentiment Analysis\nVADER from the NLTK Python library will be used to classify our scraped media posts as negative or positive. The VADER model maps lexical features to sentiment scores via a dictionary. It is a rule-based sentiment analysis tool, which is explicitly sensitive to web-based media. Words are identified and marked as positive or negative, and these markings are utilized to compute a polarity score which identifies the overall sentiment of the message. Words with higher negative sentiment are mapped to negative scores of greater magnitude, and vice-versa. The same applies for words viewed as positive.\n\n\nwGAN-GP for Stock Price Prediction\nAn improvement upon a traditional wGAN, which enforces a gradient norm penalty in the discriminator in order to achieve Lipschitz continuity, known as wGAN-GP is used for the prediction of the closing price of a stock. wGANs improve upon traditional gans with the use of the Wasserstein distance as the loss function, promoting stability in model training.\nGANs are composed of two main components: the generator and the discriminator. The generator will be a traditional LSTM (long short-term memory) network with input units equal to the number of features in the final dataset and 512 hidden units. Finally, there will be one linear layer with a single output detailing the closing price for each day.\nThe discriminator is a CNN (convolutional neural network), chosen for its ability to extract complex patterns and trends from the dataset. The architecture is as follows, where \\(RWS\\) denotes the rolling-window size:\n\n1-Dimensional Convolutional Layer: \\(RWS+1 \\rightarrow 32\\)\n1-Dimensional Convolutional Layer: \\(32 \\rightarrow 64\\)\nLeakyReLU Layer\n1-Dimensional Convolutional Layer: \\(64 \\rightarrow 128\\)\nLeakyReLU Layer\n1-Dimensional Convolutional Layer: \\(128 \\rightarrow 256\\)\nLeakyReLU Layer\nLinear Layer: \\(256 \\rightarrow 256\\)\nLeakyReLU Layer\nLinear Layer: \\(256 \\rightarrow 256\\)\nActivation Layer\nLinear Layer: \\(256 \\rightarrow 1\\)\n\nThis model was tuned with a predefined grid search. Initially, a large set of possible parameters was tested on a small subset of the full dataset. For each incremental step afterwards, the ranges of the hyperparameters were decreased and tested with a larger subset of the dataset. In the final iteration, the model was evaluated on the full dataset."
  },
  {
    "objectID": "projects/wGAN-stock-predictions/index.html#dataset-exploration",
    "href": "projects/wGAN-stock-predictions/index.html#dataset-exploration",
    "title": "Deep Learning Equity Trading Model",
    "section": "Dataset Exploration",
    "text": "Dataset Exploration\n\nTechnical Analysis\nThe primary collection of technical data was derived from the TDAmeritrade API. With the Price History tool, TDAmeritrade would generate a JSON file of the price history for a given symbol over a specified period and frequency. For this project, we collected the daily price history over the last 10 years for our primary stock ($AAPL), comparative assets ($MSFT, $META, $AMZN, and $GOOGL), and an industry index ($SP500). This offered the Date, Open, Close, High, Low, and Volume for each security. From here, a series of technical indicators typically used by day and swing traders were calculated for $AAPL with the existing data. This included metrics such as: Stochastic Oscillators, Relative Strength Index, Simple Moving Averages for Close and Volume, and Moving Average Convergence/Divergence.\n\n\nFundamental Analysis\nThe fundamental analysis portion of this project was primarily based on analyzing the public sentiment of the Apple stock. The initial source of the data was Reddit. Using the PRAW Python library, a Reddit webscraper was built, which parsed the top 10 newest posts of the r/apple subreddit, and gathered the top 50 comments and up to 25 replies per comment. The text content of each post, comment, and reply were collected and then cleaned using the NLTK library. Their sentiment was determined using the VADER model. While this script was fully implemented, there was no way to gather data from previous dates, and therefore was not integrated within the pipeline. The second source of sentiment data was Twitter. Using the Scweet Python library, a script was written which collected up to 100 tweets per day and up to 10 replies per tweet. The only tweets gathered were those that had the keywords “$AAPL” or “AAPL”. The data was collected from January 1, 2013 to March 30, 2023. The same process was used to clean and calculate the sentiment of the tweet text content. Once the sentiment of each day was found, the final dataset was merged with the technical indicator dataset.\nSome preliminary visualizations were created to explore the sentiment dataset.\n\nIn general, the number of tweets that mention the keywrods has decreased over time. One possible explanation of this is the novelty of the stock has decreased, and the general public has placed its attention in other stocks and securities, such as cryptocurrency.\n\nOver our time frame, the sentiment of the stock has remained generally positive, staying above 0.0 for the grand majority of the time. It can be stated that Twitter users discussing the stock have an overal positive view of the company.\n\nFinally, we were curious to see if there was any correlation between the sentiment of the stock and the percent change in price of a given day. The scatterplot above illustrates that their is no relationship between the two, and can be said to be independent of each other."
  },
  {
    "objectID": "projects/wGAN-stock-predictions/index.html#prediction-results",
    "href": "projects/wGAN-stock-predictions/index.html#prediction-results",
    "title": "Deep Learning Equity Trading Model",
    "section": "Prediction Results",
    "text": "Prediction Results\n\nRolling-windows and Sentiment Analysis\nDepicted in the tables below are evaluation metrics of the returned prediction sets for running the wGAN-GP model with 3-day, 5-day, 7-day, and 10-day rolling windows. One table details the performance of the models with the inclusion of sentiment analysis as a feature, and the other without. The evaluation metrics included are: root-mean-square error (RMSE), normalized root-mean-square error (NRMSE), mean-absolute error (MAE), and mean-absolute-percentage error (MAPE). Note that the RMSE values are normalized to the range of the actual closing price range to gain the NRMSE values.\n\nEvaluation Metrics (without Sentiment Scores)\n\n\n\nRolling-Window Size\nRMSE\nNRMSE\nMAE\nMAPE\n\n\n\n\n3-day\n4.616\n0.092\n3.632\n0.024\n\n\n5-day\n5.064\n0.101\n3.991\n0.027\n\n\n7-day\n5.463\n0.109\n4.348\n0.029\n\n\n10-day\n5.841\n0.117\n4.611\n0.031\n\n\n\n\n\nEvaluation Metrics (with Sentiment Scores)\n\n\n\nRolling-Window Size\nRMSE\nNRMSE\nMAE\nMAPE\n\n\n\n\n3-day\n4.401\n0.088\n3.447\n0.023\n\n\n5-day\n4.754\n0.095\n3.743\n0.025\n\n\n7-day\n5.271\n0.105\n4.233\n0.028\n\n\n10-day\n5.411\n0.108\n4.329\n0.029\n\n\n\nFrom an initial evaluation, it appears that the inclusion of daily sentiment score averages as a feature to the prediction model resulted in a slight increase in the accuracy of the predicted closing price. With some more calculations, we can quantify this increased accuracy as a percentage, seen in the table below.\n\n\nPercentage Change in Evaluation Metrics\n\n\n\nRolling-Window Size\nRMSE\nNRMSE\nMAE\nMAPE\n\n\n\n\n3-day\n-4.668\n-4.668\n-5.09\n-4.763\n\n\n5-day\n-6.117\n-6.117\n-6.205\n-6.741\n\n\n7-day\n-3.511\n-3.511\n-2.633\n-2.400\n\n\n10-day\n-7.366\n-7.366\n-6.112\n-5.627\n\n\n\nSo, with some final averaging, this gives us an average percentage change of −5.415% for RMSE and NRMSE, −5.010% for MAE, and −4.882% for MAPE when including sentiment scores as a feature in the prediction model.\nConclusively, we see around a 5% more accurate prediction when sentiment analysis is included in the prediction process. This follows with our initial assumptions that public sentiment is beneficial to accurately predicting the stock market’s daily movements. Though it is only a slight increase, it is believed that with better scraping and data-cleaning capabilities, the model could better harness public sentiment for even more accurate predictions. For instance, removing or filtering out more insignificant or misdirecting media posts could better gauge public sentiment for these purposes, likely leading to increased accuracy in the prediction model.\n\n\n\nBest Predictions\nThe best prediction set returned by the model was with a 3-day sliding window, including sentiment scored from Twitter. The model takes 3 days of basic indicators as input and outputs the predicted closing price of the 4th day. After tuning, the following hyperparameters were chosen for their performance on the validation set:\n\nBatch Size = 65\nLearning Rate = 0.00005\nCritic Training Iterations per Generator Iteration = 5\nEpochs = 150\n\nAdditionally, RMSProp, or root-mean-square propagation, was used as the optimization algorithm for the model. Visualizations of the generator and discriminator losses follow.\n\n\nThe discriminator loss appears to converge approximately to 0, while the generator loss appears to converge approximately to 5. These exact values are mostly unimportant. However, it is important to note that, for this dataset, the losses do appear to converge. In prior versions of the model, the losses did not ever converge. This improvement is thought to be due to the implementation of the gradient penalty, in order to enforce the Lipschitz contraint in the discriminator. The usage of the wGAN-GP model’s improvements to training stability and convergence is likely to credit for this result. Evaluation of the model’s stock price predictions are detailed below.\n\n\nFor the training dataset, the model returned predictions with an RMSE of 0.779. The model seems capable of identifying the movement of the stock price and accurately predicting the magnitude of said movement. The inclusion of various technical indicators and sentiment analysis scores improved this greatly from previous iterations of the model.\nRegarding the validation and testing datasets, the model returned predictions with an averaged RMSE of 4.401 or 0.088 when normalized to the range of the true closing price values. This is a bit higher than that on the training set, indicating a bit of overfitting in the model. Further tuning may be requiring to hone in on a bettergeneralized model. Despite this, the model still performs well in identifying stock price movements and even comes fairly close to the magnitude of the movement, returning an averaged MAE of 3.447 on these sets.\nTaking in the scale of the predicted values, the RMSE appears quite good. Conclusively, the model achieved predictions within an error of about $3.50 or 2.3% off of the actual stock price on average. Although not entirely useful for day trading, this model could prove useful in longer-termed swing trades."
  },
  {
    "objectID": "projects/regression-randomForest/index.html",
    "href": "projects/regression-randomForest/index.html",
    "title": "Housing Price Forecasting with the Help of Random Forests",
    "section": "",
    "text": "Housing prices vary significantly year after year and continue to do so throughout multiple generations. Housing prices are notoriously hard to understand. Being affected by a multitude of variables including location, architecture, and even being affected by the national housing market, it is easy to see how complex data analytics in this field can be. Thankfully, continual improvements in the field of machine learning, we are more equipped than ever to tackle many of the complex problems which we have been faced with for many years.\nThroughout this project, we will embark through on a journey through the dynamic realm of the California housing market. Armed with the tools granted to us by machine learning, we will navigate the intricacies of the California Housing dataset and unveil the potential of Random Forests in predicting median housing prices. We will walk through and end-to-end machine learning project, detailing steps from data exploration and preprocessing to the practical application of Random Forests in a regression task.\n\nThe Data\nUsually in these types of projects, I would begin with all the imports we will need for the entire project. However, today, let’s take a bit of a detour from our usual route and import the packages as we need them. Hopefully, this will let the post seem more linear, and you (the reader) will not have to jump around anywhere to see packages which are required for a specific step in our project.\nTo begin, we will load and take a look at the data we will be using. The data we will be using today is the California Housing dataset (available from sklearn). This dataset is commonly used for testing out regression models. Let’s start by loading it in from the sklearn package. We will set as_frame to True when loading the dataset, so that the data is returned as a pandas dataframe.\n\n# Import\nfrom sklearn.datasets import fetch_california_housing\nimport pandas as pd\n\n# fetch the dataset and store it as a frame\ncali_housing = fetch_california_housing(as_frame = True)\nprint(type(cali_housing))\n\n&lt;class 'sklearn.utils._bunch.Bunch'&gt;\n\n\nThe fetch_california_housing() function returns an sklearn bunch object which has some nice properties which we can take a look at. In the properties, we have a short description of the dataset we can take a look at. Let’s now print out the description and see what is going on.\n\n# Print dataset description\nprint(cali_housing.DESCR)\n\n.. _california_housing_dataset:\n\nCalifornia Housing dataset\n--------------------------\n\n**Data Set Characteristics:**\n\n    :Number of Instances: 20640\n\n    :Number of Attributes: 8 numeric, predictive attributes and the target\n\n    :Attribute Information:\n        - MedInc        median income in block group\n        - HouseAge      median house age in block group\n        - AveRooms      average number of rooms per household\n        - AveBedrms     average number of bedrooms per household\n        - Population    block group population\n        - AveOccup      average number of household members\n        - Latitude      block group latitude\n        - Longitude     block group longitude\n\n    :Missing Attribute Values: None\n\nThis dataset was obtained from the StatLib repository.\nhttps://www.dcc.fc.up.pt/~ltorgo/Regression/cal_housing.html\n\nThe target variable is the median house value for California districts,\nexpressed in hundreds of thousands of dollars ($100,000).\n\nThis dataset was derived from the 1990 U.S. census, using one row per census\nblock group. A block group is the smallest geographical unit for which the U.S.\nCensus Bureau publishes sample data (a block group typically has a population\nof 600 to 3,000 people).\n\nA household is a group of people residing within a home. Since the average\nnumber of rooms and bedrooms in this dataset are provided per household, these\ncolumns may take surprisingly large values for block groups with few households\nand many empty houses, such as vacation resorts.\n\nIt can be downloaded/loaded using the\n:func:`sklearn.datasets.fetch_california_housing` function.\n\n.. topic:: References\n\n    - Pace, R. Kelley and Ronald Barry, Sparse Spatial Autoregressions,\n      Statistics and Probability Letters, 33 (1997) 291-297\n\n\n\nGreat! We get some dataset characteristics and even a reference for the dataset.\nThis dataset was originally derived from the 1990 U.S. census, with each row detailing the data for a census block group. The dataset contains 20640 total samples, with each sample having 8 numeric attributes (or features). The features detail information such as the median income in a block group or the average number of bedrooms per household. Also, note that, our target variable here is the median house value for California districts. This value is expressed as hundreds of thousands of dollars. So, 3 here would mean $300,000. Since our target variable is continuous, this means that our machine learning task here is regression. Finally, also note that there are no missing values in this dataset (as written in the description). So, we will not have to deal with missing values!\nNow that we know a bit about the dataset, lets take a look at the actual data. The whole dataset can be accessed from our bunch with the .frame property.\n\n# Show full dataframe\ncali_housing.frame\n\n\n\n\n\n\n\n\nMedInc\nHouseAge\nAveRooms\nAveBedrms\nPopulation\nAveOccup\nLatitude\nLongitude\nMedHouseVal\n\n\n\n\n0\n8.3252\n41.0\n6.984127\n1.023810\n322.0\n2.555556\n37.88\n-122.23\n4.526\n\n\n1\n8.3014\n21.0\n6.238137\n0.971880\n2401.0\n2.109842\n37.86\n-122.22\n3.585\n\n\n2\n7.2574\n52.0\n8.288136\n1.073446\n496.0\n2.802260\n37.85\n-122.24\n3.521\n\n\n3\n5.6431\n52.0\n5.817352\n1.073059\n558.0\n2.547945\n37.85\n-122.25\n3.413\n\n\n4\n3.8462\n52.0\n6.281853\n1.081081\n565.0\n2.181467\n37.85\n-122.25\n3.422\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n20635\n1.5603\n25.0\n5.045455\n1.133333\n845.0\n2.560606\n39.48\n-121.09\n0.781\n\n\n20636\n2.5568\n18.0\n6.114035\n1.315789\n356.0\n3.122807\n39.49\n-121.21\n0.771\n\n\n20637\n1.7000\n17.0\n5.205543\n1.120092\n1007.0\n2.325635\n39.43\n-121.22\n0.923\n\n\n20638\n1.8672\n18.0\n5.329513\n1.171920\n741.0\n2.123209\n39.43\n-121.32\n0.847\n\n\n20639\n2.3886\n16.0\n5.254717\n1.162264\n1387.0\n2.616981\n39.37\n-121.24\n0.894\n\n\n\n\n20640 rows × 9 columns\n\n\n\nWe can now get our features with .data and the target variable with .target. We will store the features as X and the target as y. Also, using .values on each of these will give us a workable numpy array.\n\n# Get features (X) and target (y)\nX = cali_housing.data.values\ny = cali_housing.target.values\n\nSo, that pretty much concludes the amount of work required to process our data. Since there are no missing values, there is no work to be done there. Additionally, since we will be making use of a random forest model, no scaling is necessary. In fact, scaling will not really change anything at all (maybe a slightly faster convergence rate, but that’s about it).\n\n\nSome Exploration\nWith the processing done, we can move into some data exploration. When you go to purchase any home, one of the most important factors to consider is its location. And, in fact, location is usually a large factor in determining the price of the home as well. Thus, it follows that, we can start to assume that the longitude and latitude of a group of homes in California may be an important factor to consider when predicting its price.\nBut, this is just an assumption. To check our assumption, we can do a simple scatterplot of our samples using the longitude and latitude attributes. For this, we will first need to import seaborn and matplotlib to make our visualizations. Here, I will also be overlaying the scatterplot on top of a simple outline map of California. This will give us some idea of location. Additionally, when making our seaborn scatterplot, we can change the size and hue of the points according to our target variable (MedHouseValue) to see if the longitude and latitude does actually affect the variable we are trying to predict. The code cell below creates this visualization.\n\n# Imports\nimport seaborn as sns\nimport matplotlib.pyplot as plt\n\n# Set sns styling\nsns.set(style=\"dark\",\n        font_scale=1.5)\n\n# Read map png\nimg = plt.imread('./data/cali_map.png')\n\n# Create figure\nplt.figure(figsize=(10, 8), dpi=300)\n\n# Show map png\nplt.imshow(img,\n           extent=[min(cali_housing.data['Longitude']),\n                   max(cali_housing.data['Longitude']),\n                   min(cali_housing.data['Latitude']),\n                   max(cali_housing.data['Latitude'])],\n           aspect='auto',\n           cmap='gray')\n\n# Plot longitude and latitude, with size and hue by house value\nsns.scatterplot(\n    data=cali_housing.frame,\n    x=\"Longitude\",\n    y=\"Latitude\",\n    size=\"MedHouseVal\",\n    hue=\"MedHouseVal\",\n    palette=\"viridis\",\n    alpha=0.5\n)\n\n# Set legend and title\nplt.legend(title=\"MedHouseVal\",\n           bbox_to_anchor=(1.02, 1),\n           loc=\"upper left\")\nplt.title(\"Median House Value By Location in California\")\n\n# Set outside border\nax = plt.gca()\nax.spines['left'].set_color('black')\nax.spines['left'].set_linewidth(1)\nax.spines['bottom'].set_color('black')\nax.spines['bottom'].set_linewidth(1)\nax.spines['right'].set_color('black')\nax.spines['right'].set_linewidth(1)\nax.spines['top'].set_color('black')\nax.spines['top'].set_linewidth(1)\nplt.show()\n\n\n\n\nTaking a look at our visualization, we can confirm our assumption from before. For example, most of the highest valued homes are placed by the coast. If you know anything about the California housing market, this will be no surprise to you. Also, much of the inland empire has a lower median house value. Thus, location is a pretty good predictor of house value. Different median house values are clustered into specific locations around the state of California.\nSo, it appears that location (a combination of both longitude and latitude) is pretty heavily correlated with median house value. Let’s now expand our view a bit and start to look at all the other features in our dataset. What other features are also highly correlated with median house value? To answer this, we can make use of a traditional correlation matrix.\nA correlation matrix shows the correlation coefficients (in our case, using pearson correlation) between many variables. Each cell in the matrix represents the correlation between two variables. Here, correlation is a statistical measure which indicates the extent to which two variables change together. In other words, it quantifies the strength and direction of a relationship between the two variables. A correlation positive correlation coefficient means that when one variable increases, so does the other. On the other hand, a negative correlation coefficient means that as one variable increases, the other tends to decrease. Finally, a correlation close to zero indicates a weak (or not any) correlation between the two variables.\nIn order to visualize a correlation matrix, we will first have to compute the correlation coefficients between every pair variables in our original dataset. Thankfully, computing a correlation matrix is very easy to do with pandas. We will use pandas’ corr() function on our full dataset to do this.\n\n# Compute correlation matrix\ncorr_mat = cali_housing.frame.corr()\ncorr_mat\n\n\n\n\n\n\n\n\nMedInc\nHouseAge\nAveRooms\nAveBedrms\nPopulation\nAveOccup\nLatitude\nLongitude\nMedHouseVal\n\n\n\n\nMedInc\n1.000000\n-0.119034\n0.326895\n-0.062040\n0.004834\n0.018766\n-0.079809\n-0.015176\n0.688075\n\n\nHouseAge\n-0.119034\n1.000000\n-0.153277\n-0.077747\n-0.296244\n0.013191\n0.011173\n-0.108197\n0.105623\n\n\nAveRooms\n0.326895\n-0.153277\n1.000000\n0.847621\n-0.072213\n-0.004852\n0.106389\n-0.027540\n0.151948\n\n\nAveBedrms\n-0.062040\n-0.077747\n0.847621\n1.000000\n-0.066197\n-0.006181\n0.069721\n0.013344\n-0.046701\n\n\nPopulation\n0.004834\n-0.296244\n-0.072213\n-0.066197\n1.000000\n0.069863\n-0.108785\n0.099773\n-0.024650\n\n\nAveOccup\n0.018766\n0.013191\n-0.004852\n-0.006181\n0.069863\n1.000000\n0.002366\n0.002476\n-0.023737\n\n\nLatitude\n-0.079809\n0.011173\n0.106389\n0.069721\n-0.108785\n0.002366\n1.000000\n-0.924664\n-0.144160\n\n\nLongitude\n-0.015176\n-0.108197\n-0.027540\n0.013344\n0.099773\n0.002476\n-0.924664\n1.000000\n-0.045967\n\n\nMedHouseVal\n0.688075\n0.105623\n0.151948\n-0.046701\n-0.024650\n-0.023737\n-0.144160\n-0.045967\n1.000000\n\n\n\n\n\n\n\nTaking a look at the matrix, we can see that every cell in the diagonal has a value of 1. This is perfectly expected, as every variable will have a perfect correlation with itself. Additionally, note that cells on opposing sides of the diagonal contain the same values. For instance, the value in cell (MedInc, HouseAge) is the same as the value in cell (HouseAge, MedInc). This is simply due to how the matrix is generated. So, we can easily get away with only visualizing the lower triangle of the matrix, as the upper triangle is exactly the same.\nTo visualize the plot, we will again make use of matplotlib as well as seaborn. First, however, we will have to generate a mask for the upper triangle so that we do not show it and make the visualization more confusing than it has to be. This will also get rid of our meaningless diagonal full of 1s. For visualizing the matrix, we will use a heatmap with a diverging color palette. The color palette is best as a diverging one because this will make it easy to pick out positive versus negative correlations in our visualization. We will be using a built-in diverging color palette from seaborn called ‘vlag_r’. This way, positive correlations will be blue with the negatives being red. No correlation cells will be white. The following code cell creates this visualization.\n\n# Imports\nimport numpy as np\nimport seaborn as sns\n\n# Set sns styling\nsns.set(style=\"white\",\n        font_scale=1.5)\n\n# Create a mask for the upper triangle\ntr_mask = np.triu(np.ones_like(corr_mat, dtype = bool))\n\n# Create figure\nplt.subplots(figsize=(10, 8), dpi=300)\n\n# Create viz for corr matrix\nsns.heatmap(corr_mat,          # Correlation Matrix\n            mask = tr_mask,    # Mask\n            cmap = 'vlag_r',   # Color Palette\n            center = 0,        # Center color palette to 0 value\n            square = True,     # Set cells to be squares\n            linewidths = 1)  # Set line widths\n\n# Set title\nplt.title(\"Correlation Matrix on California Housing Dataset Variables\")\nplt.show()\n\n\n\n\nNow, you might be confused as to why latitude and longitude do not show heavy correlations with the median house value after we had that whole section above this. Well, in reality, any change in the actual values for longitude and latitude themselves may not be related to changes in the median house values. In the visualization before, we saw that very specific values for longitude and latitude (those related to coastal areas) were indicative of high house prices. Despite this, increasing latitude or longitude alone, does not directly change the housing prices. In short, this correlation matrix does not tell us the whole story. This is why we are doing it along with the other visualization.. so we can try to see the larger picture. Note that, this does not mean that the latitude and longitude are not important features in predicting median house incomes. Rather, this just tells us that the actual values for these features are not highly correlated with the value for MedHouseVal.\nOn another note, one thing we can take away from this visualization is that median income has a high positive correlation with median house value. Thus, as median income increases, so does the median house value. So, we can takeaway the fact that MedInc may be another good predictor of median house value (along with the longitude and latitude from the analysis we did before).\n\n\nApplying a Random Forest Regressor\nYes, I know this section is titled “Applying a Random Forest Regressor”, but before that, we should at least know what a random forest model is. A random forest is an ensemble model containing many decision trees. It works by constructing multiple decision trees during training and outputs the average prediction from these trees as its prediction. The ensemble approach helps overcome some over-fitting problems, as it uses not just one model to make its predictions. A decision tree, for those who don’t know, if like a flowchart, where decisions are made by recursively splitting the data into subsets based on a set of inferred rules, ultimately leading to a predicted outcome. A simple decision tree for the XOR function is below:\n\nWith that out of the way, let’s talk about hyperparameters. A couple of important hyperparameters for a random forest model are: - Number of trees in the ensemble - Number of features to consider when looking for the best split - Maximum depth of a tree\nAlong with these there are a multitude of other hyperparameters which you could change to alter the behavior of the Random Forest Model. Changing these may or may not change the model’s performance, but for sake of brevity in this post we will only try and tune the 3 parameters above.\nThere are a ton of ways to tune hyperparameters (Bayesian optimization, random search, etc.), but today we will be working with a grid search. Grid search involves defining a grid of hyperparameter values to explore, and then training and evaluating a model with these hyperparameters. This is an exhaustive search over the hyperparameter space that we manually define. We will be using grid search cross validation (GridSearchCV from sklearn) for our purposes. The CV part means that the models will be evaluated with cross-validation. With cross validation, the original dataset is divided into multiple subsets (or folds) and the model is trained and evaluated multiple times using different folds for training and testing. This helps evaluations by reducing the impact of variations due to data splitting.\nTo start off, our first step is to split our original data into train and test sets. For this, we will make use of sklearn’s train_test_split() method. We will use 1/4 of our original data for final evaluations. This splitting will ensure that no data leakage has occurred and our model will be properly evaluated on unseen data. Random state is set to make this project reproducible.\n\n# Import\nfrom sklearn.model_selection import train_test_split\n\n# Split data into train and test sets\nX_train, X_test, y_train, y_test = train_test_split(X,\n                                                    y,\n                                                    test_size = 1/4,\n                                                    random_state = 42)\n\nAfter this, let’s define our hyperparameter search space. For the number of trees in our ensemble, we can set up a simple geometric series and simply doubling the number each time from 100 to 400. Regarding the number of features to consider when looking for the best split, our maximum is the number of features in our dataset (8). So, let’s define an array from 4 to 8 by 2 for this. Finally, for bounding the maximum depth of a decision tree, we can set the minimum number of samples required to split an internal node. The default is 2, so we will set up list from 2 to 4. Keeping our search space small will let shorten the search time significantly. The downside to this is that may not find the optimal parameters, just the optimal ones for our search space. If you are doing this for a project, I suggest trying to keep the search space as much as possible, but still ensuring that you are nearby the optimal parameters. The code below sets up the parameter grid, for the search space we just decided on.\n\n# Define hyperparameter space\nparams = {'n_estimators' : [100, 200, 400],\n          'max_features' : [4, 6, 8],\n          'min_samples_split' : [2, 3, 4]}\n\nWith the parameter space now defined, we can run our grid search. For evaluating our predictions, we will use the r2 score. Otherwise known as the coefficient of determination, this score is a measure of how well the regression line approximates the actual data. It is the proportion of the variation in the dependent variable which is predictable from the independent variables. In other words, it evaluates the scatter of the data points around the regression line. The best possible score is 1. We will use 4-fold cross validation here. After we create the grid search object, we will run the fit() function to run the grid search. Please note that running the grid search may take some time. In the meantime, check this out 🐕.\nHere, we will have to import the RandomForestRegressor and GridSearchCV from sklearn.\n\n# Imports\nfrom sklearn.model_selection import GridSearchCV\nfrom sklearn.ensemble import RandomForestRegressor\n\n# Set up grid search\ngrid_search = GridSearchCV(estimator = RandomForestRegressor(),\n                           param_grid = params,\n                           verbose = 3,\n                           cv = 4,\n                           scoring = 'r2')\n\n# Run grid search\ngrid_search.fit(X_train, y_train)\n\nFitting 4 folds for each of 27 candidates, totalling 108 fits\n[CV 1/4] END max_features=4, min_samples_split=2, n_estimators=100;, score=0.811 total time=  13.5s\n[CV 2/4] END max_features=4, min_samples_split=2, n_estimators=100;, score=0.802 total time=  13.2s\n[CV 3/4] END max_features=4, min_samples_split=2, n_estimators=100;, score=0.815 total time=  13.2s\n[CV 4/4] END max_features=4, min_samples_split=2, n_estimators=100;, score=0.809 total time=  13.1s\n[CV 1/4] END max_features=4, min_samples_split=2, n_estimators=200;, score=0.811 total time=  26.9s\n[CV 2/4] END max_features=4, min_samples_split=2, n_estimators=200;, score=0.803 total time=  26.4s\n[CV 3/4] END max_features=4, min_samples_split=2, n_estimators=200;, score=0.816 total time=  26.4s\n[CV 4/4] END max_features=4, min_samples_split=2, n_estimators=200;, score=0.808 total time=  26.9s\n[CV 1/4] END max_features=4, min_samples_split=2, n_estimators=400;, score=0.813 total time=  59.9s\n[CV 2/4] END max_features=4, min_samples_split=2, n_estimators=400;, score=0.804 total time=  53.5s\n[CV 3/4] END max_features=4, min_samples_split=2, n_estimators=400;, score=0.817 total time=  53.0s\n[CV 4/4] END max_features=4, min_samples_split=2, n_estimators=400;, score=0.811 total time=  53.4s\n[CV 1/4] END max_features=4, min_samples_split=3, n_estimators=100;, score=0.809 total time=  12.2s\n[CV 2/4] END max_features=4, min_samples_split=3, n_estimators=100;, score=0.803 total time=  12.3s\n[CV 3/4] END max_features=4, min_samples_split=3, n_estimators=100;, score=0.813 total time=  12.4s\n[CV 4/4] END max_features=4, min_samples_split=3, n_estimators=100;, score=0.807 total time=  12.2s\n[CV 1/4] END max_features=4, min_samples_split=3, n_estimators=200;, score=0.812 total time=  24.5s\n[CV 2/4] END max_features=4, min_samples_split=3, n_estimators=200;, score=0.803 total time=  24.8s\n[CV 3/4] END max_features=4, min_samples_split=3, n_estimators=200;, score=0.817 total time=  24.4s\n[CV 4/4] END max_features=4, min_samples_split=3, n_estimators=200;, score=0.809 total time=  24.4s\n[CV 1/4] END max_features=4, min_samples_split=3, n_estimators=400;, score=0.815 total time=  49.4s\n[CV 2/4] END max_features=4, min_samples_split=3, n_estimators=400;, score=0.805 total time=  49.3s\n[CV 3/4] END max_features=4, min_samples_split=3, n_estimators=400;, score=0.816 total time=  49.5s\n[CV 4/4] END max_features=4, min_samples_split=3, n_estimators=400;, score=0.808 total time=  49.0s\n[CV 1/4] END max_features=4, min_samples_split=4, n_estimators=100;, score=0.810 total time=  11.9s\n[CV 2/4] END max_features=4, min_samples_split=4, n_estimators=100;, score=0.803 total time=  11.5s\n[CV 3/4] END max_features=4, min_samples_split=4, n_estimators=100;, score=0.815 total time=  11.6s\n[CV 4/4] END max_features=4, min_samples_split=4, n_estimators=100;, score=0.805 total time=  11.6s\n[CV 1/4] END max_features=4, min_samples_split=4, n_estimators=200;, score=0.813 total time=  23.3s\n[CV 2/4] END max_features=4, min_samples_split=4, n_estimators=200;, score=0.803 total time=  23.7s\n[CV 3/4] END max_features=4, min_samples_split=4, n_estimators=200;, score=0.816 total time=  23.2s\n[CV 4/4] END max_features=4, min_samples_split=4, n_estimators=200;, score=0.809 total time=  23.5s\n[CV 1/4] END max_features=4, min_samples_split=4, n_estimators=400;, score=0.812 total time=  47.0s\n[CV 2/4] END max_features=4, min_samples_split=4, n_estimators=400;, score=0.803 total time=  47.2s\n[CV 3/4] END max_features=4, min_samples_split=4, n_estimators=400;, score=0.817 total time=  47.1s\n[CV 4/4] END max_features=4, min_samples_split=4, n_estimators=400;, score=0.810 total time=  46.5s\n[CV 1/4] END max_features=6, min_samples_split=2, n_estimators=100;, score=0.808 total time=  19.4s\n[CV 2/4] END max_features=6, min_samples_split=2, n_estimators=100;, score=0.798 total time=  18.8s\n[CV 3/4] END max_features=6, min_samples_split=2, n_estimators=100;, score=0.808 total time=  18.9s\n[CV 4/4] END max_features=6, min_samples_split=2, n_estimators=100;, score=0.802 total time=  19.2s\n[CV 1/4] END max_features=6, min_samples_split=2, n_estimators=200;, score=0.809 total time=  38.0s\n[CV 2/4] END max_features=6, min_samples_split=2, n_estimators=200;, score=0.797 total time=  38.3s\n[CV 3/4] END max_features=6, min_samples_split=2, n_estimators=200;, score=0.809 total time=  37.9s\n[CV 4/4] END max_features=6, min_samples_split=2, n_estimators=200;, score=0.804 total time=  38.3s\n[CV 1/4] END max_features=6, min_samples_split=2, n_estimators=400;, score=0.807 total time= 1.3min\n[CV 2/4] END max_features=6, min_samples_split=2, n_estimators=400;, score=0.798 total time= 1.3min\n[CV 3/4] END max_features=6, min_samples_split=2, n_estimators=400;, score=0.812 total time= 1.3min\n[CV 4/4] END max_features=6, min_samples_split=2, n_estimators=400;, score=0.805 total time= 1.3min\n[CV 1/4] END max_features=6, min_samples_split=3, n_estimators=100;, score=0.805 total time=  18.1s\n[CV 2/4] END max_features=6, min_samples_split=3, n_estimators=100;, score=0.796 total time=  17.6s\n[CV 3/4] END max_features=6, min_samples_split=3, n_estimators=100;, score=0.809 total time=  17.8s\n[CV 4/4] END max_features=6, min_samples_split=3, n_estimators=100;, score=0.802 total time=  18.2s\n[CV 1/4] END max_features=6, min_samples_split=3, n_estimators=200;, score=0.806 total time=  35.6s\n[CV 2/4] END max_features=6, min_samples_split=3, n_estimators=200;, score=0.797 total time=  35.9s\n[CV 3/4] END max_features=6, min_samples_split=3, n_estimators=200;, score=0.811 total time=  35.4s\n[CV 4/4] END max_features=6, min_samples_split=3, n_estimators=200;, score=0.803 total time=  35.9s\n[CV 1/4] END max_features=6, min_samples_split=3, n_estimators=400;, score=0.809 total time= 1.2min\n[CV 2/4] END max_features=6, min_samples_split=3, n_estimators=400;, score=0.797 total time= 1.2min\n[CV 3/4] END max_features=6, min_samples_split=3, n_estimators=400;, score=0.811 total time= 1.2min\n[CV 4/4] END max_features=6, min_samples_split=3, n_estimators=400;, score=0.805 total time= 1.2min\n[CV 1/4] END max_features=6, min_samples_split=4, n_estimators=100;, score=0.807 total time=  16.9s\n[CV 2/4] END max_features=6, min_samples_split=4, n_estimators=100;, score=0.796 total time=  17.2s\n[CV 3/4] END max_features=6, min_samples_split=4, n_estimators=100;, score=0.808 total time=  17.0s\n[CV 4/4] END max_features=6, min_samples_split=4, n_estimators=100;, score=0.801 total time=  16.8s\n[CV 1/4] END max_features=6, min_samples_split=4, n_estimators=200;, score=0.808 total time=  34.5s\n[CV 2/4] END max_features=6, min_samples_split=4, n_estimators=200;, score=0.799 total time=  33.8s\n[CV 3/4] END max_features=6, min_samples_split=4, n_estimators=200;, score=0.809 total time=  34.4s\n[CV 4/4] END max_features=6, min_samples_split=4, n_estimators=200;, score=0.803 total time=  33.9s\n[CV 1/4] END max_features=6, min_samples_split=4, n_estimators=400;, score=0.809 total time= 1.1min\n[CV 2/4] END max_features=6, min_samples_split=4, n_estimators=400;, score=0.797 total time= 1.1min\n[CV 3/4] END max_features=6, min_samples_split=4, n_estimators=400;, score=0.811 total time= 1.2min\n[CV 4/4] END max_features=6, min_samples_split=4, n_estimators=400;, score=0.804 total time= 1.1min\n[CV 1/4] END max_features=8, min_samples_split=2, n_estimators=100;, score=0.805 total time=  25.1s\n[CV 2/4] END max_features=8, min_samples_split=2, n_estimators=100;, score=0.793 total time=  24.7s\n[CV 3/4] END max_features=8, min_samples_split=2, n_estimators=100;, score=0.807 total time=  25.0s\n[CV 4/4] END max_features=8, min_samples_split=2, n_estimators=100;, score=0.798 total time=  24.7s\n[CV 1/4] END max_features=8, min_samples_split=2, n_estimators=200;, score=0.806 total time=  50.0s\n[CV 2/4] END max_features=8, min_samples_split=2, n_estimators=200;, score=0.794 total time=  49.7s\n[CV 3/4] END max_features=8, min_samples_split=2, n_estimators=200;, score=0.806 total time=  50.0s\n[CV 4/4] END max_features=8, min_samples_split=2, n_estimators=200;, score=0.800 total time=  49.9s\n[CV 1/4] END max_features=8, min_samples_split=2, n_estimators=400;, score=0.805 total time= 1.7min\n[CV 2/4] END max_features=8, min_samples_split=2, n_estimators=400;, score=0.793 total time= 1.7min\n[CV 3/4] END max_features=8, min_samples_split=2, n_estimators=400;, score=0.808 total time= 1.7min\n[CV 4/4] END max_features=8, min_samples_split=2, n_estimators=400;, score=0.799 total time= 1.7min\n[CV 1/4] END max_features=8, min_samples_split=3, n_estimators=100;, score=0.801 total time=  23.2s\n[CV 2/4] END max_features=8, min_samples_split=3, n_estimators=100;, score=0.790 total time=  23.4s\n[CV 3/4] END max_features=8, min_samples_split=3, n_estimators=100;, score=0.805 total time=  23.1s\n[CV 4/4] END max_features=8, min_samples_split=3, n_estimators=100;, score=0.798 total time=  22.9s\n[CV 1/4] END max_features=8, min_samples_split=3, n_estimators=200;, score=0.804 total time=  46.8s\n[CV 2/4] END max_features=8, min_samples_split=3, n_estimators=200;, score=0.792 total time=  46.8s\n[CV 3/4] END max_features=8, min_samples_split=3, n_estimators=200;, score=0.808 total time=  46.4s\n[CV 4/4] END max_features=8, min_samples_split=3, n_estimators=200;, score=0.801 total time=  46.4s\n[CV 1/4] END max_features=8, min_samples_split=3, n_estimators=400;, score=0.806 total time= 1.6min\n[CV 2/4] END max_features=8, min_samples_split=3, n_estimators=400;, score=0.794 total time= 1.5min\n[CV 3/4] END max_features=8, min_samples_split=3, n_estimators=400;, score=0.808 total time= 1.6min\n[CV 4/4] END max_features=8, min_samples_split=3, n_estimators=400;, score=0.800 total time= 1.6min\n[CV 1/4] END max_features=8, min_samples_split=4, n_estimators=100;, score=0.803 total time=  22.3s\n[CV 2/4] END max_features=8, min_samples_split=4, n_estimators=100;, score=0.792 total time=  22.4s\n[CV 3/4] END max_features=8, min_samples_split=4, n_estimators=100;, score=0.806 total time=  22.1s\n[CV 4/4] END max_features=8, min_samples_split=4, n_estimators=100;, score=0.798 total time=  22.1s\n[CV 1/4] END max_features=8, min_samples_split=4, n_estimators=200;, score=0.804 total time=  45.0s\n[CV 2/4] END max_features=8, min_samples_split=4, n_estimators=200;, score=0.794 total time=  44.6s\n[CV 3/4] END max_features=8, min_samples_split=4, n_estimators=200;, score=0.807 total time=  44.3s\n[CV 4/4] END max_features=8, min_samples_split=4, n_estimators=200;, score=0.798 total time=  44.6s\n[CV 1/4] END max_features=8, min_samples_split=4, n_estimators=400;, score=0.804 total time= 1.5min\n[CV 2/4] END max_features=8, min_samples_split=4, n_estimators=400;, score=0.794 total time= 1.5min\n[CV 3/4] END max_features=8, min_samples_split=4, n_estimators=400;, score=0.807 total time= 1.5min\n[CV 4/4] END max_features=8, min_samples_split=4, n_estimators=400;, score=0.799 total time= 1.5min\n\n\nGridSearchCV(cv=4, estimator=RandomForestRegressor(),\n             param_grid={'max_features': [4, 6, 8],\n                         'min_samples_split': [2, 3, 4],\n                         'n_estimators': [100, 200, 400]},\n             scoring='r2', verbose=3)In a Jupyter environment, please rerun this cell to show the HTML representation or trust the notebook. On GitHub, the HTML representation is unable to render, please try loading this page with nbviewer.org.GridSearchCVGridSearchCV(cv=4, estimator=RandomForestRegressor(),\n             param_grid={'max_features': [4, 6, 8],\n                         'min_samples_split': [2, 3, 4],\n                         'n_estimators': [100, 200, 400]},\n             scoring='r2', verbose=3)estimator: RandomForestRegressorRandomForestRegressor()RandomForestRegressorRandomForestRegressor()\n\n\nOnce the grid search is done, we can print the best parameters and the best score. We can also get the model from the grid search which scored the highest. So, let’s do that next.\n\n# Print out best parameters with best score\nprint('---Best Parameters From Grid Search---')\nprint('\\n'.join([f'{param}: {value}' for param, value in grid_search.best_params_.items()]))\n\nprint('\\n---Best Score---')\nprint(grid_search.best_score_)\n\n# Save best model\nbest_rf = grid_search.best_estimator_\n\n---Best Parameters From Grid Search---\nmax_features: 4\nmin_samples_split: 2\nn_estimators: 400\n\n---Best Score---\n0.8108895538068155\n\n\n\n\nEvaluations\nFrom the grid search, we know the best r2 score that we were able to get on the training set. Now, with the best model from the grid search (which is already trained), we can evaluate the model on the testing set that we split off earlier. For the predictions on the test set (X_test), we will compare it with the correct values (y_test). Instead of just scoring the model with the r2 score, let’s compute some other popular regression metrics. Along with the r2 score, we will compute the mean absolute error (MAE) and the root mean squared error (RMSE). MAE is simply the average of the absolute differences between the actual target values and the predicted values. RMSE is the square root of MSE, and MSE is the average squared difference between the target and predicted values. To compute MAE, we can simply use the mean_absolute_error() function from sklearn. For RMSE, we will compute MSE with mean_squared_error() function and then take the square root. So that we don’t have to keep computing and printing metrics, we can create a helper function to do it for us.\n\n# Imports\nfrom sklearn.metrics import r2_score\nfrom sklearn.metrics import mean_absolute_error\nfrom sklearn.metrics import mean_squared_error\nfrom math import sqrt\n\n# Helper function to compute and print regression metrics\ndef print_reg_metrics(y_true, y_pred):\n    # Compute R2, MAE, and RMSE\n    r2 = r2_score(y_true, y_pred)\n    mae_score = mean_absolute_error(y_true, y_pred)\n    rmse_score = sqrt(mean_squared_error(y_true, y_pred))\n    \n    # Print Eval Metrics\n    print('+-----------------------------------+')\n    print('|  Evaluation Metrics for Test Set  |')\n    print('+-----------------------------------+')\n    print('|'+f'{\"R2\": &lt;5}= {r2: .3f}'.center(35)+'|')\n    print('|'+f'{\"MAE\": &lt;5}= {mae_score: .3f}'.center(35)+'|')\n    print('|'+f'{\"RMSE\": &lt;5}= {rmse_score: .3f}'.center(35)+'|')\n    print('+-----------------------------------+')\n\nNow, we will predict on the test set and print out our evaluation metrics with the help of our new function.\n\n# Get predictions for the test set\ny_pred = best_rf.predict(X_test)\n\n# Use helper function to compute and print metrics\nprint_reg_metrics(y_test, y_pred)\n\n+-----------------------------------+\n|  Evaluation Metrics for Test Set  |\n+-----------------------------------+\n|           R2   =  0.816           |\n|           MAE  =  0.322           |\n|           RMSE =  0.494           |\n+-----------------------------------+\n\n\nLooks good! Our RMSE tells us that our random forest model explains 81.7% of the variation in the response variable around its mean. Not bad at all! With 0.322 for MAE, we can say that our model predicts a median housing value that is, on average, about $32,200 off of the actual value. For RMSE, we can say that the square root of the variance of the residuals (the differences between the actual values and the predicted values) is 0.493.\nNow, these seem to be pretty good metrics. But, how does our model compare to some other form of regression technique? To find out, we can create a simple linear regression model as a baseline and then compare our evaluation metrics (this is why I decided to make that helper function lol). sklearn implements a linear regression model, so we will make use of that.\n\n# Imports\nfrom sklearn.linear_model import LinearRegression\n\n# Create linear model and train\nlinear_model = LinearRegression()\nlinear_model.fit(X_train, y_train)\n\n# Predict on the test set\ny_pred = linear_model.predict(X_test)\n\n# Print eval metrics\nprint_reg_metrics(y_test, y_pred)\n\n+-----------------------------------+\n|  Evaluation Metrics for Test Set  |\n+-----------------------------------+\n|           R2   =  0.591           |\n|           MAE  =  0.530           |\n|           RMSE =  0.736           |\n+-----------------------------------+\n\n\nWell, as you probably expected, the random forest model far outclasses a simple linear regression model for this task. The r2 score is about 20% less. Both errors are also around 20% greater when using the linear regression.\nThis is likely because there is some more complex, or non-linear, relationships which the random forest was able to learn but the linear regression was not. All in all, using a more complex model was beneficial here. However, it may not always be, so be sure to understand the data and try multiple models out with your own projects.\n\n\nConclusion\nToday in this post, we have walked through how you could go about applying a random forest model for a regression problem. We dove into some data exploration and visualizations, some hyperparameter tuning, and even pitted our model against a baseline to assess performance.\nTo end this post off, our journey through the application of a random forest regressor model on the California Housing dataset has been both insightful and practical. We navigated the intricacies of every step in a typical machine learning project and navigated the intricacies of this simple, but powerful, ensemble-based model. By applying this model, we were able to unlock the potential for making accurate predictions with a dataset containing some pretty complex feature relationships. The ensemble nature of our model has proven its viability in classic regression tasks due to its resilience against over-fitting and ability to capture intricate patterns within the data. As we continue in our lives, away from this post equipped with new found knowledge, you and I are both better equipped to unravel the complexities of predictive modeling and contribute to this weird new realm we call machine learning. If you ever wanted to learn more about a topic, try and write a blog post about it. Although this is written for educational purposes (if anyone is able to find it), I have learned a ton while writing here. Hopefully, you were able to follow along easily with this project and are now more learned than you were before. To sign off once again, have fun out there kid. The world is tough, but you are tougher.\n₍ᐢ•ᴥ•ᐢ₎"
  },
  {
    "objectID": "projects/pandoc-guide/index.html",
    "href": "projects/pandoc-guide/index.html",
    "title": "Pandoc Guide",
    "section": "",
    "text": "This is a detailed, topic-based guide to Pandoc aimed at students in Creating User Documentation (ENGL3814) at Virginia Tech.\n\n\nThis task includes instructions on how to download and install Pandoc on your computer. There are two sections. One is for Windows users the other is for macOS users. There are also optional steps in each section to install a LaTeX program. Installing a LaTeX program is highly recommended, as Pandoc creates PDFs using LaTeX. The LaTeX program we will be installing is MiKTeX.\n\n\nThis task includes instructions on how to download and install Pandoc for Windows users. There are also optional instructions for installing MiKTeX.\n\n\n\nComputer\nWindows Operating System\nInternet Access\n\n\n\n\n\nGo to this webpage.\n\nNote: The webpage should look like this.\n\n\nDownload the pandoc-X.XX.X.X-windows-x86.msi file by clicking on the file name. This is the installer file.\n\nNote: X.XX.X.X refers to the version number. It is 2.17.1.1 in the image above.\n\nRun the installer by double-clicking on the downloaded file.\n\nNote: A new window should pop up. This is what it should look like.\n\n\nPress Next to continue.\nFollow the directions given by the Setup Wizard to complete your installation.\n\nNote: This program automatically installs Pandoc for you.\n\nLastly, to check if Pandoc was installed, type “pandoc -v” in your terminal. If Pandoc was installed correctly, you should see that the next line in the terminal is: “pandoc.exe X.XX.X.X” where the X’s represent the version number. If an error occurs, return to step 1 and try again. &gt; Note: You can access the Windows PowerShell terminal with Windows key + X. Select Windows PowerShell from the menu that pops up. (See image below for reference).\n\n\n\n\n\n\nGo to this webpage.\n\nNote: The webpage should look like this.\n\n\nUnder the Windows tab, select the Installer tab.\nPress the Download button to download the installer.\nRun the installer by double-clicking on the downloaded file.\n\nNote: A new window should pop up. This is what it should look like.\n\n\nRead the conditions and check the “I accept the MiKTeX copying conditions.” option.\nPress Next to continue.\nFollow the instructions in the installer to complete the installation.\nOnce you have reached this page in the installer, you are done. MiKTeX has been installed!\n\n\n\n\n\n\nThis task includes instructions on how to download and install Pandoc for macOS users. There are also optional instructions for installing MiKTeX.\n\n\n\nComputer\nmacOS Operating System\nInternet Access\n\n\n\n\n\nGo to this webpage.\n\nNote: The webpage should look like this.\n\n\nDownload the pandoc-X.XX.X.X-macOS.pkg file by clicking on the file name. This is the installer file.\n\nNote: X.XX.X.X refers to the version number. It is 2.17.1.1 in the image above.\n\nRun the installer by double-clicking on the downloaded file.\n\nNote: A new window should pop up. This is what it should look like.\n\n\nPress Continue to continue.\nFollow the directions given by the Installer to complete your installation.\n\nNote: This program automatically installs Pandoc for you.\n\nLastly, to check if Pandoc was installed, type “pandoc -v” in your terminal. If Pandoc was installed correctly, you should see that the next line in the terminal is: “pandoc.exe X.XX.X.X” where the X’s represent the version number. If an error occurs, return to step 1 and try again.\n\nNote: You can access the terminal on a Mac by opening the Launchpad, typing Terminal in the search field, and clicking Terminal.\n\n\n\n\n\n\nGo to this webpage.\n\nNote: The webpage should look like this.\n\n\nSelect the Mac tab.\nPress the Download button to download the disk image (.dmg) file.\nDouble-click on the downloaded disk image (.dmg) file.\n\nNote: A new window should pop up. This is what it should look like.\n\n\nDrag the MiKTeX icon onto the Applications folder icon.\n\nNote: This will install the MiKTeX Console application and all required files.\n\nRun the MiKTeX Console application from Launchpad.\n\nNote: A new window should pop up. This is what it should look like.\n\n\nSelect the Finish private setup option.\nMiKTeX has been installed!\n\n\n\n\n\n\nTo easily convert files we’ll be using a writing and publishing environment. Visual Studio Code is a popular coding editor and we’ll be using it with Pandoc to convert files. These steps will walk you through downloading, installing, and setting up Visual Studio Code.\n\n\n\nComputer running either Windows or Mac\n\n\n\n\n\nGo to the Visual Studio Code website.\nDownload Visual Studio code for your machine.\n\nInstall the package.\nOpen the program.\nNavigate to the extensions tab on the left of the screen.\n\nInstall any useful extensions you might need.\nNote helpful extensions for markdown are markdownlint and Markdown All in One.\nCustomize with themes (Optional).\nOpen the folder your files will be in or create a new folder.\nAdd a new markdown file.\nAdd content.\n\n\n\n\n\nWhen writing documents for different occasions, it is often necessary to know how to convert a document from one form to another. While some professors may require PDF files, others may require Markdown files. So, with the help of Pandoc converting documents becomes a simple task.\nPandoc is a tool that can be installed on your computer to convert files from one format to another. Pandoc works with formats such as HTML, ebooks, documentation formats, bibliography formats and many more.\nOne example of how students have used Pandoc in the past was for a project. This project required converting a Microsoft Word file (.docx) to a Markdown file (.md). After downloading and checking for installation, by typing the correct conversion code into the computer terminal, Pandoc converted the students’ files from Microsoft Word to Markdown.\nSome benefits of Pandoc include:\n\nIt can convert formats from and to a a variety of different formats\nIts ability to convert citations into a properly formatted citation\nThe option to customize using a template system and writing filters\nWorks with slide show formats as well as text and data formats\n\n\n\n\nThis is a general tutorial for writing in Markdown syntax. It will detail the use of basic Markdown syntax elements. Additionally, since Pandoc understands an extended version of Markdown syntax, some useful extensions of traditional Markdown syntax will be sectioned at the end for use with Pandoc.\n\n\nHeadings are created by beginning a line with hashtags (#). The heading name must be separated from the hashtags by a blank space. The number of hashtags represents the level of the heading. For instance, a level one heading would be written as “# Level 1 Heading”.\n\n\n\nHeading Level\nMarkdown Syntax\n\n\n\n\nLevel 1\n# Level 1 Heading\n\n\nLevel 2\n## Level 2 Heading\n\n\nLevel 3\n### Level 3 Heading\n\n\nLevel 4\n#### Level 4 Heading\n\n\nLevel 5\n##### Level 5 Heading\n\n\nLevel 6\n###### Level 6 Heading\n\n\n\n\nThe table above illustrates syntax for various level headings. Note that markdown only supports six levels of headings.\n\n\n\n\n\nParagraphs can be created by using a blank line to separate blocks of text.\nFor instance, these two paragraphs have been separated by a blank space and thus will be rendered as such.\n\nNote: If you must indent a paragraph use “&nbsp;&nbsp;&nbsp;&nbsp;”. This bit of text will add 4 empty spaces.\n\n     This sentence has been indented using this technique.\n\n\n\n\nLine breaks can be used by adding two or more blank spaces at the end of a line and then pressing return.\nThis line has been separated with a line break.\n\n\n\n\nTo bold text, surround it with two asterisks. To italicize text, surround it with one asterisk. To both bold and italicize a piece of text, surround it with three asterisks.\n\n\n\n\n\n\n\nMarkdown Syntax\nOutput\n\n\n\n\n*This* is italicized.\nThis is italicized.\n\n\n**This** is in bold.\nThis is in bold.\n\n\n***This*** is in bold and italicized.\nThis is in bold and italicized.\n\n\n\n\nThe table above illustrates various ways to emphasize text.\n\n\n\n\n\nTo create block quotes, begin paragraph with &gt;.\nFor example, in markdown\n&gt; This is a block quote.\nFor a block quotes with blank line, begin the blank line with &gt;. Also, block quotes can be nested by adding &gt;&gt; in front of the nested quote. See the example below.\n&gt; This is a block quote\n&gt;\n&gt; with a blank line\n&gt;&gt; and a nested quote.\n\n\n\n\nBoth unordered lists and ordered lists can be created in Markdown. Lists can also be indented to create nested lists.\n\n\nIn Markdown, to create an ordered list, you must begin each list element with a number and a period. The numbering must begin with number one, but the order following number one does not matter.\nAn ordered list in Markdown syntax is as follows:\n1. One\n2. Two\n3. Three\nOR\n1. One\n1. Two\n1. Three\nOR\n1. One\n75. Two\n35. Three\nWhen rendered, the lists all look like this:\n\nOne\nTwo\nThree\n\n\n\n\nUnordered lists can be created by adding dashes (-), asterisks (*), or plus signs (+) before each list element.\nAn unordered list in Markdown syntax is as follows:\n- One\n- Two\n- Three\nOR\n* One\n* Two\n* Three\nOR\n+ One\n+ Two\n+ Three\nWhen rendered, the lists all look like this:\n\nOne\nTwo\nThree\n\nHere is an example of a nested list.\nMarkdown Syntax:\n1. One\n2. Two\n    - Indented One\n    - Indented Two\n3. Three\nOutput:\n\nOne\nTwo\n\nIndented One\nIndented Two\n\nThree\n\n\n\n\n\n\nTo escape any Markdown rendering for bits of code, surround your text with backticks (`).\nFor example: *This* will not be in italics since I surrounded it with backticks.\nYou can also use backslash (\\) to escape other characters as:\n\nbackslash \\\nbacktick `\nasterisk *\nunderscore _\ncurly braces { }\nbrackets [ ]\nangle brackets &lt; &gt;\nparentheses ( )\nhashtag #\nplus sign +\ndash -\nperiod .\nexclamation mark !\npipe |\n\nMarkdown Syntax: \\# Without a backslash, this would be a heading.\nOutput: # Without a backslash, this would be a heading.\n\n\n\n\nIn order to insert a horizontal rule in Markdown, you must use three or more asterisks, dashes, or underscores on a line, like so:\n***\n---\n___\n\nA horizontal rule is below this.\n\n\n\n\n\nFor links, you can enclose link text in brackets and follow it with the URL in parentheses.\nFor instance: [Google](google.com)\nwill look like: Google\n\nYou can also link to other files by replacing the link address with a file name. Make sure the file is accessible in your current working directory.\n\n\n\n\n\nFor images, you can add an exclamation mark (!), followed by image alt text enclosed by brackets, and then followed by the image file path or URL enclosed by parentheses. You can also add a title to the image with quotations marks besides your path or URL. See below for an example.\nMarkdown Syntax:\n![Markdown Logo](/media/markdown.jpg \"Markdown\")\nOutput: \nFinally, you can link images to a URL by adding the link address enclosed by parentheses immediately following the image. You must also surround the image code with brackets. See the following example.\nMarkdown Syntax:\n[![Markdown Logo](/media/markdown.jpg \"Markdown\")](https://www.markdownguide.org/getting-started/)\nOutput: \n\n\n\n\nYou can strikethrough text by surrounding the text with two tildes.\nFor example: ~~This~~ is using strikethrough.\nOutput: This is using strikethrough.\n\n\n\n\nTo create a table, use three or more hyphens to create the column headers, and use pipes to create each column. See below for an example.\nMarkdown Syntax:\n| Header 1 | Header 2 |\n| -------- | -------- |\n| Element1 | Element2 |\n| Element3 | Element4 |\nOutput:\n\n\n\nHeader 1\nHeader 2\n\n\n\n\nElement1\nElement2\n\n\nElement3\nElement4\n\n\n\n\n\n\n\nPandoc understands an extended version of Markdown, so here are some useful extensions of the syntax which will work with Pandoc.\n\n\nYou can create headings in setext-style by following a heading with a line of equal signs (for level 1 headings) or dashes (for level 2 headings).\nFor example:\nThis is a level 1 heading\n=========================\nThis is a level 2 heading\n-------------------------\n\n\n\nPandoc allows for definition lists with the following syntax.\nTerm 1\n\n:   Definition 1\n\nTerm 2\n\n:   Definition 2\n\n    Second paragraph of definition 2.\n\n\n\nPandoc also supports multiline tables. Table rows may span multiple lines of text. See below for an example on how you may format one.\n-------------------------------------------------------------\n Centered   Default           Right Left\n  Header    Aligned         Aligned Aligned\n----------- ------- --------------- -------------------------\nFirst       row                12.0 Example of a row that\n                                    spans multiple lines.\n\nSecond      row                5.0 Here's another one. Note\n                                    the blank line between\n                                    rows.\n-------------------------------------------------------------\n\nTable: Here's the caption. It, too, may span\nmultiple lines.\n\n\n\nGrid Tables can also be created with Pandoc. A row of equal signs must separate the headers, and plus signs must indicate corners of cells. See below for an example.\n: Sample grid table.\n\n+---------------+---------------+--------------------+\n| Fruit         | Price         | Advantages         |\n+===============+===============+====================+\n| Bananas       | $1.34         | - built-in wrapper |\n|               |               | - bright color     |\n+---------------+---------------+--------------------+\n| Oranges       | $2.10         | - cures scurvy     |\n|               |               | - tasty            |\n+---------------+---------------+--------------------+\nCell elements may contain multiple paragraphs, code blocks, lists, etc. However, cannot span multiple columns or rows.\n\n\n\nSuperscripts can be created by surrounding the superscript with carets (^). Subscripts can be creating by surounding the subscript with tildes (~).\nFor example: H~2~O is a liquid. 2^10^ is 1024.\n\n\n\nAny text surrounded by two dollar signs ($) will be treated as TeX math. The output will depend on the output format. There must not be a whitespace after the first dollar sign or before the second one.\nExample: $2+2=4$\nFor example, in Docx format, the text will be rendered using OMML math markup.\n\n\n\n\n\nFor more information about Markdown syntax, a good resource is the official Markdown Guide.\nFor more information about Pandoc’s extended syntax, a good resource is the official Pandoc Manual.\n\n\n\n\nThis section is a comprehensive guide on how to convert Markdown files to EPUB, HTML, Word, and PDF files. At the end of this process you will have the knowledge necessary to effortlessly make this conversion in the future.\n\n\nThe following are assumed to have been installed/obtained before beginning the guide\n\nPandoc\nMarkdown File\npdfLatex FOR PDF PUBLISHES ONLY\n\n\n\n\n\nOpen your File Explorer and locate the Markdown file that you would like to convert. Your screen should look similar to the image below (where the “convertMe” file is YOUR file).\n\nClick on the Path of your current directory in your file explorer. This can be located directly above the files shown, and is shown highlighted in the image below.\n\nType cmd over your highlighted path. Once it is highlighted from the previous step, all you must do is begin typing. Once this is complete, your screen should look similar to the image below.\n\nPress enter. This will open the command prompt at the directory that you were in during the File Explorer session. Once this is complete, your screen should look similar to the image below.\n\nType the appropriate command into the command prompt. The generalized formula for your command is the following\npandoc -s YOURFILE.md -o YOUROUTPUT.FILETYPE\nThe YOURFILE.md should be renamed based upon the name of the Markdown file you are attempting to convert. In our examples below, this file is named “convertMe”.\nThe YOUROUTPUT.FILETYPE should be renamed based upon the desired name of your output file and the type of file you are converting to. In our examples below, the file is named “convertedFile” and is being converted to Word.\nFor EPUB\npandoc -s convertMe.md -o convertedFile.epub\nFor HTML\npandoc -s convertMe.md -o convertedFile.html\nFor Word\npandoc -s convertMe.md -o convertedFile.docx\nFor PDF must have pdfLatex installed\npandoc -s convertMe.md -o convertedFile.pdf\nOnce this is complete, your screen’s command should look similar to the image below.\n\nPress enter. This will make Pandoc execute the command. The output file will now be present in the folder of your original Markdown file.\n\nIf you are receiving an error upon hitting enter you have made some small mistake in the previous steps. Carefully retrace your steps and be certain you are typing everything exactly as expected into the command prompt.\n\nOnce this is complete, your screen should look similar to the image below.\n\n\n\n\n\nYou have successfully converted your file."
  },
  {
    "objectID": "projects/pandoc-guide/index.html#installing-pandoc",
    "href": "projects/pandoc-guide/index.html#installing-pandoc",
    "title": "Pandoc Guide",
    "section": "",
    "text": "This task includes instructions on how to download and install Pandoc on your computer. There are two sections. One is for Windows users the other is for macOS users. There are also optional steps in each section to install a LaTeX program. Installing a LaTeX program is highly recommended, as Pandoc creates PDFs using LaTeX. The LaTeX program we will be installing is MiKTeX.\n\n\nThis task includes instructions on how to download and install Pandoc for Windows users. There are also optional instructions for installing MiKTeX.\n\n\n\nComputer\nWindows Operating System\nInternet Access\n\n\n\n\n\nGo to this webpage.\n\nNote: The webpage should look like this.\n\n\nDownload the pandoc-X.XX.X.X-windows-x86.msi file by clicking on the file name. This is the installer file.\n\nNote: X.XX.X.X refers to the version number. It is 2.17.1.1 in the image above.\n\nRun the installer by double-clicking on the downloaded file.\n\nNote: A new window should pop up. This is what it should look like.\n\n\nPress Next to continue.\nFollow the directions given by the Setup Wizard to complete your installation.\n\nNote: This program automatically installs Pandoc for you.\n\nLastly, to check if Pandoc was installed, type “pandoc -v” in your terminal. If Pandoc was installed correctly, you should see that the next line in the terminal is: “pandoc.exe X.XX.X.X” where the X’s represent the version number. If an error occurs, return to step 1 and try again. &gt; Note: You can access the Windows PowerShell terminal with Windows key + X. Select Windows PowerShell from the menu that pops up. (See image below for reference).\n\n\n\n\n\n\nGo to this webpage.\n\nNote: The webpage should look like this.\n\n\nUnder the Windows tab, select the Installer tab.\nPress the Download button to download the installer.\nRun the installer by double-clicking on the downloaded file.\n\nNote: A new window should pop up. This is what it should look like.\n\n\nRead the conditions and check the “I accept the MiKTeX copying conditions.” option.\nPress Next to continue.\nFollow the instructions in the installer to complete the installation.\nOnce you have reached this page in the installer, you are done. MiKTeX has been installed!\n\n\n\n\n\n\nThis task includes instructions on how to download and install Pandoc for macOS users. There are also optional instructions for installing MiKTeX.\n\n\n\nComputer\nmacOS Operating System\nInternet Access\n\n\n\n\n\nGo to this webpage.\n\nNote: The webpage should look like this.\n\n\nDownload the pandoc-X.XX.X.X-macOS.pkg file by clicking on the file name. This is the installer file.\n\nNote: X.XX.X.X refers to the version number. It is 2.17.1.1 in the image above.\n\nRun the installer by double-clicking on the downloaded file.\n\nNote: A new window should pop up. This is what it should look like.\n\n\nPress Continue to continue.\nFollow the directions given by the Installer to complete your installation.\n\nNote: This program automatically installs Pandoc for you.\n\nLastly, to check if Pandoc was installed, type “pandoc -v” in your terminal. If Pandoc was installed correctly, you should see that the next line in the terminal is: “pandoc.exe X.XX.X.X” where the X’s represent the version number. If an error occurs, return to step 1 and try again.\n\nNote: You can access the terminal on a Mac by opening the Launchpad, typing Terminal in the search field, and clicking Terminal.\n\n\n\n\n\n\nGo to this webpage.\n\nNote: The webpage should look like this.\n\n\nSelect the Mac tab.\nPress the Download button to download the disk image (.dmg) file.\nDouble-click on the downloaded disk image (.dmg) file.\n\nNote: A new window should pop up. This is what it should look like.\n\n\nDrag the MiKTeX icon onto the Applications folder icon.\n\nNote: This will install the MiKTeX Console application and all required files.\n\nRun the MiKTeX Console application from Launchpad.\n\nNote: A new window should pop up. This is what it should look like.\n\n\nSelect the Finish private setup option.\nMiKTeX has been installed!"
  },
  {
    "objectID": "projects/pandoc-guide/index.html#setting-up-a-writing-and-publishing-environment",
    "href": "projects/pandoc-guide/index.html#setting-up-a-writing-and-publishing-environment",
    "title": "Pandoc Guide",
    "section": "",
    "text": "To easily convert files we’ll be using a writing and publishing environment. Visual Studio Code is a popular coding editor and we’ll be using it with Pandoc to convert files. These steps will walk you through downloading, installing, and setting up Visual Studio Code.\n\n\n\nComputer running either Windows or Mac\n\n\n\n\n\nGo to the Visual Studio Code website.\nDownload Visual Studio code for your machine.\n\nInstall the package.\nOpen the program.\nNavigate to the extensions tab on the left of the screen.\n\nInstall any useful extensions you might need.\nNote helpful extensions for markdown are markdownlint and Markdown All in One.\nCustomize with themes (Optional).\nOpen the folder your files will be in or create a new folder.\nAdd a new markdown file.\nAdd content."
  },
  {
    "objectID": "projects/pandoc-guide/index.html#what-is-pandoc",
    "href": "projects/pandoc-guide/index.html#what-is-pandoc",
    "title": "Pandoc Guide",
    "section": "",
    "text": "When writing documents for different occasions, it is often necessary to know how to convert a document from one form to another. While some professors may require PDF files, others may require Markdown files. So, with the help of Pandoc converting documents becomes a simple task.\nPandoc is a tool that can be installed on your computer to convert files from one format to another. Pandoc works with formats such as HTML, ebooks, documentation formats, bibliography formats and many more.\nOne example of how students have used Pandoc in the past was for a project. This project required converting a Microsoft Word file (.docx) to a Markdown file (.md). After downloading and checking for installation, by typing the correct conversion code into the computer terminal, Pandoc converted the students’ files from Microsoft Word to Markdown.\nSome benefits of Pandoc include:\n\nIt can convert formats from and to a a variety of different formats\nIts ability to convert citations into a properly formatted citation\nThe option to customize using a template system and writing filters\nWorks with slide show formats as well as text and data formats"
  },
  {
    "objectID": "projects/pandoc-guide/index.html#how-to-write-in-markdown",
    "href": "projects/pandoc-guide/index.html#how-to-write-in-markdown",
    "title": "Pandoc Guide",
    "section": "",
    "text": "This is a general tutorial for writing in Markdown syntax. It will detail the use of basic Markdown syntax elements. Additionally, since Pandoc understands an extended version of Markdown syntax, some useful extensions of traditional Markdown syntax will be sectioned at the end for use with Pandoc.\n\n\nHeadings are created by beginning a line with hashtags (#). The heading name must be separated from the hashtags by a blank space. The number of hashtags represents the level of the heading. For instance, a level one heading would be written as “# Level 1 Heading”.\n\n\n\nHeading Level\nMarkdown Syntax\n\n\n\n\nLevel 1\n# Level 1 Heading\n\n\nLevel 2\n## Level 2 Heading\n\n\nLevel 3\n### Level 3 Heading\n\n\nLevel 4\n#### Level 4 Heading\n\n\nLevel 5\n##### Level 5 Heading\n\n\nLevel 6\n###### Level 6 Heading\n\n\n\n\nThe table above illustrates syntax for various level headings. Note that markdown only supports six levels of headings.\n\n\n\n\n\nParagraphs can be created by using a blank line to separate blocks of text.\nFor instance, these two paragraphs have been separated by a blank space and thus will be rendered as such.\n\nNote: If you must indent a paragraph use “&nbsp;&nbsp;&nbsp;&nbsp;”. This bit of text will add 4 empty spaces.\n\n     This sentence has been indented using this technique.\n\n\n\n\nLine breaks can be used by adding two or more blank spaces at the end of a line and then pressing return.\nThis line has been separated with a line break.\n\n\n\n\nTo bold text, surround it with two asterisks. To italicize text, surround it with one asterisk. To both bold and italicize a piece of text, surround it with three asterisks.\n\n\n\n\n\n\n\nMarkdown Syntax\nOutput\n\n\n\n\n*This* is italicized.\nThis is italicized.\n\n\n**This** is in bold.\nThis is in bold.\n\n\n***This*** is in bold and italicized.\nThis is in bold and italicized.\n\n\n\n\nThe table above illustrates various ways to emphasize text.\n\n\n\n\n\nTo create block quotes, begin paragraph with &gt;.\nFor example, in markdown\n&gt; This is a block quote.\nFor a block quotes with blank line, begin the blank line with &gt;. Also, block quotes can be nested by adding &gt;&gt; in front of the nested quote. See the example below.\n&gt; This is a block quote\n&gt;\n&gt; with a blank line\n&gt;&gt; and a nested quote.\n\n\n\n\nBoth unordered lists and ordered lists can be created in Markdown. Lists can also be indented to create nested lists.\n\n\nIn Markdown, to create an ordered list, you must begin each list element with a number and a period. The numbering must begin with number one, but the order following number one does not matter.\nAn ordered list in Markdown syntax is as follows:\n1. One\n2. Two\n3. Three\nOR\n1. One\n1. Two\n1. Three\nOR\n1. One\n75. Two\n35. Three\nWhen rendered, the lists all look like this:\n\nOne\nTwo\nThree\n\n\n\n\nUnordered lists can be created by adding dashes (-), asterisks (*), or plus signs (+) before each list element.\nAn unordered list in Markdown syntax is as follows:\n- One\n- Two\n- Three\nOR\n* One\n* Two\n* Three\nOR\n+ One\n+ Two\n+ Three\nWhen rendered, the lists all look like this:\n\nOne\nTwo\nThree\n\nHere is an example of a nested list.\nMarkdown Syntax:\n1. One\n2. Two\n    - Indented One\n    - Indented Two\n3. Three\nOutput:\n\nOne\nTwo\n\nIndented One\nIndented Two\n\nThree\n\n\n\n\n\n\nTo escape any Markdown rendering for bits of code, surround your text with backticks (`).\nFor example: *This* will not be in italics since I surrounded it with backticks.\nYou can also use backslash (\\) to escape other characters as:\n\nbackslash \\\nbacktick `\nasterisk *\nunderscore _\ncurly braces { }\nbrackets [ ]\nangle brackets &lt; &gt;\nparentheses ( )\nhashtag #\nplus sign +\ndash -\nperiod .\nexclamation mark !\npipe |\n\nMarkdown Syntax: \\# Without a backslash, this would be a heading.\nOutput: # Without a backslash, this would be a heading.\n\n\n\n\nIn order to insert a horizontal rule in Markdown, you must use three or more asterisks, dashes, or underscores on a line, like so:\n***\n---\n___\n\nA horizontal rule is below this.\n\n\n\n\n\nFor links, you can enclose link text in brackets and follow it with the URL in parentheses.\nFor instance: [Google](google.com)\nwill look like: Google\n\nYou can also link to other files by replacing the link address with a file name. Make sure the file is accessible in your current working directory.\n\n\n\n\n\nFor images, you can add an exclamation mark (!), followed by image alt text enclosed by brackets, and then followed by the image file path or URL enclosed by parentheses. You can also add a title to the image with quotations marks besides your path or URL. See below for an example.\nMarkdown Syntax:\n![Markdown Logo](/media/markdown.jpg \"Markdown\")\nOutput: \nFinally, you can link images to a URL by adding the link address enclosed by parentheses immediately following the image. You must also surround the image code with brackets. See the following example.\nMarkdown Syntax:\n[![Markdown Logo](/media/markdown.jpg \"Markdown\")](https://www.markdownguide.org/getting-started/)\nOutput: \n\n\n\n\nYou can strikethrough text by surrounding the text with two tildes.\nFor example: ~~This~~ is using strikethrough.\nOutput: This is using strikethrough.\n\n\n\n\nTo create a table, use three or more hyphens to create the column headers, and use pipes to create each column. See below for an example.\nMarkdown Syntax:\n| Header 1 | Header 2 |\n| -------- | -------- |\n| Element1 | Element2 |\n| Element3 | Element4 |\nOutput:\n\n\n\nHeader 1\nHeader 2\n\n\n\n\nElement1\nElement2\n\n\nElement3\nElement4\n\n\n\n\n\n\n\nPandoc understands an extended version of Markdown, so here are some useful extensions of the syntax which will work with Pandoc.\n\n\nYou can create headings in setext-style by following a heading with a line of equal signs (for level 1 headings) or dashes (for level 2 headings).\nFor example:\nThis is a level 1 heading\n=========================\nThis is a level 2 heading\n-------------------------\n\n\n\nPandoc allows for definition lists with the following syntax.\nTerm 1\n\n:   Definition 1\n\nTerm 2\n\n:   Definition 2\n\n    Second paragraph of definition 2.\n\n\n\nPandoc also supports multiline tables. Table rows may span multiple lines of text. See below for an example on how you may format one.\n-------------------------------------------------------------\n Centered   Default           Right Left\n  Header    Aligned         Aligned Aligned\n----------- ------- --------------- -------------------------\nFirst       row                12.0 Example of a row that\n                                    spans multiple lines.\n\nSecond      row                5.0 Here's another one. Note\n                                    the blank line between\n                                    rows.\n-------------------------------------------------------------\n\nTable: Here's the caption. It, too, may span\nmultiple lines.\n\n\n\nGrid Tables can also be created with Pandoc. A row of equal signs must separate the headers, and plus signs must indicate corners of cells. See below for an example.\n: Sample grid table.\n\n+---------------+---------------+--------------------+\n| Fruit         | Price         | Advantages         |\n+===============+===============+====================+\n| Bananas       | $1.34         | - built-in wrapper |\n|               |               | - bright color     |\n+---------------+---------------+--------------------+\n| Oranges       | $2.10         | - cures scurvy     |\n|               |               | - tasty            |\n+---------------+---------------+--------------------+\nCell elements may contain multiple paragraphs, code blocks, lists, etc. However, cannot span multiple columns or rows.\n\n\n\nSuperscripts can be created by surrounding the superscript with carets (^). Subscripts can be creating by surounding the subscript with tildes (~).\nFor example: H~2~O is a liquid. 2^10^ is 1024.\n\n\n\nAny text surrounded by two dollar signs ($) will be treated as TeX math. The output will depend on the output format. There must not be a whitespace after the first dollar sign or before the second one.\nExample: $2+2=4$\nFor example, in Docx format, the text will be rendered using OMML math markup.\n\n\n\n\n\nFor more information about Markdown syntax, a good resource is the official Markdown Guide.\nFor more information about Pandoc’s extended syntax, a good resource is the official Pandoc Manual."
  },
  {
    "objectID": "projects/pandoc-guide/index.html#how-to-publish-to-epub-html-word-and-pdf",
    "href": "projects/pandoc-guide/index.html#how-to-publish-to-epub-html-word-and-pdf",
    "title": "Pandoc Guide",
    "section": "",
    "text": "This section is a comprehensive guide on how to convert Markdown files to EPUB, HTML, Word, and PDF files. At the end of this process you will have the knowledge necessary to effortlessly make this conversion in the future.\n\n\nThe following are assumed to have been installed/obtained before beginning the guide\n\nPandoc\nMarkdown File\npdfLatex FOR PDF PUBLISHES ONLY\n\n\n\n\n\nOpen your File Explorer and locate the Markdown file that you would like to convert. Your screen should look similar to the image below (where the “convertMe” file is YOUR file).\n\nClick on the Path of your current directory in your file explorer. This can be located directly above the files shown, and is shown highlighted in the image below.\n\nType cmd over your highlighted path. Once it is highlighted from the previous step, all you must do is begin typing. Once this is complete, your screen should look similar to the image below.\n\nPress enter. This will open the command prompt at the directory that you were in during the File Explorer session. Once this is complete, your screen should look similar to the image below.\n\nType the appropriate command into the command prompt. The generalized formula for your command is the following\npandoc -s YOURFILE.md -o YOUROUTPUT.FILETYPE\nThe YOURFILE.md should be renamed based upon the name of the Markdown file you are attempting to convert. In our examples below, this file is named “convertMe”.\nThe YOUROUTPUT.FILETYPE should be renamed based upon the desired name of your output file and the type of file you are converting to. In our examples below, the file is named “convertedFile” and is being converted to Word.\nFor EPUB\npandoc -s convertMe.md -o convertedFile.epub\nFor HTML\npandoc -s convertMe.md -o convertedFile.html\nFor Word\npandoc -s convertMe.md -o convertedFile.docx\nFor PDF must have pdfLatex installed\npandoc -s convertMe.md -o convertedFile.pdf\nOnce this is complete, your screen’s command should look similar to the image below.\n\nPress enter. This will make Pandoc execute the command. The output file will now be present in the folder of your original Markdown file.\n\nIf you are receiving an error upon hitting enter you have made some small mistake in the previous steps. Carefully retrace your steps and be certain you are typing everything exactly as expected into the command prompt.\n\nOnce this is complete, your screen should look similar to the image below.\n\n\n\n\n\nYou have successfully converted your file."
  },
  {
    "objectID": "projects/OT-pushRelabel/index.html",
    "href": "projects/OT-pushRelabel/index.html",
    "title": "Analysis of A Push-Relabel Based Additive Approximation for Optimal Transport",
    "section": "",
    "text": "Optimal transport is quite expensive to compute exactly, so there have been recent desires for fast and reliable approximation algorithms. Here I analyze and describe a push-relabel based additive approximation algorithm for Optimal Transport. The paper I reference for this algorithm was originally submitted to the arXiv repository on Mar 7, 2022 by Nathaniel Lahn, Sharath Raghvendra (my former theory and algorithms professor), and Kaiyi Zhang. It is based on the push-relabel framework for min-cost flow problems. And unlike many other approaches, this algorithm can be parallelized for an even quicker execution time.\nPlease see the source below to check out the original paper.\n[A Push-Relabel Based Additive Approximation for Optimal Transport]"
  },
  {
    "objectID": "projects/OT-pushRelabel/index.html#description",
    "href": "projects/OT-pushRelabel/index.html#description",
    "title": "Analysis of A Push-Relabel Based Additive Approximation for Optimal Transport",
    "section": "Description",
    "text": "Description\nThe provided paper details a simple combinatorial approach to find an \\(\\varepsilon\\)-approximation of the Optimal Transport distance. The algorithm is based on the push-relabel framework for min-cost flow problems. This algorithm aims to reduce the numerical instabilities present in the Sinkhorn method of approximating optimal transport. Similar to before, in order to apply this algorithm, the input problem must be transformed so that all edge costs equal a \\(\\varepsilon\\) integer multiple and all nodes are given a dual label. The transformation procedure is detailed below:\n\nTransformation Procedure\nInitially, the input will be a bipartite graph. Let \\(G\\) be this input graph, with two sets of nodes \\(A\\) and \\(B\\). Nodes in set \\(A\\) are referred to as demand nodes, and nodes in set \\(B\\) are referred to as supply nodes.\nFirst, we must convert edge costs to integer multiples of \\(\\varepsilon\\), where \\(\\varepsilon\\) is some constant such that \\(\\varepsilon&gt;0\\). Let \\(c(u,v)\\) denote the cost of edge \\((u,v)\\), and \\(\\overline{c}(u,v)\\) denote the transformed edge cost. For every edge \\((u,v) \\in A \\times B\\), we must transform the edge cost such that, \\(\\overline{c}(u,v) = \\varepsilon\\lfloor c(u,v) / \\varepsilon \\rfloor\\)\nIn addition to this, we must assign dual weights to every vertex \\(v \\in A \\cup B\\). These dual weights must satisfy a set of feasibility conditions. A matching \\(M\\) is considered \\(\\varepsilon\\)-feasible, if for every edge \\((a,b) \\in A \\times B\\):\n\n\\(y(a) + y(b) \\leq \\overline{c}(a,b) + \\varepsilon\\) if edge \\((a,b)\\) is not in the matching \\(M\\).\nor \\(y(a) + y(b) = \\overline{c}(a,b)\\) if edge \\((a,b)\\) is in the matching \\(M\\)."
  },
  {
    "objectID": "projects/OT-pushRelabel/index.html#algorithm",
    "href": "projects/OT-pushRelabel/index.html#algorithm",
    "title": "Analysis of A Push-Relabel Based Additive Approximation for Optimal Transport",
    "section": "Algorithm",
    "text": "Algorithm\nDual weights for every supply node \\(b\\in B\\) are initialized such that \\(y(b) = \\varepsilon\\). Similarly, dual weights for every demand node \\(a\\in A\\) are initialized such that \\(y(a) = 0\\). Finally, we will initialize the matching \\(M\\) to be empty. In other words, \\(M = \\emptyset\\).\nThis algorithm executes in phases, wherein each phase consists of three steps. In each phase, the algorithm will construct a set of free supply nodes, consisting of all free nodes from set \\(B\\). Let \\(B'\\) denote this set of free supply nodes. If \\(|B'| \\leq \\varepsilon n\\), then \\(M\\) is an \\(\\varepsilon\\)-feasible matching and the algorithm will arbitrarily match the remaining free vertices and return the new matching. In the case that \\(|B'| &gt; \\varepsilon n\\), the algorithm will continue by computing a subset of admissible edges with at least least one end point in the free supply node set \\(B'\\).\nLet \\(E\\) denote the full set of admissible edges in the graph, and let \\(E'\\) denote the set of admissible edges with at least least one end point in the free supply node set \\(B'\\). It follows that \\(E' \\subseteq E\\). Finally, let \\(A'\\) denote the set of demand nodes that participate in at least on edge in \\(E'\\). In other words \\(A' = \\{a|a \\in A\\) and \\((a,b) \\in E' \\}\\). Now, the following steps are executed by the algorithm:\n\nGreedy Step: Computes a maximal matching \\(M'\\) in the graph \\(G'(A' \\cup B', E')\\).\nMatching Update: Let \\(A''\\) denote the set of nodes in \\(A'\\) which are present in both \\(M\\) and \\(M'\\). Also, let \\(M''\\) denote the set of edges in \\(M\\) that are incident on some node in \\(A''\\). The algorithm will add the edges of \\(M'\\) to \\(M\\) and remove the edges of \\(M''\\) from \\(M\\).\nDual Weight Update: Let \\(\\overline{y}(v)\\) denote the updated dual weight of some node \\(v\\). For every edge \\((a,b) \\in M'\\), the dual weight of node \\(a\\in A\\) will be updated such that \\(\\overline{y}(a) = y(a) - \\varepsilon\\). Additionally, for every node \\(b\\in B\\) in the set \\(B'\\) such that \\(b\\) is free with respect to \\(M'\\), the dual weight of node \\(b\\) will be updated such that \\(\\overline{y}(b) = y(b) + \\varepsilon\\).\n\n\nExtending the Algorithm to the Optimal Transport Problem\nWith some instance \\(\\mathcal{I}\\) of the optimal transport problem, we can scale the supplies and demands at each node by some multiplicative factor of \\(\\Theta\\). Doing so will not delegitimize the optimal transport plan, as the cost of the optimal transport plan will simply be increased by a factor of \\(\\Theta\\).\nNow, we can choose \\(\\Theta = 4n/\\varepsilon\\), and transform supply and demand values to integer values. To transform the supply and demand values, we can just round supply values down and round demand values up to the nearest integers.\nNext, we create a matching instance by replacing each node with a group of nodes, such that the total of number of replacement nodes equals the respective supply or demand value of the original node. Each node in the replacement group will be set to have a supply or demand value of exactly 1.\nThen, let \\(\\mathcal{A}\\) and \\(\\mathcal{B}\\) be the multi-set of the demand and supply nodes we just produced. Let this new instance of the unbalanced matching problem be denoted by \\(\\mathcal{I'}\\). Now, we can solve the problem instance \\(\\mathcal{I'}\\) with the aforementioned algorithm."
  },
  {
    "objectID": "projects/OT-pushRelabel/index.html#correctness",
    "href": "projects/OT-pushRelabel/index.html#correctness",
    "title": "Analysis of A Push-Relabel Based Additive Approximation for Optimal Transport",
    "section": "Correctness",
    "text": "Correctness\nBefore beginning an assessment of this algorithm’s correctness, we must define some invariants which the algorithm maintains. The invariants and their respective proofs follow:\n(Invariant 1): For every node \\(b\\in B\\), \\(y(b) \\geq 0\\). In other words, every node in set \\(B\\) has a non-negative dual weight. Additionally, for every node \\(a\\in A\\), \\(y(a) \\leq 0\\). In other words, every node in set \\(A\\) is associated with a non-positive dual weight. Furthermore, every free node in set \\(A\\) is associated with a dual weight of 0.\n\nProof. Note that the invariant is true at the start of the algorithm. This is true because dual weights for all nodes in set \\(B\\) are initialized to \\(\\varepsilon\\) and are thus non-negative, and dual weights for all nodes in set \\(A\\) are initialized to \\(0\\) and are thus non-positive. Also, all nodes in set \\(A\\) are initially free as the matching \\(M\\) is initialized to the empty set. Now, inductively assume that the invariant holds at the start of every phase. Next, we must show that the invariant continues to hold at the end of any given phase. Firstly, consider the dual weight update step. When updated, nodes from set \\(B\\) are increased by a factor of \\(\\varepsilon\\), while nodes from set \\(A\\) are decreased by the same factor. Thus, it follows that, as execution progresses, dual weights for nodes in set \\(B\\) strictly increase and dual weights for nodes in set \\(A\\) strictly decrease. So, dual weights for nodes in \\(B\\) will remain non-negative, and dual weights for nodes in \\(A\\) will remain non-positive. Now, we must show that every free node in set \\(A\\) with respect to the matching \\(M\\) will continue to retain a dual weight of \\(0\\). Computing the maximal matching in the greedy step will not affect this condition. Regarding the matching update step, we can conclude that after the matching \\(M\\) has been updated, any node of set \\(A\\) which was matched before the update will remain matched post-update. Because of this, we can further conclude that dual weights for every free node in set \\(A\\) retain zero values. Finally, regarding the dual weight update step, dual weights of nodes from set \\(A\\) are only reduced if said node it matched in \\(M'\\). And, since the previous step will have added edges of \\(M'\\) to \\(M\\), every node in \\(A\\) whose dual weight is updated will be matched in \\(M\\) post-update. Thus, dual weights for all free nodes in set \\(A\\) will retain a zero value. ◻\n\n(Invariant 2): The matching \\(M\\) and a set of dual weights maintains \\(\\varepsilon\\)-feasibility.\n\nProof. Note that the invariant is true at the start of the algorithm, as all \\(\\varepsilon\\)-feasibility conditions are maintained in initializing \\(M\\) and the set of dual weights. Now, inductively assume that the invariant holds at the start of every phase. Next, we must show that the invariant continues to hold at the end of any given phase. So, we must show that edges remain feasible through a phase. To do this, we need to show that the slack of all edges remains non-negative and all edges in the matching \\(M\\) have zero slack. Let \\(s(a,b)\\) denote the slack of edge \\((a,b)\\). Computing the maximal matching in the greedy step will not affect this condition. Only the matching update and dual weight update steps may violate feasibility. Consider an edge \\((a,b)\\) which is feasible but not admissible at the beginning of a phase. In other words \\(s(a,b) \\geq \\varepsilon\\). As the matching \\(M'\\) contains only admissible edges, edge \\((a,b)\\) cannot be in \\(M'\\) nor \\(M\\). Now, the dual weights of nodes \\(a\\) and \\(b\\) may be updated. If the dual weight \\(y(a)\\) is updated, \\(y(a)\\) will only be reduced by \\(\\varepsilon\\), in turn increasing \\(s(a,b)\\) by \\(\\varepsilon\\). Here, \\(s(a,b)\\) will retain a non-negative slack, and so feasibility is maintained. If the dual weight \\(y(b)\\) is updated, \\(y(b)\\) will only be increased by \\(\\varepsilon\\), in turn decreasing \\(s(a,b)\\) by \\(\\varepsilon\\). As, \\(s(a,b) \\geq \\varepsilon\\), \\(s(a,b)\\) will still be non-negative, and so feasibility is maintained. So, in the case that edge \\((a,b)\\) is not admissible at the beginning of a phase, feasibility is maintained. Next, we must consider the case where edge \\((a,b)\\) is admissible at the start of the phase. At the end of the phase, either \\((a,b)\\) is or is not present in the updated matching \\(M\\). Both of these cases are detailed below:\n\nIf \\((a,b)\\) is present in \\(M\\), there are two possible cases. Either, \\((a,b)\\) was in \\(M\\) before the update or \\((a,b)\\) is an edge in \\(M'\\) which was added to the matching and thus exists in \\(M\\) post-update. These further cases are detailed below.\n\nIf \\((a,b)\\) was in \\(M\\) before the update, dual weights for nodes \\(a\\) and \\(b\\) remain unchanged. Thus, feasibility is maintained.\nIf \\((a,b)\\) is an edge in \\(M'\\) which was added to the matching during the update, then \\((a,b) \\in E'\\) was a non-matching admissible edge prior to the update. So, \\(y(a)+y(b) = \\overline{c}(a,b) + \\varepsilon\\). When the dual weights for both nodes are updated, \\(y(a)\\) will be reduced by \\(\\varepsilon\\) and \\(y(b)\\) will remain unchanged, maintaining feasibility. So, once the phase completes, every edge of \\(M\\) retains feasibility.\n\nIf \\((a,b)\\) is not present in \\(M\\), there are two possible cases to explore. Either node \\(b\\) is not present in set \\(B'\\) or b is present in set \\(B'\\). These further cases are detailed below.\n\nIn the case that node \\(b\\) is not present in set \\(B'\\), the edge \\((a,b)\\) satisfies feasibility at the start of a phase. When updating the dual weights of these nodes, only nodes which are present inset \\(B'\\) are updated. And so, since node \\(b\\) is not in \\(B'\\), \\(y(b)\\) is left unchanged. Regarding node \\(a\\), \\(y(a)\\) may be decreased by \\(\\varepsilon\\), in turn increasing \\(s(a,b)\\). Thus, feasibility is always maintained.\nIn the case that node \\(b\\) is present in set \\(B'\\), since \\((a,b)\\) is admissible and is not in \\(M\\) post-update, \\((a,b)\\) cannot be in \\(M'\\). Now, consider the fact that \\(M'\\) is a maximal matching. Because of this, node \\(a\\) is matched with some other node \\(b' \\in M'\\). So, if \\(y(b)\\) is updated, and is thus increased by \\(\\varepsilon\\), \\(y(a)\\) will also updated, and thus will be decreased by \\(\\varepsilon\\). Due to this, \\(s(a,b)\\) remains non-negative, and feasibility is maintained.\n\n\n ◻\n\nNow, finally that the invariants have been proven, we can continue to assess this algorithm’s correctness. Rounding edge costs in the manner described previously will introduce an error of \\(\\varepsilon n\\). Then, after computing a matching of size at least \\((1-\\varepsilon)n\\), when arbitrarily matching the last \\(\\varepsilon n\\) vertices, we incur an error no greater than \\(\\varepsilon n\\). Finally, with reference to the following lemma (Lemma 3.1), we can conclude that the total error in the computed matching is no greater than \\(+3\\varepsilon n\\). And thus, the matching is \\(\\varepsilon\\)-feasible.\nLemma 3.1: The \\(\\varepsilon\\)-feasible matching of size at least \\((1-\\varepsilon)n\\) that is computed by the algorithm is within an additive error of \\(\\varepsilon n\\) from the optimal matching with respect to the rounded edge costs.\n\nProof. Let \\(M\\) be the matching computed by the algorithm. Furthermore, let \\(M_{OPT}\\) be the optimal matching with respect to the rounded edge costs. Considering the first feasibility condition and the fact that dual weights for all free vertices with respect to \\(M\\) are non-negative we can conclude that: \\(\\sum_{(a,b)\\in M} \\overline{c}(a,b) = \\sum_{(a,b)\\in M}(y(a) + y(b)) \\leq \\sum_{v\\in A\\cup B} y(v)\\). Now, as \\(M_{OPT}\\) is a perfect matching, we can further conclude from the second feasibility condition that: \\(\\sum_{v\\in A\\cup B} y(v) = \\sum_{(a,b)\\in M_{OPT}}(y(a) + y(b)) \\leq \\sum_{(a,b)\\in M_{OPT}}\\overline{c}(a,b) + \\varepsilon n\\). And so, \\(\\sum_{(a,b)\\in M} \\overline{c}(a,b) \\leq \\sum_{(a,b)\\in M_{OPT}}\\overline{c}(a,b) + \\varepsilon n\\). Thus, conclusively, the matching \\(M\\) is within an additive error of \\(\\varepsilon n\\) from the optimal matching with respect to rounded edge costs. ◻"
  },
  {
    "objectID": "projects/OT-pushRelabel/index.html#efficiency",
    "href": "projects/OT-pushRelabel/index.html#efficiency",
    "title": "Analysis of A Push-Relabel Based Additive Approximation for Optimal Transport",
    "section": "Efficiency",
    "text": "Efficiency\nBefore beginning an assessment of this algorithm’s efficiency, we must provide some lemmas. The lemmas and their respective proofs follow:\nLemma 3.2: For any node \\(v \\in A \\cup B\\), \\(|y(v)| \\leq (1+2\\varepsilon\\).\n\nProof. Note that the algorithm only ever increases the magnitude of any given dual weight. So, it is sufficient to show that the claim holds at the end of the algorithm. For all nodes in \\(A \\cup B\\), there are only two cases to consider. Either, a node is present in set \\(A\\) or in set \\(B\\). The two cases are described below:\n\nLet \\(a\\) be some node such that \\(a \\in A\\). If node \\(a\\) is free, then the claim holds true due to Invariant 1. Now, if node \\(a\\) is not free, and is thus matched to some node \\(b \\in B\\), edge \\((a,b)\\) is feasible. Because edge \\((a,b)\\) is feasible, we can conclude that \\(y(a) = \\overline{c}(a,b) - y(b) \\geq -y(b) \\geq -(1+2\\varepsilon)\\). So, \\(|y(a)| \\leq (1+2\\varepsilon)\\).\nLet \\(a\\) be some free node at the start of the final phase of the algorithm such that \\(a \\in A\\). We know, from Invariant 1, that \\(y(a) = 0\\). Now, for every vertex \\(b \\in B\\), edge \\((a,b)\\) satisfies feasibility. And so, \\(y(b) \\leq \\overline{c}(a,b) + \\varepsilon - y(a) = \\overline{c}(a,b) + \\varepsilon \\leq 1 + \\varepsilon\\). So, since dual weights for nodes in set \\(B\\) are only ever updated adding \\(\\varepsilon\\), the dual weight for any node \\(b \\in B\\) when the algorithm reaches completion is at most \\(1 + \\varepsilon + \\varepsilon = (1 + 2\\varepsilon)\\).\n\n ◻\n\nLemma 3.3: The sum of the magnitude of the dual weights increases by at least \\(\\varepsilon n_i\\) in each phase.\n\nProof. Let \\(b\\) be some node at the beginning of some phase such that \\(b\\in B'\\). Now, once the phase completes, there are two cases we must consider. Either, node \\(b\\) is still free or it has been matched. Both cases are detailed below:\n\nIn the case that node \\(b\\) is still free once the phase completes, \\(|y(b)|\\) increases by \\(\\varepsilon\\).\nIn the case that \\(b\\) is no longer free once the phase completes, and thus has been matched to some arbitrary node \\(a \\in M'\\), \\(|y(a)|\\) increases by \\(\\varepsilon\\) as well.\n\nSo, conclusively, we can see that each node in set \\(B'\\) causes the dual weight magnitude of some vertex to increase by exactly \\(\\varepsilon\\). Thus, the total dual weight increase in each phase is at least \\(\\varepsilon n_i\\). ◻\n\nLemma 3.4: The execution time of each phase is \\(O(n \\times n_i)\\).\n\nProof. Note that computing the set of free vertices at the start of each phase can be done in \\(O(n)\\) time. Now, consider the greedy step. Initially, the algorithm must compute a maximal matching in the graph \\(G'(A' \\cup B', E')\\). To construct this graph, we can scan over all edges incident to nodes in \\(B'\\), taking \\(O(n \\times n_i)\\) time. Then, the algorithm will find the maximal matching by processing each free node \\(b \\in B'\\). The algorithm will then attempt to match node \\(b\\) by finding the first edge \\((a,b)\\) in the graph such that \\(a\\) is not present in \\(M'\\). If such an edge is found, it is then added to \\(M'\\). If such an edge cannot be found, then node \\(b\\) cannot be matched in \\(M'\\). Once every node in \\(B'\\) has been processed, \\(M'\\) is maximal. In this step, the only procedure significant to the execution time is that which constructs graph \\(G'\\) in \\(O(n \\times n_i)\\) time. So, this step executes in \\(O(n \\times n_i)\\) time.\nNext, consider the matching update and dual weight update steps. Since any matching has a size which is \\(O(n)\\) and each node’s dual weight is updated at most once within any given phase, executing both of these steps will cost \\(O(n)\\) time. Finally, the execution time of the entire phase comes out to be \\(O( (n \\times n_i) + n ) = O(n \\times n_i)\\). ◻\n\nNow, given the lemmas, we can continue assessing the overall execution time of the algorithm. Suppose that the algorithm completes \\(t\\) phases during runtime. Let \\(n_i\\) denote the size of \\(B'\\) in phase \\(i\\) where \\(i\\) is some integer such that \\(1 &lt; i \\leq t\\). From the termination condition, we can conclude that phase \\(i\\) is only executed if \\(n_i &gt; \\varepsilon n\\). In other words, if the size of \\(B'\\) is more than \\(\\varepsilon n\\), the algorithm terminates. Now, from Lemma 3.2, we know that the magnitude of the dual weight of any node is at most \\((1+2\\varepsilon)\\). So, we use this as an upper bound for the magnitude of the dual weight of any given node. Furthering this concept, we can see that the sum total of the dual weight magnitudes over all nodes is at most \\(n(1+2\\varepsilon)\\). Also, from Lemma 3.3, we know that for some phase \\(i\\), the sum total of the dual weight magnitudes over all nodes increases by at least \\(\\varepsilon n_i\\).\nWith these observations, it follows that: \\(\\sum_{i=1}^{t} n_i \\leq n(1+2\\varepsilon)/\\varepsilon\\). And, \\(n(1+2\\varepsilon)/\\varepsilon\\) is \\(O(n/\\varepsilon)\\). Now, from Lemma 3.4, we know that the execution time of any given phase is \\(O(n \\times n_i)\\). So, the total sequential execution time of the algorithm becomes \\(O(n \\cdot \\sum_{i=1}^{t} n_i) = O(n \\cdot \\frac{n}{\\varepsilon}) = O (n^2/\\varepsilon)\\).\nRegarding the execution time of the algorithm when run in parallel, note that the matching update and dual weight update steps are trivially parallelizable, making them execute in constant time. Thus, with a classical parallel program to find maximal matchings for the greedy step in \\(O(\\log n)\\) time, the execution time of the program becomes \\(O(\\log n/\\varepsilon^2)\\)."
  },
  {
    "objectID": "projects/OT-applications/index.html",
    "href": "projects/OT-applications/index.html",
    "title": "Hypothetical Applications of Optimal Transport",
    "section": "",
    "text": "Here, I discuss some possible hypothetical applications of the Optimal Transport algorithms.\n\n\n\nOptimal transport theory has been shown countless times to be used in interpolating the transition from one image to another. With such, we can gain a set of intermediary images to develop a seamless transition. Consider the same concept of a seamless transition from one state to another, but regard transitioning from one sound wave to another. This is the idea I would like to describe.\nWith music streaming services such as Spotify, seamless playback is available with a few different methods, but are any of them “really” seamless? The first available method of seamless playback offered by Spotify utilizes a crossfade, wherein the first track fades out (decreases in volume) as the next track fades in (increases in volume). This is not ideal, as the ending of the first track is lost in the introduction of the new track. Despite this, it is still the most popular method to transitioning between sound clips as it removes abrupt ends in most circumstances. Sometimes, however, if two consecutive tracks differ greatly in pitch or the like, crossfading between them will not remove the abruptness of the transition, worsening the listening experience greatly. Also, with crossfade, a reduction in volume is noticable during the transition.\nAnother available transition method offered by Spotify is their “Automix” feature. This is an improvement over the crossfade feature, but is not available on all platforms nor does it actually regard the pitch of the notes from either track. Spotify defines the feature on their website as follows:\n“(Automix) Allows beat-matched seamless transitions between songs on select Spotify playlists. Works even on Shuffle.”\nIt essentially mixes the previous track with the next track at a specific time, determined by some algorithm, in order to maintain seamless beat transitions. But again, this is not actually seamless, as the transition is still quite noticeable by the human ear. And I, for one, absolutely hate that there is not yet a widely available way to have this seamless transition.\nSo, how do we make a transition seamless? With optimal transport theory, obviously! We can use optimal transport to determine the most efficient transportation plan between the ending notes of the previous track and the beginning of the next track. This way, we will not experience any abrupt transitions between two songs. One song will simply “become” the other without a loss in volume or a noticeable gap in pitch! The sounds can be split into frequencies and moved in such a manner to produce a completely different song. Transitioning directly from the notes of one track to that of another with an optimal transport plan will provide us with a set of intermediary sounds in the transition. These intermediary sounds can then be placed between the two tracks to provide fully seamless sound transitions between tracks!\nIn addition to this use case, we can extend this method of seamlessly transitioning between sound clips to many other aspects of sound engineering as well. For instance, DJs constantly shift between tracks on a set list while performing. They would benefit largely from the ability to rid their performances of abrupt song changes. Additionally, music artists could engineer this into music on their albums and/or performances. Film score composers could even use this in movies to further immerse viewers.\n\n\n\n\nFor this next application, I wish to further continue on the topic of optimal transport-based interpolation. The topic I wish to address here, more specifically, is level transitions in 2D-platformer video games. Consider possibly the most well-known platform game ever, Super Mario Bros. From one stage to the next, the user must transition abruptly between levels. This involves many stoppages. Mario first becomes unplayable after riding the flagpole at the end of a stage. Then the user is sent to a black screen detailing the level number and the number of lives left. Then, Mario will return to being playable once the next stage is loaded to the screen. Can we make this level transition seamless? If it were seamless, the game would have a much different and possibly more cohesive playing experience.\nEnter optimal transport theory.\nFirstly, we should define what it means for a transition to be seamless in this case. Visually, a transition would appear seamless, if one stage’s environment morphs into the next stage without abrupt visual stoppages. In the case with Super Mario Bros, visually, the black screen between stages is the only contributing factor to the transition’s abruptness. By simply using optimal transport-based image interpolation between the environment at the end of the previous stage, where Mario is no longer playable, to that of the next stage, we would gain a set of intermediary environments. Then, placing these intermediary environments in place of the black screen would provide us with a visually seamless transition. Next, we can draw the collision boxes around the new sprites and blocks for the next level. Once that is done, we may return control of Mario back to the user.\nBut, I do not believe that this transition would be truly seamless, as Mario is still unactionable during this transition. So, using simple image interpolation would still yield a somewhat “seam-ful” transition between levels. In other words, user-control is disrupted and hence the transition is still not truly seamless. So, let us keep Mario actionable during this transition. If Mario is actionable, however, we must correctly implement the collision boxes of the environment in real time so that the playability of the next level is left unaltered by our different method of transitioning.\nA trivial solution to this follows. Consider, every individual frame in the transition between two stages, using optimal transport-based image interpolation. In the intermediary images, collisionable objects from the first stage will begin to disappear as new collisionable objects from the new stage appear. For each frame, we must redraw collision boxes to fit correctly over the current environment. And, since redrawing collision boxes is possible during playtime, evident when Mario eats a mushroom and gets larger, our solution is plausible. With this, all frames within the transition would be correctly drawn, and thus Mario would be correctly playable for the entire transition.\nAlso, it should be noted that I am assuming a locked viewbox when transitioning between levels. In other words, the level will not “scroll” during this transition. Therefore, we are only interpolating between static images. Static in this sense is only regarding the environment, and not the playable character (Mario). And although this may seem like an oversight, it is a condition which is easily enforced in a manner that leaves the player unaffected. For instance, we can speed up and slow down the transition so that it is physically impossible for Mario to reach a position which initiates the “scrolling” before the next level has been wholly reproduced. Once the transition is over, we can unlock the viewbox and the next level will continue normally.\nFinally, we have reached a point of seamless transitions between levels in a platformer. The same concept can be extended to 3-dimensional games with level transitions as well. It would require some more severe modifications, however. Instead of image interpolation, we could interpolate a transition between surfaces on 3-dimensional mesh objects in the environment, and redraw the collision boxes over the intermediary models. The applications are limitless!\n\n\n\n\nFinally, with this last application of optimal transport theory, I would like to harp on the main goal of optimal transport. That is, the desire to develop a better method of describing the statistic distance between distributions, or how different two distributions are. With utilization of the Wasserstein distance, we can do this with optimal transport theory.\nBegin by considering wave energy. Wave energy, simply put, is a method of producing renewable energy from the motion of ocean waves. Unfortunately, this form of energy production is mostly defunct nowadays. And this is due to the inconsistency of waves and the expensive costs associated with developing and operating such a power plant. The amount of power produced by a wave energy plant is primarily dependent on the sheer size of the wave and the kinetic energy it encompasses. Often, the operational costs of the plant will outweigh the price of the energy it produces, and so the entire venture becomes unmanageable from a financial sense.\nNow, one solution to this problem would be to intermittently run the power plant only at times where the price of the energy produced is greater than the current operational costs. This way, operational costs are not incurred when power output is too low (i.e. the waves are not large enough). But, with this solution, we raise another important question: When exactly should the power plant be run? To put this solution into effect, we would need some way to decide when the ideal times to activate the power plant are. That is where optimal transport theory comes in.\nTo apply optimal transport theory here, there is something we must know first. Assume that we know the optimal wave shape to produce the most energy at any given time. Although time-consuming, finding the optimal wave shape would be plausible. For instance, over the course of a year, researchers could chart waveforms along with the amount of energy produced by the plant. Then, they could assign the optimal waveform as that which produced the most energy in that year. This waveform may differ from year to year, so I will refer to the optimal waveform as the current optimal waveform. With optimal transport, we can determine the Wasserstein distance between two waves, denoting how different the two waves are. At any given time, we are able to compute this distance between the optimal waveform and the current waveform. So, in realtime, we could compute how “different” our current waveform is from the optimal waveform.\nNow, over the course of another year, suppose we charted the Wasserstein distances between the optimal waveform and the current waveform. In addition to this, we would keep track of when the power plant becomes profitable (i.e. the operational costs are outweighed by the energy production). To conceptualize this further, consider the fact that at some point, any given waveform may be too “different” from the optimal waveform to produce enough energy to offset the current operational costs. This is why we keep track of when the plant becomes profitable. With the strategy above, we can determine an upper limit to the Wasserstein distance \\(W\\) such that any distance greater than \\(W\\) causes the plant to become unprofitable. With this upper limit, we can now know at which times we should activate the generator to garner the greatest profit margins. And finally with greater profit margins, running a wave energy plant may finally become a financially manageable venture.\nIt is important to note that I do not know definitively if there is any correlation between the Wasserstein distance between two wave forms and the amount of kinetic energy these waves contain. For example, does a large Wasserstein distance between some waveform and the optimal waveform mean that the two waveforms contain largely different amounts of kinetic energy? I am not sure. I do, however, know that the kinetic energy of a wave is directly correlated with a wave’s amplitude, frequency, and wavelength. And all such factors which contribute to a wave’s kinetic energy are also influential to its shape. I just am not educated enough on this topic to definitively understand whether variations in frequency and wavelength contribute largely to a variation in Wasserstein distance. Or even if ocean waves vary enough in frequency and wavelength to consider these values at all."
  },
  {
    "objectID": "projects/OT-applications/index.html#seamless-music-transitions",
    "href": "projects/OT-applications/index.html#seamless-music-transitions",
    "title": "Hypothetical Applications of Optimal Transport",
    "section": "",
    "text": "Optimal transport theory has been shown countless times to be used in interpolating the transition from one image to another. With such, we can gain a set of intermediary images to develop a seamless transition. Consider the same concept of a seamless transition from one state to another, but regard transitioning from one sound wave to another. This is the idea I would like to describe.\nWith music streaming services such as Spotify, seamless playback is available with a few different methods, but are any of them “really” seamless? The first available method of seamless playback offered by Spotify utilizes a crossfade, wherein the first track fades out (decreases in volume) as the next track fades in (increases in volume). This is not ideal, as the ending of the first track is lost in the introduction of the new track. Despite this, it is still the most popular method to transitioning between sound clips as it removes abrupt ends in most circumstances. Sometimes, however, if two consecutive tracks differ greatly in pitch or the like, crossfading between them will not remove the abruptness of the transition, worsening the listening experience greatly. Also, with crossfade, a reduction in volume is noticable during the transition.\nAnother available transition method offered by Spotify is their “Automix” feature. This is an improvement over the crossfade feature, but is not available on all platforms nor does it actually regard the pitch of the notes from either track. Spotify defines the feature on their website as follows:\n“(Automix) Allows beat-matched seamless transitions between songs on select Spotify playlists. Works even on Shuffle.”\nIt essentially mixes the previous track with the next track at a specific time, determined by some algorithm, in order to maintain seamless beat transitions. But again, this is not actually seamless, as the transition is still quite noticeable by the human ear. And I, for one, absolutely hate that there is not yet a widely available way to have this seamless transition.\nSo, how do we make a transition seamless? With optimal transport theory, obviously! We can use optimal transport to determine the most efficient transportation plan between the ending notes of the previous track and the beginning of the next track. This way, we will not experience any abrupt transitions between two songs. One song will simply “become” the other without a loss in volume or a noticeable gap in pitch! The sounds can be split into frequencies and moved in such a manner to produce a completely different song. Transitioning directly from the notes of one track to that of another with an optimal transport plan will provide us with a set of intermediary sounds in the transition. These intermediary sounds can then be placed between the two tracks to provide fully seamless sound transitions between tracks!\nIn addition to this use case, we can extend this method of seamlessly transitioning between sound clips to many other aspects of sound engineering as well. For instance, DJs constantly shift between tracks on a set list while performing. They would benefit largely from the ability to rid their performances of abrupt song changes. Additionally, music artists could engineer this into music on their albums and/or performances. Film score composers could even use this in movies to further immerse viewers."
  },
  {
    "objectID": "projects/OT-applications/index.html#game-level-transitions",
    "href": "projects/OT-applications/index.html#game-level-transitions",
    "title": "Hypothetical Applications of Optimal Transport",
    "section": "",
    "text": "For this next application, I wish to further continue on the topic of optimal transport-based interpolation. The topic I wish to address here, more specifically, is level transitions in 2D-platformer video games. Consider possibly the most well-known platform game ever, Super Mario Bros. From one stage to the next, the user must transition abruptly between levels. This involves many stoppages. Mario first becomes unplayable after riding the flagpole at the end of a stage. Then the user is sent to a black screen detailing the level number and the number of lives left. Then, Mario will return to being playable once the next stage is loaded to the screen. Can we make this level transition seamless? If it were seamless, the game would have a much different and possibly more cohesive playing experience.\nEnter optimal transport theory.\nFirstly, we should define what it means for a transition to be seamless in this case. Visually, a transition would appear seamless, if one stage’s environment morphs into the next stage without abrupt visual stoppages. In the case with Super Mario Bros, visually, the black screen between stages is the only contributing factor to the transition’s abruptness. By simply using optimal transport-based image interpolation between the environment at the end of the previous stage, where Mario is no longer playable, to that of the next stage, we would gain a set of intermediary environments. Then, placing these intermediary environments in place of the black screen would provide us with a visually seamless transition. Next, we can draw the collision boxes around the new sprites and blocks for the next level. Once that is done, we may return control of Mario back to the user.\nBut, I do not believe that this transition would be truly seamless, as Mario is still unactionable during this transition. So, using simple image interpolation would still yield a somewhat “seam-ful” transition between levels. In other words, user-control is disrupted and hence the transition is still not truly seamless. So, let us keep Mario actionable during this transition. If Mario is actionable, however, we must correctly implement the collision boxes of the environment in real time so that the playability of the next level is left unaltered by our different method of transitioning.\nA trivial solution to this follows. Consider, every individual frame in the transition between two stages, using optimal transport-based image interpolation. In the intermediary images, collisionable objects from the first stage will begin to disappear as new collisionable objects from the new stage appear. For each frame, we must redraw collision boxes to fit correctly over the current environment. And, since redrawing collision boxes is possible during playtime, evident when Mario eats a mushroom and gets larger, our solution is plausible. With this, all frames within the transition would be correctly drawn, and thus Mario would be correctly playable for the entire transition.\nAlso, it should be noted that I am assuming a locked viewbox when transitioning between levels. In other words, the level will not “scroll” during this transition. Therefore, we are only interpolating between static images. Static in this sense is only regarding the environment, and not the playable character (Mario). And although this may seem like an oversight, it is a condition which is easily enforced in a manner that leaves the player unaffected. For instance, we can speed up and slow down the transition so that it is physically impossible for Mario to reach a position which initiates the “scrolling” before the next level has been wholly reproduced. Once the transition is over, we can unlock the viewbox and the next level will continue normally.\nFinally, we have reached a point of seamless transitions between levels in a platformer. The same concept can be extended to 3-dimensional games with level transitions as well. It would require some more severe modifications, however. Instead of image interpolation, we could interpolate a transition between surfaces on 3-dimensional mesh objects in the environment, and redraw the collision boxes over the intermediary models. The applications are limitless!"
  },
  {
    "objectID": "projects/OT-applications/index.html#wave-shape-analysis-for-tidal-power-estimation",
    "href": "projects/OT-applications/index.html#wave-shape-analysis-for-tidal-power-estimation",
    "title": "Hypothetical Applications of Optimal Transport",
    "section": "",
    "text": "Finally, with this last application of optimal transport theory, I would like to harp on the main goal of optimal transport. That is, the desire to develop a better method of describing the statistic distance between distributions, or how different two distributions are. With utilization of the Wasserstein distance, we can do this with optimal transport theory.\nBegin by considering wave energy. Wave energy, simply put, is a method of producing renewable energy from the motion of ocean waves. Unfortunately, this form of energy production is mostly defunct nowadays. And this is due to the inconsistency of waves and the expensive costs associated with developing and operating such a power plant. The amount of power produced by a wave energy plant is primarily dependent on the sheer size of the wave and the kinetic energy it encompasses. Often, the operational costs of the plant will outweigh the price of the energy it produces, and so the entire venture becomes unmanageable from a financial sense.\nNow, one solution to this problem would be to intermittently run the power plant only at times where the price of the energy produced is greater than the current operational costs. This way, operational costs are not incurred when power output is too low (i.e. the waves are not large enough). But, with this solution, we raise another important question: When exactly should the power plant be run? To put this solution into effect, we would need some way to decide when the ideal times to activate the power plant are. That is where optimal transport theory comes in.\nTo apply optimal transport theory here, there is something we must know first. Assume that we know the optimal wave shape to produce the most energy at any given time. Although time-consuming, finding the optimal wave shape would be plausible. For instance, over the course of a year, researchers could chart waveforms along with the amount of energy produced by the plant. Then, they could assign the optimal waveform as that which produced the most energy in that year. This waveform may differ from year to year, so I will refer to the optimal waveform as the current optimal waveform. With optimal transport, we can determine the Wasserstein distance between two waves, denoting how different the two waves are. At any given time, we are able to compute this distance between the optimal waveform and the current waveform. So, in realtime, we could compute how “different” our current waveform is from the optimal waveform.\nNow, over the course of another year, suppose we charted the Wasserstein distances between the optimal waveform and the current waveform. In addition to this, we would keep track of when the power plant becomes profitable (i.e. the operational costs are outweighed by the energy production). To conceptualize this further, consider the fact that at some point, any given waveform may be too “different” from the optimal waveform to produce enough energy to offset the current operational costs. This is why we keep track of when the plant becomes profitable. With the strategy above, we can determine an upper limit to the Wasserstein distance \\(W\\) such that any distance greater than \\(W\\) causes the plant to become unprofitable. With this upper limit, we can now know at which times we should activate the generator to garner the greatest profit margins. And finally with greater profit margins, running a wave energy plant may finally become a financially manageable venture.\nIt is important to note that I do not know definitively if there is any correlation between the Wasserstein distance between two wave forms and the amount of kinetic energy these waves contain. For example, does a large Wasserstein distance between some waveform and the optimal waveform mean that the two waveforms contain largely different amounts of kinetic energy? I am not sure. I do, however, know that the kinetic energy of a wave is directly correlated with a wave’s amplitude, frequency, and wavelength. And all such factors which contribute to a wave’s kinetic energy are also influential to its shape. I just am not educated enough on this topic to definitively understand whether variations in frequency and wavelength contribute largely to a variation in Wasserstein distance. Or even if ocean waves vary enough in frequency and wavelength to consider these values at all."
  },
  {
    "objectID": "projects/clustering-ceramics/index.html",
    "href": "projects/clustering-ceramics/index.html",
    "title": "Exploring Ceramic Composition with K-Means Clustering",
    "section": "",
    "text": "Spanning centuries of craftsmanship and art, the world of ceramics provides us with unique insights into artisans of the past. Today, we will delve into the realm of historic pottery samples, seeking to unravel the latent structures within the chemical compositions of their glazes and bodies.\nCeramic materials embody a diverse array of molecules, each contributing something unique to the look and feel of a finished product. Traditional methods of identifying ceramic samples would often rely heavily on expert knowledge and visual inspection, but in the modern era of data-driven research, the potential to uncover hidden patterns through computational methods becomes increasingly promising.\nFor this project, we will be using a dataset, available from the UCI Machine Learning Repository, titled “Chemical Composition of Ceramic Samples”. This dataset details 88 total samples of ceramic pieces from the the Longquan and Jingdezhen kilns. The samples are of one of the earliest types of porcelain from ancient China. Dating back thousands of years, the Longquan (and later the Jingdezhen) kiln became a hot-spot for celadon production. During the Ming Dynasty, the Chinese government invested heavily into these kilns to promote the craftsmanship and trade of this precious product. These samples of celadon were examined by an energy dispersive X-ray fluorescence microprobe to detect the chemical compositions of the bodies and glazes. The percentage composition (either by weight, or ppm) of the compounds identified in the samples are the features of our dataset. Each sample is also categorized as being part of the body or the glaze.\nThe .csv file we will be using can be found here.\nThroughout this post, we will apply the classic K-Means clustering algorithm as a tool for discerning underlying patterns within our dataset and leverage the algorithm’s ability to identify clusters for our ceramic sample types. We hope to distinguish pieces of the body from pieces of the glaze.\n\nUnsupervised Learning\nK-Means is a popular unsupervised learning algorithm, but why is it ‘unsupervised’? In machine learning, there exist many different types of learning tasks. Some of these include Unsupervised, Semi-Supervised, and Supervised learning. Unsupervised learning tasks involve exploring and understanding the inherent patterns in a dataset without utilizing the labels. Instead of trying to draw connections between the features and the labels, unsupervised techniques try to unveil the hidden structure of the features without any regard to the labels.\nToday, we will use unsupervised learning for clustering. The K-Means clustering algorithm aims to group similar data points together based on feature similarities. This will allow us to separate the dataset into distinct clusters, where items within a cluster are more similar to each other than those in other clusters. For our project, this means evaluating the similarities and differences in the chemical compositions of ceramic samples in order to group them into clusters.\n\n\nK-Means\nSo, how does K-Means do this? The K-Means algorithm will begin by randomly selecting K initial cluster centroids from the dataset. It will then assign each data point to the cluster whose centroid is the closest. Normally, the Euclidean distance is used to evaluate the closeness of data points. The centroid of the clusters are then recomputed as the mean of all data points within the given cluster. The assignment and centroid computations iteratively repeat until the model converges and these steps no longer change the assignments between iterations. Once convergence is reached, all data points will be clustered into exactly K clusters.\nThe graphic below details a bit of what we are looking for when using K-Means. \n\n\nImports\nWith the background information covered, let’s start off as most projects do and work through the imports we will need. First off, we will need pandas and numpy to work with our data, as well as seaborn and matplotlib for some visualizations. Finally, we will be using sklearn for the KMeans clustering model and some data preprocessing. The PCA module is used at the end to aid in some visualizations as well.\n\n# Imports\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport sklearn as sk\nfrom sklearn.cluster import KMeans\nfrom sklearn.decomposition import PCA\n\nWith the packages imported, we can now read in the data to take a look at it\n\n# Read in data\ndata = pd.read_csv(\"./data/Chemical Composion of Ceramic.csv\")\ndata.head()\n\n\n\n\n\n\n\n\nCeramic Name\nPart\nNa2O\nMgO\nAl2O3\nSiO2\nK2O\nCaO\nTiO2\nFe2O3\nMnO\nCuO\nZnO\nPbO2\nRb2O\nSrO\nY2O3\nZrO2\nP2O5\n\n\n\n\n0\nFLQ-1-b\nBody\n0.62\n0.38\n19.61\n71.99\n4.84\n0.31\n0.07\n1.18\n630\n10\n70\n10\n430\n0\n40\n80\n90\n\n\n1\nFLQ-2-b\nBody\n0.57\n0.47\n21.19\n70.09\n4.98\n0.49\n0.09\n1.12\n380\n20\n80\n40\n430\n-10\n40\n100\n110\n\n\n2\nFLQ-3-b\nBody\n0.49\n0.19\n18.60\n74.70\n3.47\n0.43\n0.06\n1.07\n420\n20\n50\n50\n380\n40\n40\n80\n200\n\n\n3\nFLQ-4-b\nBody\n0.89\n0.30\n18.01\n74.19\n4.01\n0.27\n0.09\n1.23\n460\n20\n70\n60\n380\n10\n40\n70\n210\n\n\n4\nFLQ-5-b\nBody\n0.03\n0.36\n18.41\n73.99\n4.33\n0.65\n0.05\n1.19\n380\n40\n90\n40\n360\n10\n30\n80\n150\n\n\n\n\n\n\n\nUsing the head() function from pandas, we can get an idea of how our dataframe looks. We can see that we have categorical variables for ‘Ceramic Name’ which describes the sample, as well as ‘Part’ which tells us what part of the celadon which the sample comes from (either from the body or from the glaze). The next 17 columns describe the percentages of molecules found in the sample and are real-valued. The molecules along with the measurement type (percentage composition by weight or ppm) are:\n\nNa2O (wt%)\nMgO (wt%)\nAl2O3 (wt%)\nSiO2 (wt%)\nK2O (wt%)\nCaO (wt%)\nTiO2 (wt%)\nFe2O3 (wt%)\nMnO (ppm)\nCuO (ppm)\nZnO (ppm)\nPbO2 (ppm)\nRb2O (ppm)\nSrO (ppm)\nY2O3 (ppm)\nZrO2 (ppm)\nP2O5 (ppm)\n\n\n\nData Preprocessing\nSo, we can see that our real-valued features are of two different scales (this will come into play later). For our task, the ‘Ceramic Name’ feature is simply unneeded, so we can just drop it from our set. Also, since ‘Part’ is categorical, lets encode it as a binary feature, where 1 indicates ‘Body’ and 0 indicates ‘Glaze’.\n\n# Encode categorical feature\ndata['Part'] = data['Part'].map({'Body':1, 'Glaze':0})\n\n# Drop unneeded 'Ceramic Name'\ndata = data.drop(['Ceramic Name'], axis=1)\ndata.head()\n\n\n\n\n\n\n\n\nPart\nNa2O\nMgO\nAl2O3\nSiO2\nK2O\nCaO\nTiO2\nFe2O3\nMnO\nCuO\nZnO\nPbO2\nRb2O\nSrO\nY2O3\nZrO2\nP2O5\n\n\n\n\n0\n1\n0.62\n0.38\n19.61\n71.99\n4.84\n0.31\n0.07\n1.18\n630\n10\n70\n10\n430\n0\n40\n80\n90\n\n\n1\n1\n0.57\n0.47\n21.19\n70.09\n4.98\n0.49\n0.09\n1.12\n380\n20\n80\n40\n430\n-10\n40\n100\n110\n\n\n2\n1\n0.49\n0.19\n18.60\n74.70\n3.47\n0.43\n0.06\n1.07\n420\n20\n50\n50\n380\n40\n40\n80\n200\n\n\n3\n1\n0.89\n0.30\n18.01\n74.19\n4.01\n0.27\n0.09\n1.23\n460\n20\n70\n60\n380\n10\n40\n70\n210\n\n\n4\n1\n0.03\n0.36\n18.41\n73.99\n4.33\n0.65\n0.05\n1.19\n380\n40\n90\n40\n360\n10\n30\n80\n150\n\n\n\n\n\n\n\nGreat! Now, we can pull out the ‘Part’ column from the set since our task is unsupervised.\n\n# Store labels separately\nlabels = data['Part'].copy()\n\n# Drop labels from data\ndata = data.drop(['Part'], axis=1)\n\nlabels\n\n0     1\n1     1\n2     1\n3     1\n4     1\n     ..\n83    0\n84    0\n85    0\n86    0\n87    0\nName: Part, Length: 88, dtype: int64\n\n\nSince our features are scaled differently, and the euclidean distance metric will be heavily affected by this, we can scale our data equally using sklearn’s MinMaxScaler().\n\nscaler = sk.preprocessing.MinMaxScaler()\nscaled_features = scaler.fit_transform(data.values)\n\nscaled_features\n\narray([[0.31891892, 0.248     , 0.54743083, ..., 0.33333333, 0.08823529,\n        0.02564103],\n       [0.29189189, 0.32      , 0.65151515, ..., 0.33333333, 0.14705882,\n        0.03846154],\n       [0.24864865, 0.096     , 0.48089592, ..., 0.33333333, 0.08823529,\n        0.09615385],\n       ...,\n       [0.10810811, 0.136     , 0.1113307 , ..., 0.33333333, 0.20588235,\n        0.27564103],\n       [0.05945946, 0.312     , 0.08695652, ..., 0.33333333, 0.23529412,\n        0.67307692],\n       [0.05945946, 0.448     , 0.19433465, ..., 0.33333333, 0.17647059,\n        0.41025641]])\n\n\nOur features are now scaled properly for the model. To finish up, we can set X (features) and y (labels).\n\nX = scaled_features\ny = labels.values\n\n\n\nApplying K-Means Clustering\nWith the processing now done, we can fit our clustering algorithm to the data. For this, we will first need to create the K-Means model from sklearn. To instantiate our model, we will need to provide it with some parameters first.\nRemember how K-Means clusters data points into K distinct clusters? Well, we now have to tell it how many clusters we want it to find. Our goal is to try and cluster the ceramic samples by type (body or glaze), so we are going to tell K-Means to find 2 clusters in our dataset with the n_clusters parameter. Also, we will define n_init to tell the model how many times to run the algorithm with different initial centroids. The best model from all the runs will be the model we get out. A standard K-Means algorithm uses random initial centroids, so we may get different results across runs. To change this a bit, we will set init to change the centroid initialization method. Instead of random, we can use the k-means++ method which selects centroids with sampling based on an empirical probability distribution of each points contribution to the overall inertia. In K-Means clustering, inertia is a metric used to assess how well the data points within a cluster are grouped around their centroid. Inertia is defined as the sum of the squared distances between each data point in a cluster and the centroid of that cluster. In sklearn, the k-means++ method is actually greedy k-means++ which makes several trials at each sampling step and choosing the best centroid. Finally, we set max_iter to an arbitrarily high value, giving the model time to converge. verbose is set to 1 so that we get some outputs when we fit the model.\n\n# Create the K-Means Model\nkmeans_model = KMeans(n_clusters = 2,\n                      n_init = 'auto',\n                      init = 'k-means++',\n                      max_iter = 500,\n                      verbose = 1)\n\nNext, we can fit our model. We will use the model’s fit() function to fit it. We must provide the features to the fit method for the model to use when training. This is easy. Just put X inside the fit call.\nAfter the model finished fitting, we can get the cluster assignments through the model’s labels_ property. The underscore at the end indicates that this is a learned property which only has meaning after the model has been fit.\n\n# Fit the model\nkmeans_model.fit(X)\n\n# Get cluster assignments for data\nclusters = kmeans_model.labels_\nclusters\n\nInitialization complete\nIteration 0, inertia 84.44756352990454.\nIteration 1, inertia 50.70926171516276.\nConverged at iteration 1: strict convergence.\n\n\narray([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1,\n       1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1])\n\n\nAfter the model is fitted, we get the cluster assignments. From the output above, we can see what those assignments are. Also, we can see how the inertia was minimized by the algorithm and thus decreases in successive iterations.\nHere, it is important to note that these cluster assignments have no tie to the original labels. In other words, a cluster assignment label of 1 does not necessarily mean that the data point is part of the body of a ceramic sample. These cluster assignments are simply what the K-Means model thinks is correct after it has been trained. The data points in cluster 1 are just the most similar (by way of euclidean distance) to other data points within its own cluster, and dissimilar from those in cluster 0.\n\n\nEvaluation with Visualizations\nTo get an idea of what the clusters assignments mean, we can plot the data points and color them by their cluster assignment. Then, we can compare that to a plot of the data points colored by their true label (body or glaze) and see if K-Means was able to cluster the points together by ceramic sample type. To do this, however, we need to reduce the dimensionality of our feature-set to something that we can visually comprehend. The easiest way to do this is to squish the features down from 17 dimensions down to 2. In 2 dimensions, we can simply plot it on a 2D coordinate plane. How can we do this?\nThis is where principal component analysis (PCA) comes in. PCA tries to identify a new set of axes (components) which captures the most variation in the data. This allows us to reduce dimensionality while preserving the most essential characteristics of the input data. To use PCA, we can just create a PCA() object and tell it to reduce to 2 components (for a 2D visualization). Then, we can call the fit_transform() method with X to get out out our 2 principal components.\n\n# Create PCA\npca = PCA(n_components=2)\n\n# Fit PCA and transform X into principal components\ncomponents = pca.fit_transform(X)\n\nNow, we can build our plot using the first component as x and the second component as y. Additionally, we can add some styling to make our plot visually pleasing.\n\n# Get 2 Principal Components\ncomponent_1 = components[:, 0]\ncomponent_2 = components[:, 1]\n\n# Create dataframe for plot\ncluster_data = pd.DataFrame({'Principal Component 1' : component_1,\n                             'Principal Component 2' : component_2,\n                             'Clusters' : clusters})\n\n\n# Set sns styling\nsns.set(style='darkgrid',\n        font_scale = 1.4)\n\n\n# Visualize with scatterplot\nplt.figure(figsize=(10,8), dpi=300)\nsns.scatterplot(data = cluster_data,\n                x = 'Principal Component 1',\n                y = 'Principal Component 2',\n                hue = 'Clusters',\n                style = 'Clusters',\n                palette='pastel',\n                s = 150,\n                markers=['o', '^'])\n\n# Add plot styling\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.title('Principal Components Colored By Cluster Assignment')\nplt.legend(loc = 'lower right', title = 'Cluster')\nplt.show()\n\n\n\n\nNext, do the same plot as above, but instead of cluster assignments, we will use the actual labels from the original dataset.\n\n# Create dataframe for plot\nlabel_data = pd.DataFrame({'Principal Component 1' : component_1,\n                           'Principal Component 2' : component_2,\n                           'Part' : labels})\n\n\n# Set sns styling\nsns.set(style='darkgrid',\n        font_scale = 1.4)\n\n\n# Visualize with scatterplot\nplt.figure(figsize=(10,8), dpi=300)\nsns.scatterplot(data = label_data,\n                x = 'Principal Component 1',\n                y = 'Principal Component 2',\n                hue = 'Part',\n                style = 'Part',\n                palette='pastel',\n                s = 150,\n                markers=['o', '^'])\n\n# Add plot styling\nplt.xlabel('Principal Component 1')\nplt.ylabel('Principal Component 2')\nplt.title('Principal Components Colored By Original Label')\nplt.legend(loc = 'lower right', title = 'Part', labels=['Body', 'Glaze'])\nplt.show()\n\n\n\n\nWoah, those look exactly the same. This means that our clustering algorithm worked quite well. We can see that K-Means perfectly split up our samples into one cluster for body samples and one cluster for glaze samples just by chemical composition.\nWith these plots, we can also conclude that cluster 0 maps directly to glaze parts, and cluster 1 maps directly to body parts. And, each sample was clustered correctly with respect to the ceramic sample type.\n\n\nVisualizing K-Means Iterations\nOur project is pretty much done at this point, but let’s take a second to try and understand what K-Means is doing behind the scenes. To do this, we will try and manually iterate the K-Means algorithm.\nHere, we set a for loop to run 5 iterations. For each iteration, we will create a new K-Means model and only do a single run, by setting n_init to 1 and only having it do a single iteration with max_iter set to one. We will then fit the model, store the centroids for the next for loop iteration, save the model to the models array, and print the inertia for each iteration. init will be set to k-means++ initially, and then set to the centroids from the previous iteration for successive loops. The random state is set in order to make this reproducible for any readers.\n\niterations = 5\ncentroids = None\nmodels = []\n\nfor i in range(iterations):\n    # Set centroids to previous iteration's centroids if they exits\n    # If this is the first iteration, then set it to k-means++\n    p_centroids = centroids\n    if (p_centroids is None): p_centroids = 'k-means++'\n        \n    # Build model\n    kmeans_model = KMeans(n_clusters = 2,\n                          n_init = 1,\n                          init = p_centroids,\n                          max_iter = 1,\n                          random_state=8)\n    \n    # Fit model\n    kmeans_model.fit(X)\n    \n    # Store centroids for next iter\n    centroids = kmeans_model.cluster_centers_\n    \n    # Save model\n    models.append(kmeans_model)\n    \n    # Print inertia\n    print(f'Iteration {i}, intertia {kmeans_model.inertia_}.')\n\nIteration 0, intertia 80.69248216214388.\nIteration 1, intertia 67.91899805018879.\nIteration 2, intertia 52.70484611171963.\nIteration 3, intertia 50.868247072045826.\nIteration 4, intertia 50.70926171516276.\n\n\nNow, for each saved model, we want to make the same plots as before to see how the clusters may change over iterations. We will make use of subplots for this. We simply pull label clusters for each model, and then plot them on scatterplots. We will also plot the centroids to see how they move around.\n\n# Create figure and subplot axes\nfig, axes = plt.subplots(1, 5, figsize=(20, 5), dpi=300, sharey = True)\n\nfor idx, (model, ax) in enumerate(zip(models, axes), start=1):\n    # Create dataframe for plot\n    cluster_data = pd.DataFrame({'Principal Component 1' : component_1,\n                                 'Principal Component 2' : component_2,\n                                 'Cluster' : model.labels_})\n    \n    # Make scatterplot\n    sns.set(style='darkgrid',\n        font_scale = 1.4)\n\n    sns.scatterplot(ax = ax,\n                    data = cluster_data,\n                    x = 'Principal Component 1',\n                    y = 'Principal Component 2',\n                    hue = 'Cluster',\n                    style = 'Cluster',\n                    palette='pastel',\n                    s = 150,\n                    markers=['o', '^'])\n    \n    ax.set_title(f'Iteration {idx-1}')\n    ax.set_xlabel('PC1')\n    ax.set_ylabel('PC2')\n    ax.legend(loc = 'lower right', title = 'Cluster')\n    if i != 1:\n        ax.legend().set_visible(False)\n    \n    \n    # Plot Centroids\n    centroids = pca.transform(model.cluster_centers_)\n    ax.scatter(centroids[:, 0],\n               centroids[:, 1],\n               c='black',\n               marker='X',\n               s=50,\n               label='Centroids')\n\nfig.suptitle('Cluster Assignments for Each K-Means Iteration')\nplt.tight_layout()\nplt.show()\n\n\n\n\nWith these plots done, we can take a look at how the centroids slowly move towards each distinct grouping in the dataset. The centroids are marked on each plot with a black X.\nIn the iteration 0, the algorithm does not really understand what is going on. It has tried to set initial centroids with the k-means++ sampling method, but those centroids are far from perfect. No worthwhile clustering exists. We can see circles and triangles all mixed up since the algorithm has yet to figure out the best clusters yet. As the we move to next iterations, centroids are re-computed and data points are re-assigned to the nearest centroids. The centroids move further and further towards the left and right (where the groups of body and glaze points exist). At around iteration 2, we are nearing the optimal solution and so the centroids no longer have to move very much. Then, after iteration 3, we seem to have found an optimal solution. After this, no further re-assignments are required. This is why iterations 3 and 4 look exactly the same.\n\n\nConclusion\nConclusively, K-Means has proved to be quite a powerful clustering algorithm, allowing us to cluster ceramic samples by just chemical composition alone. With this new-found knowledge of unsupervised machine learning techniques, we have gained extremely valuable insights into the inherent patterns and structures present within our dataset. The K-Means model’s ability to automatically group our data points based on shared characteristics regarding chemical composition has not only allowed us to identify distinct body and glaze pieces, but also paved the way for further exploration into the field of unsupervised learning.\nWith our project, we have shown how machine learning techniques can be applied in fields like archaeology and material sciences. But, machine learning can do so much more! The insights gained here has assisted us in clustering pottery samples, but can also be applied into almost any field of work imaginable. As machine learning continues to develop and we begin integrating newer and more advanced techniques for various tasks, we will all look back on these foundational concepts with appreciation. For now, I will sign off, but for those still around, I hope you learned something and I am excited for whatever the future has in store for you.\n{\\__/}\n( • . •)\n/ &gt; &gt;"
  },
  {
    "objectID": "projects/anomalyDetection-malware/index.html",
    "href": "projects/anomalyDetection-malware/index.html",
    "title": "Unveiling Malicious Software with Anomaly Detection",
    "section": "",
    "text": "In the ever-changing field of cybersecurity, the persistent challenge of malware detection continues to demand more sophisticated approaches day by day. Today, we will dive into an implementation of Density-Based Spatial Clustering of Applications with Noise (DBSCAN) for anomaly detection as a robust tool for identifying malicious software.\nIn order to demonstrate the efficacy of DBSCAN in real-world scenarios, we will turn our attention to the TUNADROMD dataset. Comprising 4,465 instances and 241 features, this dataset details a collection of Android software samples. Every sample is classified as either ‘malware’ or ‘goodware’, with features 1-214 describing android permissions and 214-241 describe android-specific API calls such as calls to getCellLocation() from TelephonyManager. All features are binary values and thus categorical in nature. This dataset was originally published by Parthajit Borah, DK Bhattacharyya, and J. Kalita in 2020. Please refer to the link below for the original article:\nMalware Dataset Generation and Evaluation\nBy Parthajit Borah, DK Bhattacharyya, J. Kalita. 2020\nPublished in Conference Information and Communication Technology\nThis dataset is also publicly available via the UCI Machine Learning Repository here.\nUsing this data, our task is to leverage DBSCAN’s outlier detection capabilities to identify instances of malware. From the permissions requested by the application along with some API calls, we will try to unveil the hidden patterns which tell us whether a given android software is malware or not.\n\nImports\nAs usual with most projects like this, let’s begin with where every and any machine learning project stems from… the imports. Throughout this project, we will be utilizing pandas and numpy for our general data-wrangling needs. Additionally, we will make use of a personal favourite, seaborn, along with the tried and true matplotlib for any visualizations. Finally, we will make use of gower (for easily computing gower’s distances for our categorical features) and sklearn for models and metrics.\n\n# Imports\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport matplotlib.pyplot as plt\nimport gower\nimport sklearn as sk\nfrom sklearn.cluster import DBSCAN\nfrom sklearn.manifold import TSNE\nfrom sklearn.neighbors import NearestNeighbors\n\nWith the imports tidied away, we will first define a small helper function to print out the unique values, along with their respective counts, from an array. Here, we simply use np.unique() to do all the hard work for us, then print out the return values.\n\n# Define function for printing unique values of array along with counts\ndef print_unique_with_counts(arr):\n    # Get unique values and their counts\n    unique_values, counts = np.unique(arr, return_counts=True)\n\n    # Print unique values and their counts\n    for value, count in zip(unique_values, counts):\n        print(f\"Value: {value}, Count: {count}\")\n\n\n\nData Preprocessing\nGreat. Let’s go ahead and load in our data to take a look at it.\n\n# Load data\ndata = pd.read_csv('./data/TUANDROMD.csv')\ndata\n\n\n\n\n\n\n\n\nACCESS_ALL_DOWNLOADS\nACCESS_CACHE_FILESYSTEM\nACCESS_CHECKIN_PROPERTIES\nACCESS_COARSE_LOCATION\nACCESS_COARSE_UPDATES\nACCESS_FINE_LOCATION\nACCESS_LOCATION_EXTRA_COMMANDS\nACCESS_MOCK_LOCATION\nACCESS_MTK_MMHW\nACCESS_NETWORK_STATE\n...\nLandroid/telephony/TelephonyManager;-&gt;getLine1Number\nLandroid/telephony/TelephonyManager;-&gt;getNetworkOperator\nLandroid/telephony/TelephonyManager;-&gt;getNetworkOperatorName\nLandroid/telephony/TelephonyManager;-&gt;getNetworkCountryIso\nLandroid/telephony/TelephonyManager;-&gt;getSimOperator\nLandroid/telephony/TelephonyManager;-&gt;getSimOperatorName\nLandroid/telephony/TelephonyManager;-&gt;getSimCountryIso\nLandroid/telephony/TelephonyManager;-&gt;getSimSerialNumber\nLorg/apache/http/impl/client/DefaultHttpClient;-&gt;execute\nLabel\n\n\n\n\n0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n...\n1.0\n1.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n1.0\n\n\n1\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n...\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n1.0\n\n\n2\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n3\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n1.0\n1.0\n1.0\n1.0\n0.0\n1.0\n0.0\n0.0\n1.0\n\n\n4\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n4460\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n1.0\n1.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n\n\n4461\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n...\n0.0\n1.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n4462\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n4463\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n4464\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n\n\n4465 rows × 242 columns\n\n\n\nOur data, as aforementioned, contains 4464 samples and 241 features along with a target column. 1 in the target column is malware, and 0 is goodware.\nAs you can see, all of the features are binary valued. For permission-based features, a value of 1 indicates that the software requires that permission. For API-based features, a value of 1 means that the software makes that function call.\nNow, we can see whether or not we will have to deal with any missing values in our dataset. For this purpose, pandas’ isna() and python’s any() function come in quite handy.\n\n# Check for missing values\nhas_na = data.isna().any().any()\n\nprint(\"Data contains missing values. :(\" if has_na\n      else \"Data does not have any missing values. :)\")\n\nData contains missing values. :(\n\n\nOh no. It appears that the dataset has some missing values.\nTo investigate further, we will check to see how many rows in the dataframe contain missing values.\n\nprint(f'{data.isna().any(axis=1).sum()} rows contain missing values.')\n\n1 rows contain missing values.\n\n\nFortunately, only 1 row has missing values, so we can just drop it from the dataframe and forget about it. The code cell below does just that.\n\ndata = data.dropna()\ndata\n\n\n\n\n\n\n\n\nACCESS_ALL_DOWNLOADS\nACCESS_CACHE_FILESYSTEM\nACCESS_CHECKIN_PROPERTIES\nACCESS_COARSE_LOCATION\nACCESS_COARSE_UPDATES\nACCESS_FINE_LOCATION\nACCESS_LOCATION_EXTRA_COMMANDS\nACCESS_MOCK_LOCATION\nACCESS_MTK_MMHW\nACCESS_NETWORK_STATE\n...\nLandroid/telephony/TelephonyManager;-&gt;getLine1Number\nLandroid/telephony/TelephonyManager;-&gt;getNetworkOperator\nLandroid/telephony/TelephonyManager;-&gt;getNetworkOperatorName\nLandroid/telephony/TelephonyManager;-&gt;getNetworkCountryIso\nLandroid/telephony/TelephonyManager;-&gt;getSimOperator\nLandroid/telephony/TelephonyManager;-&gt;getSimOperatorName\nLandroid/telephony/TelephonyManager;-&gt;getSimCountryIso\nLandroid/telephony/TelephonyManager;-&gt;getSimSerialNumber\nLorg/apache/http/impl/client/DefaultHttpClient;-&gt;execute\nLabel\n\n\n\n\n0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n...\n1.0\n1.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n1.0\n\n\n1\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n...\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n1.0\n0.0\n0.0\n1.0\n\n\n2\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n3\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n1.0\n1.0\n1.0\n1.0\n0.0\n1.0\n0.0\n0.0\n1.0\n\n\n4\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n4460\n0.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n1.0\n1.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n0.0\n0.0\n\n\n4461\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n...\n0.0\n1.0\n1.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n4462\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n4463\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n4464\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n1.0\n...\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n0.0\n\n\n\n\n4464 rows × 242 columns\n\n\n\nNext, we can set our X (features) and y (labels) with the pandas iloc. Also, we can print out the class counts with the function we defined earlier.\n\n# Get X (features) and y (labels)\nX = data.iloc[:, :-1].values\ny = data.iloc[:, -1].values\n\n\n# Print class counts\nprint_unique_with_counts(y)\n\nValue: 0.0, Count: 899\nValue: 1.0, Count: 3565\n\n\nIt looks like we have a little bit of a class imbalance. Our dataset contains almost 80% malware samples and only 20% goodware samples. Since our dataset is mostly comprised of examples of malware, we will try and use DBSCAN to cluster the malware samples and treat goodware samples as outliers.\nUnfortunately, before we get to that, we will have to do some feature encoding to transform our binary categorical features into something that DBSCAN can work with. Well, to be honest, we could simply provide DBSCAN with the binary data, but it would be meaningless, as our results would not give us any sort of beneficial insight. So, encoding it is!\nTo transform our binary valued data, we will make use of gower’s distance. Gower’s distance (or Gower’s dissimilarity index), is a metric used for measuring the dissimilarity between two vectors. This metric calculates the dissimilarity between the two vectors by summing the absolute differences between corresponding elements and normalizing the result. We can then create a distance matrix with this metric which yields a matrix that reflects the dissimilarity between all pairs of vectors.\nAlthough the formula can be annoying, thankfully we don’t have to compute these distances by hand. Instead, we can simply make use of the gower package that we imported, and call the gower_matrix() function on our features (X).\n\n# Compute gower's distance for categorical variables\n# Store as distance matrix\ndist_matrix = gower.gower_matrix(X)\n\n\n\nData Exploration\nWith that done, it would be nice if we could take a look at our data just to see if we can spot anything out. But, we cannot do that yet since our initial data 241-dimensional. I can visualize 2 or 3 dimension (maybe 4 if you project it to 3 dimensions), but there’s no way I can do that for 241 dimensions. And luckily, neither can any data visualization software (as far as I know).\nSo, what do we do? There are a ton of dimensionality reduction techniques catered directly to this task. For our case, we will make use of t-SNE (t-distributed stochastic neighbor embedding). t-SNE is a dimensionality reduction technique which transforms high-dimensional data into a lower-dimensional space while preserving local similarities between data points. This algorithm works by modeling the probability distributions of pairwise similarities between data points in both the high-dimensional and low-dimensional space. It tries to minimize the divergence between these distributions (typically using the KL-divergence). This effectively maps similar points in the high-dimensional space to nearby points in the low-dimensional space and dissimilar points to more distant positions.\nScikit-learn has an implementation of this algorithm which we can easily make use of. To work with our gower’s distance matrix, we will have to set the metric parameter to ‘precomputed’ and init to ‘random’. Additionally, we will squish all our data down to 2 components, and set perplexity to 50. Here, perplexity can be seen as a measure of the effective number of neighbors for each data point. Larger datasets require a larger perplexity, and a value of 5 to 50 is recommended. Finally, we set verbose to get some output during t-SNE iterations, and n_iter to 500 to bound the number of iterations. We then fit and transform our features to 2 dimensions by calling fit_transform() on the model and storing the results.\n\n# Use TSNE for reduction to 2 dimensions\n# Initialize TSNE\ntsne = TSNE(n_components=2,\n            verbose=1,\n            perplexity=50,\n            n_iter=500,\n            metric='precomputed',\n            init='random')\n\n# Fit tsne\ntsne_results = tsne.fit_transform(dist_matrix)\n\n[t-SNE] Computing 151 nearest neighbors...\n[t-SNE] Indexed 4464 samples in 0.039s...\n[t-SNE] Computed neighbors for 4464 samples in 0.261s...\n[t-SNE] Computed conditional probabilities for sample 1000 / 4464\n[t-SNE] Computed conditional probabilities for sample 2000 / 4464\n[t-SNE] Computed conditional probabilities for sample 3000 / 4464\n[t-SNE] Computed conditional probabilities for sample 4000 / 4464\n[t-SNE] Computed conditional probabilities for sample 4464 / 4464\n[t-SNE] Mean sigma: 0.000000\n[t-SNE] KL divergence after 250 iterations with early exaggeration: 50.179688\n[t-SNE] KL divergence after 500 iterations: 0.602954\n\n\nNow that we have 2-dimensional data, we can plot it as a simple scatterplot and color each point by its class (malware or goodware). For this, we can use seaborn’s scatterplot() function.\n\n# Plot datapoints and color by class label\nplt.figure(figsize=(8,6), dpi=300)\nsns.set(style = 'darkgrid', font_scale = 1.1)\n\nsns.scatterplot(\n    x=tsne_results[:,0], y=tsne_results[:,1],\n    hue=y.astype(int),\n    palette = {0: 'blue', 1: 'red'},\n    legend='full',\n    alpha=0.3\n)\n\nplt.legend(labels=['Malware', 'Goodware'])\nplt.xlabel('Component 1')\nplt.ylabel('Component 2')\nplt.title('t-SNE Visualization Colored by Class Label')\nplt.show()\n\n\n\n\nIn this visualization, we can easily pick out a couple of clusters of malware as well as a distinct grouping of goodware. There are a few scattered data points, but most should be easy enough to cluster. The malware samples also appear to be quite densely packed, so DBSCAN should not have a huge issue picking them up. On the other hand, the goodware samples are much less densely packed, so they will be easy to identify as outliers with the right hyperparameters.\n\n\nHyperparameters for DBSCAN\nTalking about hyperparameters, how should we go about picking them? And what are the important hyperparameters anyways?\nIn DBSCAN, epsilon (\\(\\epsilon\\)) and min_samples are the most important hyperparameters which significantly influence the clustering results. Epsilon (\\(\\epsilon\\)) is defined as the radius around a data point which DBSCAN uses to count the number of neighboring points and determine whether the given point is a core point, border point, or noise. This essentially defines the neighborhood’s size. A small value of epsilon (\\(\\epsilon\\)) will create more compact clusters, while a larger epsilon (\\(\\epsilon\\)) may merge clusters or consider more points as noise. Min_samples is the minimum number of data points within the epsilon (\\(\\epsilon\\)) radius required for a data point to be considered a core point. This determines the density required to form a cluster. A higher value here enforces a higher density requirement, while a lower one will allow for more sparse clusters. With both of these parameters, the density and shape of the clusters can vary drastically. So, it is very important to tune them correctly.\nSelecting min_samples is the easier of the two. A general rule of thumb is choosing a value in the range [dimensions + 1, 2*dimensions], where dimensions is the dimensionality of your dataset. Our dataset has 241 features, so dimensions = 241. Here, we can just round up and use 250 dimensions and see what happens.\nFor choosing epsilon (\\(\\epsilon\\)), there is a popular method which includes using a nearest neighbors model. Initially, we will compute the distances between the points and their num_features nearest points. For us, num_features is 241, so we simply have to find the distances from every point to its 241 nearest neighbors. To do this, we make use of sklearn’s NearestNeighbors model and set n_neighbors equal to 241. We also must make sure to set metric to ‘precomputed’ again so we can make use of our gower’s distance matrix. Then, using the kneighbors() function, we can get the indices and distances of the 241 nearest neighbors to each point. Next, we just sort the points by the distances in ascending order and plot the distances over the indices with a line plot. At the point where the slope of the curve increases significantly, we place a point (denoted as the elbow point). The distance value for this point is what we will set epsilon (\\(\\epsilon\\)) to.\n\n# Selecting epsilon for DBSCAN\n\n# Compute the average distances between points and num_features Nearest points\nneighbors = NearestNeighbors(n_neighbors = 241,\n                             metric='precomputed').fit(dist_matrix)\ndistance, idx = neighbors.kneighbors(dist_matrix)\n\n# Sort distance values by ascending\ndistance = np.sort(distance, axis=0)\ndistance = distance[:, 1]\n\n# Plot distance over index\n    # Set up figure\nplt.figure(figsize=(8,6), dpi=300)\nsns.set(style = 'darkgrid', font_scale = 1.1)\n\n    # Plot distances and indices\nsns.lineplot(x = np.arange(len(distance)),\n             y = distance)\n\n    # Plot elbow line and point\nplt.axhline(y=0.02, color='r', linestyle='--', label='y = 0.02')\nplt.scatter(x = np.argmin(np.abs(distance - 0.02)),\n            y = 0.02,\n            color = 'r',\n            marker = 'o')\nplt.text(x = np.argmin(np.abs(distance - 0.02)) - 700,\n         y = 0.021,\n         s = 'Elbow Point',\n         color = 'black',\n         fontsize = 10)\n\n\n\n    # Finish figure with title and labels\nplt.xlabel('Data Point Index')\nplt.ylabel('Distance')\nplt.title('Average Distance to 241 Nearest Neighbors using Gower Metric')\nplt.legend()\nplt.show()\n\n\n\n\nTaking a look at this plot, we place our elbow point at \\(y = 0.02\\). Here, the distance is 0.02, so we can set epsilon (\\(\\epsilon\\)) to 0.02.\n\n\nApplying DBSCAN\nNow, we have both of our most important hyperparameters for DBSCAN: - min_samples = 250 - epsilon (\\(\\epsilon\\)) = 0.02\nWith these hyperparameters, we can now fit a DBSCAN model on our data. To do this, we just create a DBSCAN model, set our hyperparameters (remember to set metric to ‘precomputed’ again), and finally call the fit() function with our distance matrix. After this, we will use that helper function from earlier to see what the cluster labels look like.\n\n# Run DBSCAN\ndbscan = DBSCAN(eps = 0.02,\n                min_samples = 250,\n                metric='precomputed').fit(dist_matrix)\n\n# Print unique values with counts from dbscan labels   \nprint_unique_with_counts(dbscan.labels_)\n\nValue: -1, Count: 1680\nValue: 0, Count: 388\nValue: 1, Count: 1089\nValue: 2, Count: 1014\nValue: 3, Count: 293\n\n\nAfter fitting DBSCAN, we can get our labels with the label_ learned property. Taking a look at the cluster counts, we can see that DBSCAN picked up a couple clusters and left 1680 samples as outliers (data points which do not fit into any clusters). This does not tell us much, so let’s make the same plot from earlier, but color the points by cluster assignments this time. We will color the outliers as blue (since we hope that these represent the goodware), and all other clustered points as red. For this, we can just set up a simple dictionary as a color map.\n\n# Set up color map for plot\ncolor_dict = {'Outlier': 'blue'}\n\n# Map every other integer to black\nfor i in range(4):\n    color_dict[f'Cluster {i}'] = 'red' \n    \n# Map cluster labels to strings for plot legend\ncluster_mapping = {-1: 'Outlier',\n                   0: 'Cluster 0',\n                   1: 'Cluster 1',\n                   2: 'Cluster 2',\n                   3: 'Cluster 3'}\n\ncluster_labels = [cluster_mapping[label] for label in dbscan.labels_]\n    \n# Set up figure\nplt.figure(figsize=(8,6), dpi=300)\nsns.set(style = 'darkgrid', font_scale = 1.1)\n\n# Plot using tsne and color by cluster labels\nsns.scatterplot(x = tsne_results[:,0],\n                y = tsne_results[:,1],\n                hue = cluster_labels,\n                palette = color_dict,\n                legend = 'full',\n                alpha = 0.3)\n\n# Finish figure styling\nplt.xlabel('Component 1')\nplt.ylabel('Component 2')\nplt.title('t-SNE Visualization Colored by Cluster Label')\n\n# Order legend text properly\nhandles, labels = plt.gca().get_legend_handles_labels()\norder = [1, 0, 2, 3, 4]\nplt.legend([handles[idx] for idx in order],[labels[idx] for idx in order])\nplt.show()\n\n\n\n\nFrom this plot, it seems that DBSCAN was able to cluster most of the malware clusters that we identified earlier. Most of the dense malware clusters have been correctly identified. Also, the sparse cluster of goodware points were also identified as outliers, just as we had hoped. Another thing to note is that most of the sporadic points which can’t easily be placed in the clusters have also been labeled as outliers, and not clustered along with the other malware. This is simply a side-effect of the way we are approaching this task with anomaly detection. Maybe with some more fine-tuning (or a different model type) we could improve this a bit. However, for purposes of this project, this result is actually quite good.\n\n\nEvaluation\nSo, how can we tell how good DBSCAN did at identifying malware versus goodware? Well, going back to how we constructed this project, since our dataset was mostly malware samples, we were looking to cluster those samples and hopefully be able to separate the goodware as outliers. So, we can covert any cluster assignments to the 1 label, for the malware class, and then convert the outlier label (-1) to 0 for the goodware class. After this, we can compare the results with the ground truths from the original class label array (y) and present a typical confusion matrix along with scores for precision, recall, and f1.\nSo, to start, we will convert our cluster assignments to class labels in just the same way. We can use a direct mapping from a dictionary to do this. We will then print our the class counts again to see what it looks like.\n\n# Convert cluster assignments to class labels\n# Outliers are goodware, all other clusters are malware\ncluster_assignments = dbscan.labels_\ncluster_to_class_mapping = {0: 1,\n                            1: 1,\n                            2: 1,\n                            3: 1,\n                           -1: 0}\nclusters_mappedTo_classes = np.array([cluster_to_class_mapping[assignment] for assignment in cluster_assignments])\n\n# Print class counts for mapped classes\nprint_unique_with_counts(clusters_mappedTo_classes)\n\nValue: 0, Count: 1680\nValue: 1, Count: 2784\n\n\nLooks good! We now have two classes from our cluster labels. The proportions are not exactly the same as we saw in the original dataset, but they are not too far off. We seem to have missed some malwares, but if this problem was easy to solve, then we wouldn’t still be trying to find a way to fix it now.\nWe can now treat these converted cluster labels as something like a prediction and create a confusion matrix with sklearn’s confusion_matrix() function, passing in the new labels as well as the ground truths from the original dataset (y).\n\n# Make Confusion Matrix\ny_pred = clusters_mappedTo_classes\ncm = sk.metrics.confusion_matrix(y, y_pred)\ncm_df = pd.DataFrame(cm,\n                     index = ['Goodware', 'Malware'],\n                     columns = ['Goodware', 'Malware'])\n\n# Print evaluation metrics\nprint(f'Precision: {sk.metrics.precision_score(y, y_pred):.3f}')\nprint(f'Recall: {sk.metrics.recall_score(y, y_pred):.3f}')\nprint(f'F1: {sk.metrics.f1_score(y, y_pred):.3f}')\n\n# Display Confusion Matrix\nplt.figure(figsize=(8,6), dpi=300)\nsns.set(font_scale = 1.1)\nax = sns.heatmap(cm_df, annot=True, fmt='d', linewidths=2, linecolor='#d3d3d3', cmap='Greens')\nax.set_title('Cluster Assignments To Classes Confusion Matrix')\nax.set_xlabel(\"Predicted Label\", fontsize=14, labelpad=20)\nax.set_ylabel(\"Actual Label\", fontsize=14, labelpad=20)\nplt.show()\n\nPrecision: 0.998\nRecall: 0.779\nF1: 0.875\n\n\n\n\n\nTaking a look at the precision, recall, and f1 scores, we see that our precision score is almost perfect. This means that the samples which DBSCAN labeled as goodware (outliers or anomalies) were actually goodware 99.8 percent of the time. In other words, we only misclassified goodware as malware 6 total times. Next, regarding recall, we have a score of 0.779. This essentially means that we were able to correctly identify 77.9% of malware samples. Not bad! Unfortunately, on the other side, we missed 22.1% of malware instances. These are the samples which were not clustered along with other malware samples, and thus were treated as outliers (or goodware). Finally, we have a decent f1 score of 0.875, which combines both precision and recall into a single score. In conclusion, using anomaly detection worked quite well for identifying android malware.\n\n\nConclusion\nThe exploration of using DBSCAN for anomaly detection, particularly in the context of malware identification, reveals a promising perspective on enhancing cybersecurity in the modern day and age. Leveraging the power of DBSCAN, this project has just touched the surface on what types of problems can be approached via outlier detection. Working with the TUNADROMD dataset today, we have been able to demonstrate exactly how effective these machine learning techniques can discern subtle patterns indicative of potential threats in android software. Notably, we were able to correctly identify 77.9% of malware samples in our dataset (woooo!). As we navigate this crossroad between machine learning and cybersecurity, this project serves to empower machine learning practitioners and enthusiasts alike with the conceptual knowledge and skills required to fortify the digital world against the ever-evolving landscape of cyber threats. If you have made it this far, I commend you for your persistence. I hope you enjoyed this post and learned something valuable along the way. To sign off, thank you for your time and, most importantly, you’ve done well today friend.\n(\\(\\\n(-.-)\no_(“)(”)"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Finley Malloc",
    "section": "",
    "text": "Finley Malloc is the Chief Data Scientist at Wengo Analytics. When not innovating on data platforms, Finley enjoys spending time unicycling and playing with her pet iguana.\n\n\nUniversity of California, San Diego | San Diego, CA PhD in Mathematics | Sept 2011 - June 2015\nMacalester College | St. Paul MA B.A in Economics | Sept 2007 - June 2011\n\n\n\nWengo Analytics | Head Data Scientist | April 2018 - present\nGeoScynce | Chief Analyst | Sept 2012 - April 2018\n\n \n  \n   \n  \n    \n     twitter\n  \n  \n    \n     Github"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Finley Malloc",
    "section": "",
    "text": "University of California, San Diego | San Diego, CA PhD in Mathematics | Sept 2011 - June 2015\nMacalester College | St. Paul MA B.A in Economics | Sept 2007 - June 2011"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "Finley Malloc",
    "section": "",
    "text": "Wengo Analytics | Head Data Scientist | April 2018 - present\nGeoScynce | Chief Analyst | Sept 2012 - April 2018"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Shayne Biagi",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n\n\n\n\n  \n\n\n\n\nHousing Price Forecasting with the Help of Random Forests\n\n\n\n\n\n\n\ncode\n\n\nmachine learning\n\n\nregression\n\n\n\n\n\n\n\n\n\n\n\nNov 10, 2023\n\n\nShayne Biagi\n\n\n\n\n\n\n  \n\n\n\n\nHarnessing Probability Theory: Naive Bayes for Classification\n\n\n\n\n\n\n\ncode\n\n\nmachine learning\n\n\nprobability theory\n\n\nstatistics\n\n\n\n\n\n\n\n\n\n\n\nNov 8, 2023\n\n\nShayne Biagi\n\n\n\n\n\n\n  \n\n\n\n\nExploring Ceramic Composition with K-Means Clustering\n\n\n\n\n\n\n\ncode\n\n\nmachine learning\n\n\nclustering\n\n\n\n\n\n\n\n\n\n\n\nNov 3, 2023\n\n\nShayne Biagi\n\n\n\n\n\n\n  \n\n\n\n\nMulti-Class Classification with a Single-Layer Perceptron Model\n\n\n\n\n\n\n\ncode\n\n\nmachine learning\n\n\nclassification\n\n\nneural networks\n\n\n\n\n\n\n\n\n\n\n\nOct 25, 2023\n\n\nShayne Biagi\n\n\n\n\n\n\n  \n\n\n\n\nUnveiling Malicious Software with Anomaly Detection\n\n\n\n\n\n\n\ncode\n\n\nmachine learning\n\n\nclustering\n\n\ncybersecurity\n\n\n\n\n\n\n\n\n\n\n\nOct 17, 2023\n\n\nShayne Biagi\n\n\n\n\n\n\n  \n\n\n\n\nSynthetic Data Generation with Variational Autoencoders\n\n\n\n\n\n\n\ngenerative ai\n\n\ncode\n\n\nmachine learning\n\n\nneural networks\n\n\n\n\n\n\n\n\n\n\n\nSep 25, 2023\n\n\nShayne Biagi\n\n\n\n\n\n\n  \n\n\n\n\nEthics of Intelligent Government Surveillance Systems\n\n\n\n\n\n\n\nethics\n\n\nmachine learning\n\n\ndata analytics\n\n\n\n\n\n\n\n\n\n\n\nMay 23, 2023\n\n\nPierre Sarabamoun, Shaunak Juvekar, Shayne Biagi, Shrikanth Upadhayaya, Srujan Vithalani\n\n\n\n\n\n\n  \n\n\n\n\nDeep Learning Equity Trading Model\n\n\n\n\n\n\n\nmachine learning\n\n\ndata analytics\n\n\nneural networks\n\n\n\n\n\n\n\n\n\n\n\nMay 4, 2023\n\n\nShayne Biagi, Andrew Istfan, Franco Medrano\n\n\n\n\n\n\n  \n\n\n\n\nAnalysis of A Push-Relabel Based Additive Approximation for Optimal Transport\n\n\n\n\n\n\n\noptimal transport\n\n\ntransport theory\n\n\nmachine learning\n\n\nmathematics\n\n\nalgorithms\n\n\n\n\n\n\n\n\n\n\n\nApr 10, 2023\n\n\nShayne Biagi\n\n\n\n\n\n\n  \n\n\n\n\nAnalysis of A Graph Theoretic Additive Approximation of Optimal Transport\n\n\n\n\n\n\n\noptimal transport\n\n\ntransport theory\n\n\nmachine learning\n\n\nmathematics\n\n\nalgorithms\n\n\n\n\n\n\n\n\n\n\n\nApr 7, 2023\n\n\nShayne Biagi\n\n\n\n\n\n\n  \n\n\n\n\nHypothetical Applications of Optimal Transport\n\n\n\n\n\n\n\noptimal transport\n\n\ntransport theory\n\n\n\n\n\n\n\n\n\n\n\nJan 18, 2023\n\n\nShayne Biagi\n\n\n\n\n\n\n  \n\n\n\n\nWhat is the Optimal Transport Problem?\n\n\n\n\n\n\n\noptimal transport\n\n\ntransport theory\n\n\nmathematics\n\n\nstatistics\n\n\n\n\n\n\n\n\n\n\n\nDec 16, 2022\n\n\nShayne Biagi\n\n\n\n\n\n\n  \n\n\n\n\nPandoc Guide\n\n\n\n\n\n\n\ncode\n\n\ndocumentation\n\n\n\n\n\n\n\n\n\n\n\nMar 3, 2022\n\n\nMason Gelletly, Lucy Paul, Shayne Biagi, Ashlyn East, Theodore Gunn\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/classification-slp/index.html",
    "href": "projects/classification-slp/index.html",
    "title": "Multi-Class Classification with a Single-Layer Perceptron Model",
    "section": "",
    "text": "In Machine Learning, learning and implementing classification models are a fundamental step on any data scientist’s journey. Here in this post, we will delve into the realm of multi-class classification problems, exploring fundamental concepts and applying a single-layer perceptron model to attack this problem.\nWhether a seasoned veteran or just a beginner in this field, this tutorial aims to be your guide to approaching multi-class classification with neural networks. We will walk through all the essential steps of a standard machine learning project pipeline, from data preprocessing and model building to final evaluations.\nWe’ll be working with the Penguins dataset, a newer replacement for the classic Iris dataset introduced by Ronald Fisher in 1936. This dataset, available from the seaborn library, contains information about 3 penguin species. Each sample contains several informative features which provide insight into the characteristics of the different penguin species. The data were collected and made available by Dr. Kristen Gorman and the Palmer Station, Antarctica LTER (an abbreviation for Long Term Ecological Research).\n\nSingle-Layer Perceptron Models\nSo, what even is a single-layer perceptron (SLP) model anyways? An SLP model is one of the most simple forms of an artificial neural network. Just as its name implies, it is characterized by having only a single hidden layer of neurons responsible for making predictions. It is a type of feed-forward neural network, where data only flows forwards from the input layer, through the hidden layer, and then to the output layer.\nThe input layer will consist of nodes representing the features of the input data, with each node corresponding to a feature. Then, we have the hidden layer, with as many, or as few, nodes as you want. To keep this tutorial concise, this layer’s size will equal that of the input layer. There is much research done in the realm of optimizing hidden layer architecture that we simply cannot cover it here, but if you are interested, the internet is your friend. Finally, data will come from the hidden layer into the output layer. For our case, we have 3 classes to predict, so we will have three nodes which each output the probability for one of those classes.\nOur model’s architecture will resemble something like the graphic below: \n\n\nImports\nNow that we have some background, let’s start with the imports required for this project. Here we will use pandas and numpy for most of our data preparation needs. Seaborn is where our dataset will come from, and along with matplotlib, will help us with some data visualizations down the line. Sklearn gives us easy ways to split our data as well as score our final predictions. Finally, our neural network model will be built using the keras framework.\n\nimport pandas as pd\nimport numpy as np\nimport seaborn as sns\nimport sklearn as sk\nimport tensorflow.keras as keras\nimport matplotlib.pyplot as plt\nfrom keras.models import Sequential\nfrom keras.layers import Dense, Input\nfrom keras.utils import to_categorical\nfrom sklearn.model_selection import train_test_split\n\n\n\nData Preprocessing\nFirst, let’s load the penguins dataset to take a look at it. Luckily, seaborn has a nice built-in function to load this dataset. The data is returned as a pandas dataframe, so keep that in mind.\n\ndata = sns.load_dataset('penguins')\nprint(data.shape)\ndata.head()\n\n(344, 7)\n\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nMale\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nFemale\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nFemale\n\n\n3\nAdelie\nTorgersen\nNaN\nNaN\nNaN\nNaN\nNaN\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nFemale\n\n\n\n\n\n\n\nLooks great! We can see that the first column (‘species’) is our target variable. Along with that, we get the following features which the model can use for predictions:\n\nisland: The name of the island (Dream, Torgersen, or Biscoe) in the Palmer Archipelago (Antarctica) where the penguin was found.\nbill_length_mm: The penguin’s bill length in millimeters\nbill_depth_mm: The penguin’s bill depth in millimeters\nflipper_length_mm: The penguin’s flipper length in millimeters\nbody_mass_g: The penguin’s body mass in grams\nsex: The penguin’s sex\n\nSome features are categorical, like species or island. Since neural networks require real-valued data to process, we are going to have to do something about that later.\nAlso, you may see that there are some missing values in our dataset, labeled as NaN. This is a problem, but thankfully we have an easy solution. We can use pandas dropna() method to drop the rows which contain missing values.\n\ndata = data.dropna()\nprint(data.shape)\ndata.head()\n\n(333, 7)\n\n\n\n\n\n\n\n\n\nspecies\nisland\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nsex\n\n\n\n\n0\nAdelie\nTorgersen\n39.1\n18.7\n181.0\n3750.0\nMale\n\n\n1\nAdelie\nTorgersen\n39.5\n17.4\n186.0\n3800.0\nFemale\n\n\n2\nAdelie\nTorgersen\n40.3\n18.0\n195.0\n3250.0\nFemale\n\n\n4\nAdelie\nTorgersen\n36.7\n19.3\n193.0\n3450.0\nFemale\n\n\n5\nAdelie\nTorgersen\n39.3\n20.6\n190.0\n3650.0\nMale\n\n\n\n\n\n\n\nNow, you can see that our dataset no longer has missing values, and we only had to drop 11 rows.\nSo, what should we do about those categorical variables? How can we convert the species names to numerical values that the neural network can interpret?\nOne popular method for this is called one-hot encoding. Essentially, we will make a column for each species and then place a value of 1 in that column if the penguin is that species.\nFor example, here, we have 3 species of penguins (Chinstrap, Adelie, and Gentoo). So, we will create three columns (one for each species) and then place a value of 1 or 0, indicating whether that is the species of the penguin.\nFirst though, we have to get from the species names to some integer values. For this, we can use sklearn’s LabelEncoder().\n\n# Encode labels as integers\nlabels = sk.preprocessing.LabelEncoder().fit_transform(data.iloc[:, 0].values)\nnp.unique(labels)\n\narray([0, 1, 2])\n\n\nNow, each species string has been assigned to an integer value. In our case:\n\n0 indicates Adelie\n1 indicates Chinstrap\n2 indicates Gentoo\n\nNext, we can one-hot encode these integer labels using a function from keras called to_categorical(). Afterwards, we will get 3 new columns for each species of penguin.\n\n# One hot encode labels\none_hot_labels = to_categorical(labels)\nnp.unique(one_hot_labels, axis=0)\n\narray([[0., 0., 1.],\n       [0., 1., 0.],\n       [1., 0., 0.]], dtype=float32)\n\n\nWe now have our one-hot encoded variable. Our model will now be able to correctly process this.\nIn the next code chunks, we do the exact same thing to the ‘island’ and ‘sex’ features in our dataset and then replace the original columns in the dataframe with these new columns.\n\n# Encode other categorical features\n    # Island\nisland_labels = sk.preprocessing.LabelEncoder().fit_transform(data.iloc[:, 1].values)\none_hot_islands = to_categorical(island_labels)\n    \n    # Sex\nsex_labels = sk.preprocessing.LabelEncoder().fit_transform(data.iloc[:, -1].values)\none_hot_sex = to_categorical(sex_labels)\n\n\n# Aggregate back to dataframe\n\n    # Remove species column, add one-hot-encoded species features\nprocessed_data = data.copy().drop('species', axis=1)\nfor idx,species in zip([0,1,2], data['species'].unique()[::-1]):\n    processed_data.insert(0, species, one_hot_labels[:, -(idx+1)])\n    \n    # Remove island column, add one-hot-encoded island features\nprocessed_data = processed_data.drop('island', axis=1)\nfor idx,island in zip([0,2,1], data['island'].unique()):\n    processed_data.insert(3, island, one_hot_islands[:, -(idx+1)])\n    \n    # Remove sex column, add one-hot-encoded sex features\nprocessed_data = processed_data.drop('sex', axis=1)\nfor idx,sex in zip([1,0], data['sex'].unique()):\n    processed_data.insert(10, sex, one_hot_sex[:, idx])\n    \nprocessed_data\n\n\n\n\n\n\n\n\nAdelie\nChinstrap\nGentoo\nDream\nBiscoe\nTorgersen\nbill_length_mm\nbill_depth_mm\nflipper_length_mm\nbody_mass_g\nFemale\nMale\n\n\n\n\n0\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n39.1\n18.7\n181.0\n3750.0\n0.0\n1.0\n\n\n1\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n39.5\n17.4\n186.0\n3800.0\n1.0\n0.0\n\n\n2\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n40.3\n18.0\n195.0\n3250.0\n1.0\n0.0\n\n\n4\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n36.7\n19.3\n193.0\n3450.0\n1.0\n0.0\n\n\n5\n1.0\n0.0\n0.0\n0.0\n0.0\n1.0\n39.3\n20.6\n190.0\n3650.0\n0.0\n1.0\n\n\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n...\n\n\n338\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n47.2\n13.7\n214.0\n4925.0\n1.0\n0.0\n\n\n340\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n46.8\n14.3\n215.0\n4850.0\n1.0\n0.0\n\n\n341\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n50.4\n15.7\n222.0\n5750.0\n0.0\n1.0\n\n\n342\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n45.2\n14.8\n212.0\n5200.0\n1.0\n0.0\n\n\n343\n0.0\n0.0\n1.0\n0.0\n1.0\n0.0\n49.9\n16.1\n213.0\n5400.0\n0.0\n1.0\n\n\n\n\n333 rows × 12 columns\n\n\n\nWith all that done, our dataset is fully real-valued. So far, we have dropped missing values and one-hot encoded the categorical features.\nNext up, we have to get our X (features) and y (target) and do some scaling. Scaling is crucial for neural networks because it helps ensure that the input features will contribute equally to the training process. If we do not scale our data, the features with larger values (like body mass in our case) will dominate the learning process. When features are on different scales, our optimization algorithm may take longer to converge or struggle to find an optimal solution. Scaling promotes a more stable and efficient training process. Here we will use sklearn’s MinMaxScaler() to scale our values between 0 and 1.\nWhile we are messing with X and y, we might as well also split our data into train and validation sets with sklearn’s train_test_split(). We will be using a test size of 1/3, meaning that our remaining 2/3 of data will be used for training.\n\n# Get X and y\nX = processed_data.iloc[:, 3:].values\ny = processed_data.iloc[:, :3].values\n\n# Scale data\nscaler = sk.preprocessing.MinMaxScaler()\nX = scaler.fit_transform(X)\ny = scaler.fit_transform(y)\n\n# Train-test split 2/3:1/3\nX_train, X_val, y_train, y_val = train_test_split(X,\n                                                  y,\n                                                  test_size=1/3,\n                                                  stratify=y)\n\n\n\nBuilding our Model\nAnd that concludes the data preprocessing step! Woooo\nFinally, we can get on to building the neural network to actually make some predictions. Below, we build a simple, single-layer perceptron classifier to train.\nOur layer sizes from input to output go: 9 &gt; 9 &gt; 3.\nWe have 9 features to input, then we will have a hidden layer of 9 nodes to train to make some predictions. These predictions will then be output as 3 values. Each value in the output corresponds to the probability that a sample is of a specific class.\nWe use the rectified linear unit (relu) function as the activation for the hidden layer. This allows us to impose a restriction on the output of the nodes in the hidden layer and mitigate the vanishing gradient problem. There are many types of activation functions, each with their own set of pros and cons. For our simple example though, this will work perfectly. Additionally, you can see that the output layer has a softmax activation function. We use this because the softmax function squishes the outputs into the range of (0, 1) and transforms the output from our hidden layer into a vector which represents the probabilities of our classes.\n\n# Build slp classifier\nslp_clf = Sequential()\nslp_clf.add(Input(X.shape[1]))\nslp_clf.add(Dense(X.shape[1], activation='relu'))\nslp_clf.add(Dense(3, activation='softmax'))\n\nWith the model set in place, we can now compile and fit the model. Also, we will train our model with early stopping to make sure we don’t overfit due to too many epochs. We will monitor the validation loss starting from epoch 10. If it does not improve over the course of 5 consecutive epochs, then we will stop training.\nWhen compiling the model, we can use categorical crossentropy as our loss. This loss function is designed to compare probability distributions, and thus will work well in combination with our softmax activation function from earlier to tell us how far our model is from predicting classes correctly. Finally, we will monitor the accuracy of the model during training, as it is more easily interpretable. The accuracy metric tells us the fraction of correctly classified instances out of the total. A perfect model will score 1 here.\nFinally, we can fit our model. We provide X_train and y_train to train the model on. The epochs here is set arbitrarily large so that we can stop training when the early stopping kicks in. Since our dataset is small, a smaller batch size of 16 is also used. Lastly, validation data is provided along with our callback to stop early.\n\n# Early Stopping\nstop_early = keras.callbacks.EarlyStopping(monitor='val_loss',\n                                           patience=3,\n                                           start_from_epoch=10)\n\n# Compile\nslp_clf.compile(loss='categorical_crossentropy',\n                optimizer='adam',\n                metrics=['accuracy'])\n\n# Fit\nhistory = slp_clf.fit(X_train,\n                      y_train,\n                      epochs=500,\n                      batch_size=16,\n                      verbose=2,\n                      validation_data=(X_val, y_val),\n                      callbacks=[stop_early])\n\nEpoch 1/500\n14/14 - 1s - loss: 1.1940 - accuracy: 0.4369 - val_loss: 1.1427 - val_accuracy: 0.4414 - 1s/epoch - 102ms/step\nEpoch 2/500\n14/14 - 0s - loss: 1.1533 - accuracy: 0.4369 - val_loss: 1.1025 - val_accuracy: 0.4414 - 49ms/epoch - 3ms/step\nEpoch 3/500\n14/14 - 0s - loss: 1.1203 - accuracy: 0.4550 - val_loss: 1.0687 - val_accuracy: 0.5495 - 52ms/epoch - 4ms/step\nEpoch 4/500\n14/14 - 0s - loss: 1.0926 - accuracy: 0.5631 - val_loss: 1.0394 - val_accuracy: 0.6306 - 48ms/epoch - 3ms/step\nEpoch 5/500\n14/14 - 0s - loss: 1.0685 - accuracy: 0.5991 - val_loss: 1.0140 - val_accuracy: 0.6396 - 62ms/epoch - 4ms/step\nEpoch 6/500\n14/14 - 0s - loss: 1.0467 - accuracy: 0.5946 - val_loss: 0.9892 - val_accuracy: 0.6396 - 47ms/epoch - 3ms/step\nEpoch 7/500\n14/14 - 0s - loss: 1.0227 - accuracy: 0.5811 - val_loss: 0.9625 - val_accuracy: 0.6216 - 47ms/epoch - 3ms/step\nEpoch 8/500\n14/14 - 0s - loss: 0.9969 - accuracy: 0.5676 - val_loss: 0.9289 - val_accuracy: 0.6306 - 62ms/epoch - 4ms/step\nEpoch 9/500\n14/14 - 0s - loss: 0.9628 - accuracy: 0.5721 - val_loss: 0.8944 - val_accuracy: 0.6396 - 78ms/epoch - 6ms/step\nEpoch 10/500\n14/14 - 0s - loss: 0.9312 - accuracy: 0.6081 - val_loss: 0.8613 - val_accuracy: 0.7207 - 62ms/epoch - 4ms/step\nEpoch 11/500\n14/14 - 0s - loss: 0.9004 - accuracy: 0.6937 - val_loss: 0.8316 - val_accuracy: 0.7477 - 56ms/epoch - 4ms/step\nEpoch 12/500\n14/14 - 0s - loss: 0.8733 - accuracy: 0.7252 - val_loss: 0.8015 - val_accuracy: 0.7658 - 47ms/epoch - 3ms/step\nEpoch 13/500\n14/14 - 0s - loss: 0.8453 - accuracy: 0.7297 - val_loss: 0.7737 - val_accuracy: 0.7658 - 58ms/epoch - 4ms/step\nEpoch 14/500\n14/14 - 0s - loss: 0.8189 - accuracy: 0.7297 - val_loss: 0.7476 - val_accuracy: 0.7658 - 47ms/epoch - 3ms/step\nEpoch 15/500\n14/14 - 0s - loss: 0.7944 - accuracy: 0.7387 - val_loss: 0.7219 - val_accuracy: 0.7658 - 62ms/epoch - 4ms/step\nEpoch 16/500\n14/14 - 0s - loss: 0.7704 - accuracy: 0.7387 - val_loss: 0.6980 - val_accuracy: 0.7658 - 63ms/epoch - 4ms/step\nEpoch 17/500\n14/14 - 0s - loss: 0.7478 - accuracy: 0.7387 - val_loss: 0.6759 - val_accuracy: 0.7658 - 62ms/epoch - 4ms/step\nEpoch 18/500\n14/14 - 0s - loss: 0.7269 - accuracy: 0.7523 - val_loss: 0.6549 - val_accuracy: 0.7748 - 62ms/epoch - 4ms/step\nEpoch 19/500\n14/14 - 0s - loss: 0.7059 - accuracy: 0.7568 - val_loss: 0.6351 - val_accuracy: 0.7838 - 47ms/epoch - 3ms/step\nEpoch 20/500\n14/14 - 0s - loss: 0.6865 - accuracy: 0.7613 - val_loss: 0.6159 - val_accuracy: 0.7838 - 54ms/epoch - 4ms/step\nEpoch 21/500\n14/14 - 0s - loss: 0.6677 - accuracy: 0.7613 - val_loss: 0.5974 - val_accuracy: 0.7838 - 56ms/epoch - 4ms/step\nEpoch 22/500\n14/14 - 0s - loss: 0.6501 - accuracy: 0.7658 - val_loss: 0.5802 - val_accuracy: 0.7838 - 47ms/epoch - 3ms/step\nEpoch 23/500\n14/14 - 0s - loss: 0.6326 - accuracy: 0.7658 - val_loss: 0.5632 - val_accuracy: 0.7928 - 52ms/epoch - 4ms/step\nEpoch 24/500\n14/14 - 0s - loss: 0.6161 - accuracy: 0.7658 - val_loss: 0.5471 - val_accuracy: 0.7928 - 47ms/epoch - 3ms/step\nEpoch 25/500\n14/14 - 0s - loss: 0.6001 - accuracy: 0.7703 - val_loss: 0.5319 - val_accuracy: 0.7928 - 62ms/epoch - 4ms/step\nEpoch 26/500\n14/14 - 0s - loss: 0.5850 - accuracy: 0.7793 - val_loss: 0.5171 - val_accuracy: 0.7928 - 47ms/epoch - 3ms/step\nEpoch 27/500\n14/14 - 0s - loss: 0.5700 - accuracy: 0.7838 - val_loss: 0.5032 - val_accuracy: 0.7928 - 62ms/epoch - 4ms/step\nEpoch 28/500\n14/14 - 0s - loss: 0.5557 - accuracy: 0.7838 - val_loss: 0.4891 - val_accuracy: 0.7928 - 47ms/epoch - 3ms/step\nEpoch 29/500\n14/14 - 0s - loss: 0.5413 - accuracy: 0.7883 - val_loss: 0.4761 - val_accuracy: 0.7928 - 51ms/epoch - 4ms/step\nEpoch 30/500\n14/14 - 0s - loss: 0.5278 - accuracy: 0.7883 - val_loss: 0.4635 - val_accuracy: 0.7928 - 63ms/epoch - 4ms/step\nEpoch 31/500\n14/14 - 0s - loss: 0.5151 - accuracy: 0.7883 - val_loss: 0.4522 - val_accuracy: 0.7928 - 52ms/epoch - 4ms/step\nEpoch 32/500\n14/14 - 0s - loss: 0.5018 - accuracy: 0.7928 - val_loss: 0.4397 - val_accuracy: 0.7928 - 53ms/epoch - 4ms/step\nEpoch 33/500\n14/14 - 0s - loss: 0.4898 - accuracy: 0.7928 - val_loss: 0.4284 - val_accuracy: 0.7928 - 49ms/epoch - 4ms/step\nEpoch 34/500\n14/14 - 0s - loss: 0.4777 - accuracy: 0.7928 - val_loss: 0.4175 - val_accuracy: 0.7928 - 47ms/epoch - 3ms/step\nEpoch 35/500\n14/14 - 0s - loss: 0.4665 - accuracy: 0.7928 - val_loss: 0.4075 - val_accuracy: 0.8018 - 62ms/epoch - 4ms/step\nEpoch 36/500\n14/14 - 0s - loss: 0.4559 - accuracy: 0.7928 - val_loss: 0.3981 - val_accuracy: 0.8018 - 63ms/epoch - 4ms/step\nEpoch 37/500\n14/14 - 0s - loss: 0.4451 - accuracy: 0.7973 - val_loss: 0.3888 - val_accuracy: 0.8018 - 47ms/epoch - 3ms/step\nEpoch 38/500\n14/14 - 0s - loss: 0.4354 - accuracy: 0.8108 - val_loss: 0.3791 - val_accuracy: 0.8198 - 58ms/epoch - 4ms/step\nEpoch 39/500\n14/14 - 0s - loss: 0.4255 - accuracy: 0.8198 - val_loss: 0.3706 - val_accuracy: 0.8198 - 43ms/epoch - 3ms/step\nEpoch 40/500\n14/14 - 0s - loss: 0.4163 - accuracy: 0.8423 - val_loss: 0.3628 - val_accuracy: 0.8468 - 62ms/epoch - 4ms/step\nEpoch 41/500\n14/14 - 0s - loss: 0.4074 - accuracy: 0.8514 - val_loss: 0.3551 - val_accuracy: 0.8739 - 47ms/epoch - 3ms/step\nEpoch 42/500\n14/14 - 0s - loss: 0.3993 - accuracy: 0.8739 - val_loss: 0.3471 - val_accuracy: 0.8919 - 57ms/epoch - 4ms/step\nEpoch 43/500\n14/14 - 0s - loss: 0.3906 - accuracy: 0.8874 - val_loss: 0.3402 - val_accuracy: 0.8919 - 46ms/epoch - 3ms/step\nEpoch 44/500\n14/14 - 0s - loss: 0.3827 - accuracy: 0.8964 - val_loss: 0.3335 - val_accuracy: 0.9099 - 62ms/epoch - 4ms/step\nEpoch 45/500\n14/14 - 0s - loss: 0.3751 - accuracy: 0.8964 - val_loss: 0.3273 - val_accuracy: 0.9099 - 47ms/epoch - 3ms/step\nEpoch 46/500\n14/14 - 0s - loss: 0.3679 - accuracy: 0.9144 - val_loss: 0.3203 - val_accuracy: 0.9279 - 47ms/epoch - 3ms/step\nEpoch 47/500\n14/14 - 0s - loss: 0.3605 - accuracy: 0.9144 - val_loss: 0.3143 - val_accuracy: 0.9279 - 48ms/epoch - 3ms/step\nEpoch 48/500\n14/14 - 0s - loss: 0.3538 - accuracy: 0.9189 - val_loss: 0.3089 - val_accuracy: 0.9279 - 46ms/epoch - 3ms/step\nEpoch 49/500\n14/14 - 0s - loss: 0.3476 - accuracy: 0.9279 - val_loss: 0.3024 - val_accuracy: 0.9459 - 47ms/epoch - 3ms/step\nEpoch 50/500\n14/14 - 0s - loss: 0.3406 - accuracy: 0.9640 - val_loss: 0.2975 - val_accuracy: 0.9459 - 68ms/epoch - 5ms/step\nEpoch 51/500\n14/14 - 0s - loss: 0.3345 - accuracy: 0.9595 - val_loss: 0.2925 - val_accuracy: 0.9459 - 49ms/epoch - 3ms/step\nEpoch 52/500\n14/14 - 0s - loss: 0.3283 - accuracy: 0.9685 - val_loss: 0.2869 - val_accuracy: 0.9550 - 54ms/epoch - 4ms/step\nEpoch 53/500\n14/14 - 0s - loss: 0.3229 - accuracy: 0.9685 - val_loss: 0.2817 - val_accuracy: 0.9640 - 47ms/epoch - 3ms/step\nEpoch 54/500\n14/14 - 0s - loss: 0.3168 - accuracy: 0.9730 - val_loss: 0.2775 - val_accuracy: 0.9640 - 47ms/epoch - 3ms/step\nEpoch 55/500\n14/14 - 0s - loss: 0.3114 - accuracy: 0.9730 - val_loss: 0.2727 - val_accuracy: 0.9730 - 62ms/epoch - 4ms/step\nEpoch 56/500\n14/14 - 0s - loss: 0.3056 - accuracy: 0.9775 - val_loss: 0.2684 - val_accuracy: 0.9640 - 47ms/epoch - 3ms/step\nEpoch 57/500\n14/14 - 0s - loss: 0.3004 - accuracy: 0.9820 - val_loss: 0.2638 - val_accuracy: 0.9640 - 50ms/epoch - 4ms/step\nEpoch 58/500\n14/14 - 0s - loss: 0.2952 - accuracy: 0.9865 - val_loss: 0.2596 - val_accuracy: 0.9640 - 49ms/epoch - 4ms/step\nEpoch 59/500\n14/14 - 0s - loss: 0.2910 - accuracy: 0.9910 - val_loss: 0.2550 - val_accuracy: 0.9640 - 47ms/epoch - 3ms/step\nEpoch 60/500\n14/14 - 0s - loss: 0.2855 - accuracy: 0.9910 - val_loss: 0.2519 - val_accuracy: 0.9640 - 47ms/epoch - 3ms/step\nEpoch 61/500\n14/14 - 0s - loss: 0.2807 - accuracy: 0.9910 - val_loss: 0.2477 - val_accuracy: 0.9640 - 48ms/epoch - 3ms/step\nEpoch 62/500\n14/14 - 0s - loss: 0.2760 - accuracy: 0.9910 - val_loss: 0.2441 - val_accuracy: 0.9640 - 59ms/epoch - 4ms/step\nEpoch 63/500\n14/14 - 0s - loss: 0.2717 - accuracy: 0.9910 - val_loss: 0.2400 - val_accuracy: 0.9640 - 47ms/epoch - 3ms/step\nEpoch 64/500\n14/14 - 0s - loss: 0.2669 - accuracy: 0.9910 - val_loss: 0.2364 - val_accuracy: 0.9640 - 47ms/epoch - 3ms/step\nEpoch 65/500\n14/14 - 0s - loss: 0.2626 - accuracy: 0.9955 - val_loss: 0.2330 - val_accuracy: 0.9640 - 47ms/epoch - 3ms/step\nEpoch 66/500\n14/14 - 0s - loss: 0.2594 - accuracy: 0.9910 - val_loss: 0.2290 - val_accuracy: 0.9640 - 43ms/epoch - 3ms/step\nEpoch 67/500\n14/14 - 0s - loss: 0.2541 - accuracy: 0.9955 - val_loss: 0.2261 - val_accuracy: 0.9550 - 64ms/epoch - 5ms/step\nEpoch 68/500\n14/14 - 0s - loss: 0.2507 - accuracy: 0.9955 - val_loss: 0.2236 - val_accuracy: 0.9640 - 55ms/epoch - 4ms/step\nEpoch 69/500\n14/14 - 0s - loss: 0.2463 - accuracy: 1.0000 - val_loss: 0.2196 - val_accuracy: 0.9550 - 45ms/epoch - 3ms/step\nEpoch 70/500\n14/14 - 0s - loss: 0.2426 - accuracy: 0.9955 - val_loss: 0.2167 - val_accuracy: 0.9640 - 63ms/epoch - 4ms/step\nEpoch 71/500\n14/14 - 0s - loss: 0.2387 - accuracy: 0.9910 - val_loss: 0.2132 - val_accuracy: 0.9550 - 56ms/epoch - 4ms/step\nEpoch 72/500\n14/14 - 0s - loss: 0.2350 - accuracy: 0.9955 - val_loss: 0.2101 - val_accuracy: 0.9640 - 63ms/epoch - 5ms/step\nEpoch 73/500\n14/14 - 0s - loss: 0.2314 - accuracy: 0.9910 - val_loss: 0.2077 - val_accuracy: 0.9550 - 48ms/epoch - 3ms/step\nEpoch 74/500\n14/14 - 0s - loss: 0.2277 - accuracy: 0.9955 - val_loss: 0.2045 - val_accuracy: 0.9550 - 74ms/epoch - 5ms/step\nEpoch 75/500\n14/14 - 0s - loss: 0.2240 - accuracy: 0.9955 - val_loss: 0.2019 - val_accuracy: 0.9550 - 47ms/epoch - 3ms/step\nEpoch 76/500\n14/14 - 0s - loss: 0.2209 - accuracy: 0.9955 - val_loss: 0.1990 - val_accuracy: 0.9550 - 66ms/epoch - 5ms/step\nEpoch 77/500\n14/14 - 0s - loss: 0.2171 - accuracy: 0.9955 - val_loss: 0.1962 - val_accuracy: 0.9550 - 46ms/epoch - 3ms/step\nEpoch 78/500\n14/14 - 0s - loss: 0.2143 - accuracy: 0.9910 - val_loss: 0.1933 - val_accuracy: 0.9640 - 64ms/epoch - 5ms/step\nEpoch 79/500\n14/14 - 0s - loss: 0.2107 - accuracy: 0.9955 - val_loss: 0.1908 - val_accuracy: 0.9550 - 47ms/epoch - 3ms/step\nEpoch 80/500\n14/14 - 0s - loss: 0.2076 - accuracy: 0.9955 - val_loss: 0.1885 - val_accuracy: 0.9550 - 45ms/epoch - 3ms/step\nEpoch 81/500\n14/14 - 0s - loss: 0.2044 - accuracy: 0.9955 - val_loss: 0.1859 - val_accuracy: 0.9550 - 62ms/epoch - 4ms/step\nEpoch 82/500\n14/14 - 0s - loss: 0.2014 - accuracy: 0.9955 - val_loss: 0.1831 - val_accuracy: 0.9550 - 47ms/epoch - 3ms/step\nEpoch 83/500\n14/14 - 0s - loss: 0.1985 - accuracy: 1.0000 - val_loss: 0.1809 - val_accuracy: 0.9640 - 47ms/epoch - 3ms/step\nEpoch 84/500\n14/14 - 0s - loss: 0.1953 - accuracy: 0.9955 - val_loss: 0.1783 - val_accuracy: 0.9550 - 42ms/epoch - 3ms/step\nEpoch 85/500\n14/14 - 0s - loss: 0.1926 - accuracy: 0.9955 - val_loss: 0.1760 - val_accuracy: 0.9550 - 53ms/epoch - 4ms/step\nEpoch 86/500\n14/14 - 0s - loss: 0.1900 - accuracy: 0.9910 - val_loss: 0.1734 - val_accuracy: 0.9730 - 60ms/epoch - 4ms/step\nEpoch 87/500\n14/14 - 0s - loss: 0.1869 - accuracy: 0.9910 - val_loss: 0.1713 - val_accuracy: 0.9640 - 36ms/epoch - 3ms/step\nEpoch 88/500\n14/14 - 0s - loss: 0.1845 - accuracy: 1.0000 - val_loss: 0.1695 - val_accuracy: 0.9640 - 47ms/epoch - 3ms/step\nEpoch 89/500\n14/14 - 0s - loss: 0.1813 - accuracy: 1.0000 - val_loss: 0.1671 - val_accuracy: 0.9730 - 47ms/epoch - 3ms/step\nEpoch 90/500\n14/14 - 0s - loss: 0.1790 - accuracy: 0.9910 - val_loss: 0.1646 - val_accuracy: 0.9730 - 61ms/epoch - 4ms/step\nEpoch 91/500\n14/14 - 0s - loss: 0.1762 - accuracy: 0.9910 - val_loss: 0.1626 - val_accuracy: 0.9640 - 47ms/epoch - 3ms/step\nEpoch 92/500\n14/14 - 0s - loss: 0.1738 - accuracy: 1.0000 - val_loss: 0.1607 - val_accuracy: 0.9730 - 48ms/epoch - 3ms/step\nEpoch 93/500\n14/14 - 0s - loss: 0.1711 - accuracy: 1.0000 - val_loss: 0.1586 - val_accuracy: 0.9640 - 55ms/epoch - 4ms/step\nEpoch 94/500\n14/14 - 0s - loss: 0.1685 - accuracy: 1.0000 - val_loss: 0.1564 - val_accuracy: 0.9640 - 49ms/epoch - 4ms/step\nEpoch 95/500\n14/14 - 0s - loss: 0.1666 - accuracy: 0.9910 - val_loss: 0.1544 - val_accuracy: 0.9730 - 46ms/epoch - 3ms/step\nEpoch 96/500\n14/14 - 0s - loss: 0.1637 - accuracy: 0.9955 - val_loss: 0.1524 - val_accuracy: 0.9640 - 47ms/epoch - 3ms/step\nEpoch 97/500\n14/14 - 0s - loss: 0.1615 - accuracy: 1.0000 - val_loss: 0.1507 - val_accuracy: 0.9730 - 47ms/epoch - 3ms/step\nEpoch 98/500\n14/14 - 0s - loss: 0.1592 - accuracy: 1.0000 - val_loss: 0.1487 - val_accuracy: 0.9730 - 51ms/epoch - 4ms/step\nEpoch 99/500\n14/14 - 0s - loss: 0.1568 - accuracy: 1.0000 - val_loss: 0.1467 - val_accuracy: 0.9730 - 46ms/epoch - 3ms/step\nEpoch 100/500\n14/14 - 0s - loss: 0.1548 - accuracy: 0.9955 - val_loss: 0.1447 - val_accuracy: 0.9820 - 47ms/epoch - 3ms/step\nEpoch 101/500\n14/14 - 0s - loss: 0.1524 - accuracy: 1.0000 - val_loss: 0.1429 - val_accuracy: 0.9730 - 49ms/epoch - 4ms/step\nEpoch 102/500\n14/14 - 0s - loss: 0.1502 - accuracy: 1.0000 - val_loss: 0.1411 - val_accuracy: 0.9730 - 47ms/epoch - 3ms/step\nEpoch 103/500\n14/14 - 0s - loss: 0.1482 - accuracy: 1.0000 - val_loss: 0.1395 - val_accuracy: 0.9730 - 53ms/epoch - 4ms/step\nEpoch 104/500\n14/14 - 0s - loss: 0.1459 - accuracy: 1.0000 - val_loss: 0.1376 - val_accuracy: 0.9730 - 64ms/epoch - 5ms/step\nEpoch 105/500\n14/14 - 0s - loss: 0.1441 - accuracy: 0.9955 - val_loss: 0.1358 - val_accuracy: 0.9820 - 50ms/epoch - 4ms/step\nEpoch 106/500\n14/14 - 0s - loss: 0.1419 - accuracy: 0.9955 - val_loss: 0.1342 - val_accuracy: 0.9730 - 48ms/epoch - 3ms/step\nEpoch 107/500\n14/14 - 0s - loss: 0.1399 - accuracy: 1.0000 - val_loss: 0.1325 - val_accuracy: 0.9730 - 42ms/epoch - 3ms/step\nEpoch 108/500\n14/14 - 0s - loss: 0.1382 - accuracy: 0.9955 - val_loss: 0.1308 - val_accuracy: 0.9820 - 62ms/epoch - 4ms/step\nEpoch 109/500\n14/14 - 0s - loss: 0.1362 - accuracy: 1.0000 - val_loss: 0.1290 - val_accuracy: 0.9820 - 63ms/epoch - 4ms/step\nEpoch 110/500\n14/14 - 0s - loss: 0.1342 - accuracy: 1.0000 - val_loss: 0.1277 - val_accuracy: 0.9820 - 47ms/epoch - 3ms/step\nEpoch 111/500\n14/14 - 0s - loss: 0.1323 - accuracy: 1.0000 - val_loss: 0.1262 - val_accuracy: 0.9820 - 64ms/epoch - 5ms/step\nEpoch 112/500\n14/14 - 0s - loss: 0.1304 - accuracy: 1.0000 - val_loss: 0.1246 - val_accuracy: 0.9730 - 54ms/epoch - 4ms/step\nEpoch 113/500\n14/14 - 0s - loss: 0.1290 - accuracy: 1.0000 - val_loss: 0.1230 - val_accuracy: 0.9730 - 48ms/epoch - 3ms/step\nEpoch 114/500\n14/14 - 0s - loss: 0.1269 - accuracy: 1.0000 - val_loss: 0.1214 - val_accuracy: 0.9820 - 47ms/epoch - 3ms/step\nEpoch 115/500\n14/14 - 0s - loss: 0.1250 - accuracy: 1.0000 - val_loss: 0.1200 - val_accuracy: 0.9730 - 47ms/epoch - 3ms/step\nEpoch 116/500\n14/14 - 0s - loss: 0.1234 - accuracy: 1.0000 - val_loss: 0.1186 - val_accuracy: 0.9730 - 47ms/epoch - 3ms/step\nEpoch 117/500\n14/14 - 0s - loss: 0.1216 - accuracy: 1.0000 - val_loss: 0.1171 - val_accuracy: 0.9730 - 62ms/epoch - 4ms/step\nEpoch 118/500\n14/14 - 0s - loss: 0.1201 - accuracy: 1.0000 - val_loss: 0.1158 - val_accuracy: 0.9730 - 47ms/epoch - 3ms/step\nEpoch 119/500\n14/14 - 0s - loss: 0.1185 - accuracy: 1.0000 - val_loss: 0.1144 - val_accuracy: 0.9730 - 47ms/epoch - 3ms/step\nEpoch 120/500\n14/14 - 0s - loss: 0.1169 - accuracy: 1.0000 - val_loss: 0.1130 - val_accuracy: 0.9730 - 40ms/epoch - 3ms/step\nEpoch 121/500\n14/14 - 0s - loss: 0.1152 - accuracy: 1.0000 - val_loss: 0.1117 - val_accuracy: 0.9820 - 63ms/epoch - 4ms/step\nEpoch 122/500\n14/14 - 0s - loss: 0.1137 - accuracy: 1.0000 - val_loss: 0.1102 - val_accuracy: 0.9820 - 47ms/epoch - 3ms/step\nEpoch 123/500\n14/14 - 0s - loss: 0.1121 - accuracy: 1.0000 - val_loss: 0.1089 - val_accuracy: 0.9820 - 54ms/epoch - 4ms/step\nEpoch 124/500\n14/14 - 0s - loss: 0.1107 - accuracy: 1.0000 - val_loss: 0.1076 - val_accuracy: 0.9820 - 46ms/epoch - 3ms/step\nEpoch 125/500\n14/14 - 0s - loss: 0.1091 - accuracy: 1.0000 - val_loss: 0.1063 - val_accuracy: 0.9820 - 47ms/epoch - 3ms/step\nEpoch 126/500\n14/14 - 0s - loss: 0.1077 - accuracy: 1.0000 - val_loss: 0.1052 - val_accuracy: 0.9910 - 47ms/epoch - 3ms/step\nEpoch 127/500\n14/14 - 0s - loss: 0.1065 - accuracy: 1.0000 - val_loss: 0.1039 - val_accuracy: 0.9820 - 47ms/epoch - 3ms/step\nEpoch 128/500\n14/14 - 0s - loss: 0.1050 - accuracy: 1.0000 - val_loss: 0.1027 - val_accuracy: 0.9910 - 47ms/epoch - 3ms/step\nEpoch 129/500\n14/14 - 0s - loss: 0.1036 - accuracy: 1.0000 - val_loss: 0.1015 - val_accuracy: 0.9910 - 47ms/epoch - 3ms/step\nEpoch 130/500\n14/14 - 0s - loss: 0.1022 - accuracy: 1.0000 - val_loss: 0.1003 - val_accuracy: 0.9910 - 62ms/epoch - 4ms/step\nEpoch 131/500\n14/14 - 0s - loss: 0.1010 - accuracy: 1.0000 - val_loss: 0.0991 - val_accuracy: 0.9910 - 40ms/epoch - 3ms/step\nEpoch 132/500\n14/14 - 0s - loss: 0.0997 - accuracy: 1.0000 - val_loss: 0.0980 - val_accuracy: 0.9910 - 49ms/epoch - 4ms/step\nEpoch 133/500\n14/14 - 0s - loss: 0.0984 - accuracy: 1.0000 - val_loss: 0.0967 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 134/500\n14/14 - 0s - loss: 0.0971 - accuracy: 1.0000 - val_loss: 0.0956 - val_accuracy: 0.9910 - 47ms/epoch - 3ms/step\nEpoch 135/500\n14/14 - 0s - loss: 0.0957 - accuracy: 1.0000 - val_loss: 0.0945 - val_accuracy: 0.9910 - 62ms/epoch - 4ms/step\nEpoch 136/500\n14/14 - 0s - loss: 0.0945 - accuracy: 1.0000 - val_loss: 0.0935 - val_accuracy: 0.9910 - 47ms/epoch - 3ms/step\nEpoch 137/500\n14/14 - 0s - loss: 0.0933 - accuracy: 1.0000 - val_loss: 0.0924 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 138/500\n14/14 - 0s - loss: 0.0921 - accuracy: 1.0000 - val_loss: 0.0914 - val_accuracy: 0.9910 - 47ms/epoch - 3ms/step\nEpoch 139/500\n14/14 - 0s - loss: 0.0908 - accuracy: 1.0000 - val_loss: 0.0904 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 140/500\n14/14 - 0s - loss: 0.0898 - accuracy: 0.9955 - val_loss: 0.0893 - val_accuracy: 1.0000 - 62ms/epoch - 4ms/step\nEpoch 141/500\n14/14 - 0s - loss: 0.0887 - accuracy: 1.0000 - val_loss: 0.0884 - val_accuracy: 1.0000 - 44ms/epoch - 3ms/step\nEpoch 142/500\n14/14 - 0s - loss: 0.0875 - accuracy: 1.0000 - val_loss: 0.0874 - val_accuracy: 1.0000 - 55ms/epoch - 4ms/step\nEpoch 143/500\n14/14 - 0s - loss: 0.0865 - accuracy: 1.0000 - val_loss: 0.0864 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 144/500\n14/14 - 0s - loss: 0.0859 - accuracy: 1.0000 - val_loss: 0.0855 - val_accuracy: 0.9910 - 63ms/epoch - 4ms/step\nEpoch 145/500\n14/14 - 0s - loss: 0.0843 - accuracy: 1.0000 - val_loss: 0.0845 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 146/500\n14/14 - 0s - loss: 0.0833 - accuracy: 1.0000 - val_loss: 0.0836 - val_accuracy: 0.9910 - 47ms/epoch - 3ms/step\nEpoch 147/500\n14/14 - 0s - loss: 0.0823 - accuracy: 1.0000 - val_loss: 0.0827 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 148/500\n14/14 - 0s - loss: 0.0812 - accuracy: 1.0000 - val_loss: 0.0819 - val_accuracy: 1.0000 - 63ms/epoch - 4ms/step\nEpoch 149/500\n14/14 - 0s - loss: 0.0802 - accuracy: 1.0000 - val_loss: 0.0811 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 150/500\n14/14 - 0s - loss: 0.0792 - accuracy: 1.0000 - val_loss: 0.0801 - val_accuracy: 0.9910 - 57ms/epoch - 4ms/step\nEpoch 151/500\n14/14 - 0s - loss: 0.0785 - accuracy: 1.0000 - val_loss: 0.0793 - val_accuracy: 0.9910 - 49ms/epoch - 3ms/step\nEpoch 152/500\n14/14 - 0s - loss: 0.0776 - accuracy: 1.0000 - val_loss: 0.0784 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 153/500\n14/14 - 0s - loss: 0.0764 - accuracy: 1.0000 - val_loss: 0.0775 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 154/500\n14/14 - 0s - loss: 0.0756 - accuracy: 1.0000 - val_loss: 0.0767 - val_accuracy: 1.0000 - 62ms/epoch - 4ms/step\nEpoch 155/500\n14/14 - 0s - loss: 0.0746 - accuracy: 1.0000 - val_loss: 0.0759 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 156/500\n14/14 - 0s - loss: 0.0738 - accuracy: 1.0000 - val_loss: 0.0752 - val_accuracy: 1.0000 - 62ms/epoch - 4ms/step\nEpoch 157/500\n14/14 - 0s - loss: 0.0730 - accuracy: 1.0000 - val_loss: 0.0743 - val_accuracy: 0.9910 - 47ms/epoch - 3ms/step\nEpoch 158/500\n14/14 - 0s - loss: 0.0721 - accuracy: 1.0000 - val_loss: 0.0736 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 159/500\n14/14 - 0s - loss: 0.0715 - accuracy: 1.0000 - val_loss: 0.0729 - val_accuracy: 0.9910 - 62ms/epoch - 4ms/step\nEpoch 160/500\n14/14 - 0s - loss: 0.0705 - accuracy: 1.0000 - val_loss: 0.0722 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 161/500\n14/14 - 0s - loss: 0.0699 - accuracy: 0.9955 - val_loss: 0.0714 - val_accuracy: 1.0000 - 52ms/epoch - 4ms/step\nEpoch 162/500\n14/14 - 0s - loss: 0.0688 - accuracy: 1.0000 - val_loss: 0.0707 - val_accuracy: 1.0000 - 49ms/epoch - 3ms/step\nEpoch 163/500\n14/14 - 0s - loss: 0.0681 - accuracy: 1.0000 - val_loss: 0.0699 - val_accuracy: 0.9910 - 47ms/epoch - 3ms/step\nEpoch 164/500\n14/14 - 0s - loss: 0.0671 - accuracy: 1.0000 - val_loss: 0.0692 - val_accuracy: 1.0000 - 62ms/epoch - 4ms/step\nEpoch 165/500\n14/14 - 0s - loss: 0.0665 - accuracy: 1.0000 - val_loss: 0.0684 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 166/500\n14/14 - 0s - loss: 0.0659 - accuracy: 1.0000 - val_loss: 0.0678 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 167/500\n14/14 - 0s - loss: 0.0649 - accuracy: 1.0000 - val_loss: 0.0671 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 168/500\n14/14 - 0s - loss: 0.0642 - accuracy: 1.0000 - val_loss: 0.0663 - val_accuracy: 1.0000 - 62ms/epoch - 4ms/step\nEpoch 169/500\n14/14 - 0s - loss: 0.0635 - accuracy: 1.0000 - val_loss: 0.0656 - val_accuracy: 1.0000 - 41ms/epoch - 3ms/step\nEpoch 170/500\n14/14 - 0s - loss: 0.0629 - accuracy: 1.0000 - val_loss: 0.0650 - val_accuracy: 1.0000 - 49ms/epoch - 3ms/step\nEpoch 171/500\n14/14 - 0s - loss: 0.0622 - accuracy: 1.0000 - val_loss: 0.0644 - val_accuracy: 1.0000 - 51ms/epoch - 4ms/step\nEpoch 172/500\n14/14 - 0s - loss: 0.0615 - accuracy: 1.0000 - val_loss: 0.0637 - val_accuracy: 0.9910 - 62ms/epoch - 4ms/step\nEpoch 173/500\n14/14 - 0s - loss: 0.0608 - accuracy: 1.0000 - val_loss: 0.0630 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 174/500\n14/14 - 0s - loss: 0.0600 - accuracy: 1.0000 - val_loss: 0.0624 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 175/500\n14/14 - 0s - loss: 0.0594 - accuracy: 1.0000 - val_loss: 0.0618 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 176/500\n14/14 - 0s - loss: 0.0587 - accuracy: 1.0000 - val_loss: 0.0611 - val_accuracy: 1.0000 - 62ms/epoch - 4ms/step\nEpoch 177/500\n14/14 - 0s - loss: 0.0581 - accuracy: 1.0000 - val_loss: 0.0605 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 178/500\n14/14 - 0s - loss: 0.0575 - accuracy: 1.0000 - val_loss: 0.0600 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 179/500\n14/14 - 0s - loss: 0.0572 - accuracy: 1.0000 - val_loss: 0.0592 - val_accuracy: 0.9910 - 43ms/epoch - 3ms/step\nEpoch 180/500\n14/14 - 0s - loss: 0.0562 - accuracy: 1.0000 - val_loss: 0.0588 - val_accuracy: 1.0000 - 52ms/epoch - 4ms/step\nEpoch 181/500\n14/14 - 0s - loss: 0.0555 - accuracy: 1.0000 - val_loss: 0.0582 - val_accuracy: 1.0000 - 62ms/epoch - 4ms/step\nEpoch 182/500\n14/14 - 0s - loss: 0.0549 - accuracy: 1.0000 - val_loss: 0.0574 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 183/500\n14/14 - 0s - loss: 0.0544 - accuracy: 1.0000 - val_loss: 0.0570 - val_accuracy: 1.0000 - 61ms/epoch - 4ms/step\nEpoch 184/500\n14/14 - 0s - loss: 0.0538 - accuracy: 1.0000 - val_loss: 0.0563 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 185/500\n14/14 - 0s - loss: 0.0531 - accuracy: 1.0000 - val_loss: 0.0557 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 186/500\n14/14 - 0s - loss: 0.0525 - accuracy: 1.0000 - val_loss: 0.0553 - val_accuracy: 1.0000 - 62ms/epoch - 4ms/step\nEpoch 187/500\n14/14 - 0s - loss: 0.0523 - accuracy: 0.9955 - val_loss: 0.0547 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 188/500\n14/14 - 0s - loss: 0.0514 - accuracy: 1.0000 - val_loss: 0.0542 - val_accuracy: 1.0000 - 57ms/epoch - 4ms/step\nEpoch 189/500\n14/14 - 0s - loss: 0.0509 - accuracy: 1.0000 - val_loss: 0.0536 - val_accuracy: 1.0000 - 46ms/epoch - 3ms/step\nEpoch 190/500\n14/14 - 0s - loss: 0.0505 - accuracy: 1.0000 - val_loss: 0.0531 - val_accuracy: 1.0000 - 53ms/epoch - 4ms/step\nEpoch 191/500\n14/14 - 0s - loss: 0.0498 - accuracy: 1.0000 - val_loss: 0.0527 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 192/500\n14/14 - 0s - loss: 0.0493 - accuracy: 1.0000 - val_loss: 0.0521 - val_accuracy: 1.0000 - 62ms/epoch - 4ms/step\nEpoch 193/500\n14/14 - 0s - loss: 0.0488 - accuracy: 1.0000 - val_loss: 0.0515 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 194/500\n14/14 - 0s - loss: 0.0483 - accuracy: 1.0000 - val_loss: 0.0511 - val_accuracy: 1.0000 - 62ms/epoch - 4ms/step\nEpoch 195/500\n14/14 - 0s - loss: 0.0478 - accuracy: 1.0000 - val_loss: 0.0506 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 196/500\n14/14 - 0s - loss: 0.0473 - accuracy: 1.0000 - val_loss: 0.0501 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 197/500\n14/14 - 0s - loss: 0.0468 - accuracy: 1.0000 - val_loss: 0.0496 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 198/500\n14/14 - 0s - loss: 0.0464 - accuracy: 1.0000 - val_loss: 0.0492 - val_accuracy: 1.0000 - 41ms/epoch - 3ms/step\nEpoch 199/500\n14/14 - 0s - loss: 0.0460 - accuracy: 1.0000 - val_loss: 0.0487 - val_accuracy: 1.0000 - 53ms/epoch - 4ms/step\nEpoch 200/500\n14/14 - 0s - loss: 0.0454 - accuracy: 1.0000 - val_loss: 0.0482 - val_accuracy: 1.0000 - 45ms/epoch - 3ms/step\nEpoch 201/500\n14/14 - 0s - loss: 0.0450 - accuracy: 1.0000 - val_loss: 0.0478 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 202/500\n14/14 - 0s - loss: 0.0446 - accuracy: 0.9955 - val_loss: 0.0475 - val_accuracy: 1.0000 - 63ms/epoch - 4ms/step\nEpoch 203/500\n14/14 - 0s - loss: 0.0441 - accuracy: 1.0000 - val_loss: 0.0469 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 204/500\n14/14 - 0s - loss: 0.0436 - accuracy: 1.0000 - val_loss: 0.0465 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 205/500\n14/14 - 0s - loss: 0.0432 - accuracy: 1.0000 - val_loss: 0.0461 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 206/500\n14/14 - 0s - loss: 0.0428 - accuracy: 1.0000 - val_loss: 0.0456 - val_accuracy: 1.0000 - 52ms/epoch - 4ms/step\nEpoch 207/500\n14/14 - 0s - loss: 0.0426 - accuracy: 1.0000 - val_loss: 0.0451 - val_accuracy: 1.0000 - 50ms/epoch - 4ms/step\nEpoch 208/500\n14/14 - 0s - loss: 0.0420 - accuracy: 1.0000 - val_loss: 0.0451 - val_accuracy: 1.0000 - 46ms/epoch - 3ms/step\nEpoch 209/500\n14/14 - 0s - loss: 0.0416 - accuracy: 1.0000 - val_loss: 0.0445 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 210/500\n14/14 - 0s - loss: 0.0413 - accuracy: 1.0000 - val_loss: 0.0443 - val_accuracy: 1.0000 - 41ms/epoch - 3ms/step\nEpoch 211/500\n14/14 - 0s - loss: 0.0408 - accuracy: 1.0000 - val_loss: 0.0436 - val_accuracy: 1.0000 - 62ms/epoch - 4ms/step\nEpoch 212/500\n14/14 - 0s - loss: 0.0405 - accuracy: 1.0000 - val_loss: 0.0432 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 213/500\n14/14 - 0s - loss: 0.0400 - accuracy: 1.0000 - val_loss: 0.0428 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 214/500\n14/14 - 0s - loss: 0.0398 - accuracy: 0.9955 - val_loss: 0.0426 - val_accuracy: 1.0000 - 62ms/epoch - 4ms/step\nEpoch 215/500\n14/14 - 0s - loss: 0.0395 - accuracy: 0.9955 - val_loss: 0.0420 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 216/500\n14/14 - 0s - loss: 0.0390 - accuracy: 1.0000 - val_loss: 0.0417 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 217/500\n14/14 - 0s - loss: 0.0386 - accuracy: 1.0000 - val_loss: 0.0414 - val_accuracy: 1.0000 - 53ms/epoch - 4ms/step\nEpoch 218/500\n14/14 - 0s - loss: 0.0382 - accuracy: 1.0000 - val_loss: 0.0410 - val_accuracy: 1.0000 - 65ms/epoch - 5ms/step\nEpoch 219/500\n14/14 - 0s - loss: 0.0381 - accuracy: 0.9955 - val_loss: 0.0408 - val_accuracy: 1.0000 - 46ms/epoch - 3ms/step\nEpoch 220/500\n14/14 - 0s - loss: 0.0376 - accuracy: 0.9955 - val_loss: 0.0405 - val_accuracy: 1.0000 - 54ms/epoch - 4ms/step\nEpoch 221/500\n14/14 - 0s - loss: 0.0372 - accuracy: 1.0000 - val_loss: 0.0399 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 222/500\n14/14 - 0s - loss: 0.0368 - accuracy: 1.0000 - val_loss: 0.0395 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 223/500\n14/14 - 0s - loss: 0.0366 - accuracy: 1.0000 - val_loss: 0.0393 - val_accuracy: 1.0000 - 62ms/epoch - 4ms/step\nEpoch 224/500\n14/14 - 0s - loss: 0.0362 - accuracy: 1.0000 - val_loss: 0.0391 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 225/500\n14/14 - 0s - loss: 0.0360 - accuracy: 1.0000 - val_loss: 0.0386 - val_accuracy: 1.0000 - 49ms/epoch - 3ms/step\nEpoch 226/500\n14/14 - 0s - loss: 0.0357 - accuracy: 1.0000 - val_loss: 0.0382 - val_accuracy: 1.0000 - 50ms/epoch - 4ms/step\nEpoch 227/500\n14/14 - 0s - loss: 0.0354 - accuracy: 1.0000 - val_loss: 0.0382 - val_accuracy: 1.0000 - 48ms/epoch - 3ms/step\nEpoch 228/500\n14/14 - 0s - loss: 0.0350 - accuracy: 1.0000 - val_loss: 0.0379 - val_accuracy: 1.0000 - 63ms/epoch - 4ms/step\nEpoch 229/500\n14/14 - 0s - loss: 0.0347 - accuracy: 1.0000 - val_loss: 0.0374 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 230/500\n14/14 - 0s - loss: 0.0345 - accuracy: 0.9955 - val_loss: 0.0372 - val_accuracy: 1.0000 - 59ms/epoch - 4ms/step\nEpoch 231/500\n14/14 - 0s - loss: 0.0340 - accuracy: 1.0000 - val_loss: 0.0368 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 232/500\n14/14 - 0s - loss: 0.0339 - accuracy: 1.0000 - val_loss: 0.0364 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 233/500\n14/14 - 0s - loss: 0.0335 - accuracy: 1.0000 - val_loss: 0.0361 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 234/500\n14/14 - 0s - loss: 0.0333 - accuracy: 1.0000 - val_loss: 0.0361 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 235/500\n14/14 - 0s - loss: 0.0331 - accuracy: 1.0000 - val_loss: 0.0355 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 236/500\n14/14 - 0s - loss: 0.0327 - accuracy: 1.0000 - val_loss: 0.0354 - val_accuracy: 1.0000 - 53ms/epoch - 4ms/step\nEpoch 237/500\n14/14 - 0s - loss: 0.0324 - accuracy: 1.0000 - val_loss: 0.0351 - val_accuracy: 1.0000 - 60ms/epoch - 4ms/step\nEpoch 238/500\n14/14 - 0s - loss: 0.0322 - accuracy: 0.9955 - val_loss: 0.0350 - val_accuracy: 1.0000 - 49ms/epoch - 3ms/step\nEpoch 239/500\n14/14 - 0s - loss: 0.0319 - accuracy: 0.9955 - val_loss: 0.0345 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 240/500\n14/14 - 0s - loss: 0.0317 - accuracy: 1.0000 - val_loss: 0.0342 - val_accuracy: 1.0000 - 58ms/epoch - 4ms/step\nEpoch 241/500\n14/14 - 0s - loss: 0.0315 - accuracy: 1.0000 - val_loss: 0.0341 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 242/500\n14/14 - 0s - loss: 0.0312 - accuracy: 1.0000 - val_loss: 0.0336 - val_accuracy: 1.0000 - 63ms/epoch - 4ms/step\nEpoch 243/500\n14/14 - 0s - loss: 0.0308 - accuracy: 1.0000 - val_loss: 0.0334 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 244/500\n14/14 - 0s - loss: 0.0306 - accuracy: 1.0000 - val_loss: 0.0332 - val_accuracy: 1.0000 - 60ms/epoch - 4ms/step\nEpoch 245/500\n14/14 - 0s - loss: 0.0303 - accuracy: 1.0000 - val_loss: 0.0328 - val_accuracy: 1.0000 - 48ms/epoch - 3ms/step\nEpoch 246/500\n14/14 - 0s - loss: 0.0301 - accuracy: 1.0000 - val_loss: 0.0326 - val_accuracy: 1.0000 - 50ms/epoch - 4ms/step\nEpoch 247/500\n14/14 - 0s - loss: 0.0300 - accuracy: 0.9955 - val_loss: 0.0325 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 248/500\n14/14 - 0s - loss: 0.0296 - accuracy: 1.0000 - val_loss: 0.0320 - val_accuracy: 1.0000 - 63ms/epoch - 4ms/step\nEpoch 249/500\n14/14 - 0s - loss: 0.0294 - accuracy: 1.0000 - val_loss: 0.0318 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 250/500\n14/14 - 0s - loss: 0.0292 - accuracy: 1.0000 - val_loss: 0.0316 - val_accuracy: 1.0000 - 53ms/epoch - 4ms/step\nEpoch 251/500\n14/14 - 0s - loss: 0.0289 - accuracy: 1.0000 - val_loss: 0.0314 - val_accuracy: 1.0000 - 57ms/epoch - 4ms/step\nEpoch 252/500\n14/14 - 0s - loss: 0.0287 - accuracy: 1.0000 - val_loss: 0.0311 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 253/500\n14/14 - 0s - loss: 0.0287 - accuracy: 0.9955 - val_loss: 0.0310 - val_accuracy: 1.0000 - 51ms/epoch - 4ms/step\nEpoch 254/500\n14/14 - 0s - loss: 0.0283 - accuracy: 0.9955 - val_loss: 0.0306 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 255/500\n14/14 - 0s - loss: 0.0280 - accuracy: 1.0000 - val_loss: 0.0303 - val_accuracy: 1.0000 - 61ms/epoch - 4ms/step\nEpoch 256/500\n14/14 - 0s - loss: 0.0278 - accuracy: 1.0000 - val_loss: 0.0302 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 257/500\n14/14 - 0s - loss: 0.0277 - accuracy: 1.0000 - val_loss: 0.0299 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 258/500\n14/14 - 0s - loss: 0.0275 - accuracy: 1.0000 - val_loss: 0.0298 - val_accuracy: 1.0000 - 63ms/epoch - 4ms/step\nEpoch 259/500\n14/14 - 0s - loss: 0.0272 - accuracy: 1.0000 - val_loss: 0.0296 - val_accuracy: 1.0000 - 62ms/epoch - 4ms/step\nEpoch 260/500\n14/14 - 0s - loss: 0.0269 - accuracy: 1.0000 - val_loss: 0.0293 - val_accuracy: 1.0000 - 46ms/epoch - 3ms/step\nEpoch 261/500\n14/14 - 0s - loss: 0.0267 - accuracy: 1.0000 - val_loss: 0.0290 - val_accuracy: 1.0000 - 63ms/epoch - 4ms/step\nEpoch 262/500\n14/14 - 0s - loss: 0.0268 - accuracy: 1.0000 - val_loss: 0.0287 - val_accuracy: 1.0000 - 55ms/epoch - 4ms/step\nEpoch 263/500\n14/14 - 0s - loss: 0.0263 - accuracy: 1.0000 - val_loss: 0.0286 - val_accuracy: 1.0000 - 51ms/epoch - 4ms/step\nEpoch 264/500\n14/14 - 0s - loss: 0.0262 - accuracy: 1.0000 - val_loss: 0.0285 - val_accuracy: 1.0000 - 50ms/epoch - 4ms/step\nEpoch 265/500\n14/14 - 0s - loss: 0.0261 - accuracy: 1.0000 - val_loss: 0.0283 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 266/500\n14/14 - 0s - loss: 0.0258 - accuracy: 1.0000 - val_loss: 0.0282 - val_accuracy: 1.0000 - 62ms/epoch - 4ms/step\nEpoch 267/500\n14/14 - 0s - loss: 0.0257 - accuracy: 0.9955 - val_loss: 0.0279 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 268/500\n14/14 - 0s - loss: 0.0254 - accuracy: 0.9955 - val_loss: 0.0277 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 269/500\n14/14 - 0s - loss: 0.0252 - accuracy: 1.0000 - val_loss: 0.0274 - val_accuracy: 1.0000 - 64ms/epoch - 5ms/step\nEpoch 270/500\n14/14 - 0s - loss: 0.0251 - accuracy: 1.0000 - val_loss: 0.0271 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 271/500\n14/14 - 0s - loss: 0.0249 - accuracy: 1.0000 - val_loss: 0.0270 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 272/500\n14/14 - 0s - loss: 0.0247 - accuracy: 1.0000 - val_loss: 0.0269 - val_accuracy: 1.0000 - 63ms/epoch - 4ms/step\nEpoch 273/500\n14/14 - 0s - loss: 0.0245 - accuracy: 1.0000 - val_loss: 0.0267 - val_accuracy: 1.0000 - 52ms/epoch - 4ms/step\nEpoch 274/500\n14/14 - 0s - loss: 0.0244 - accuracy: 1.0000 - val_loss: 0.0266 - val_accuracy: 1.0000 - 49ms/epoch - 3ms/step\nEpoch 275/500\n14/14 - 0s - loss: 0.0243 - accuracy: 1.0000 - val_loss: 0.0263 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 276/500\n14/14 - 0s - loss: 0.0241 - accuracy: 1.0000 - val_loss: 0.0262 - val_accuracy: 1.0000 - 62ms/epoch - 4ms/step\nEpoch 277/500\n14/14 - 0s - loss: 0.0238 - accuracy: 1.0000 - val_loss: 0.0259 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 278/500\n14/14 - 0s - loss: 0.0240 - accuracy: 1.0000 - val_loss: 0.0257 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 279/500\n14/14 - 0s - loss: 0.0235 - accuracy: 1.0000 - val_loss: 0.0256 - val_accuracy: 1.0000 - 65ms/epoch - 5ms/step\nEpoch 280/500\n14/14 - 0s - loss: 0.0234 - accuracy: 1.0000 - val_loss: 0.0255 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 281/500\n14/14 - 0s - loss: 0.0232 - accuracy: 1.0000 - val_loss: 0.0252 - val_accuracy: 1.0000 - 69ms/epoch - 5ms/step\nEpoch 282/500\n14/14 - 0s - loss: 0.0231 - accuracy: 1.0000 - val_loss: 0.0250 - val_accuracy: 1.0000 - 46ms/epoch - 3ms/step\nEpoch 283/500\n14/14 - 0s - loss: 0.0230 - accuracy: 1.0000 - val_loss: 0.0250 - val_accuracy: 1.0000 - 52ms/epoch - 4ms/step\nEpoch 284/500\n14/14 - 0s - loss: 0.0227 - accuracy: 1.0000 - val_loss: 0.0248 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 285/500\n14/14 - 0s - loss: 0.0226 - accuracy: 1.0000 - val_loss: 0.0245 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 286/500\n14/14 - 0s - loss: 0.0227 - accuracy: 1.0000 - val_loss: 0.0244 - val_accuracy: 1.0000 - 51ms/epoch - 4ms/step\nEpoch 287/500\n14/14 - 0s - loss: 0.0223 - accuracy: 1.0000 - val_loss: 0.0243 - val_accuracy: 1.0000 - 62ms/epoch - 4ms/step\nEpoch 288/500\n14/14 - 0s - loss: 0.0223 - accuracy: 1.0000 - val_loss: 0.0243 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 289/500\n14/14 - 0s - loss: 0.0219 - accuracy: 1.0000 - val_loss: 0.0241 - val_accuracy: 1.0000 - 49ms/epoch - 3ms/step\nEpoch 290/500\n14/14 - 0s - loss: 0.0221 - accuracy: 1.0000 - val_loss: 0.0237 - val_accuracy: 1.0000 - 62ms/epoch - 4ms/step\nEpoch 291/500\n14/14 - 0s - loss: 0.0219 - accuracy: 1.0000 - val_loss: 0.0237 - val_accuracy: 1.0000 - 59ms/epoch - 4ms/step\nEpoch 292/500\n14/14 - 0s - loss: 0.0216 - accuracy: 1.0000 - val_loss: 0.0236 - val_accuracy: 1.0000 - 46ms/epoch - 3ms/step\nEpoch 293/500\n14/14 - 0s - loss: 0.0214 - accuracy: 1.0000 - val_loss: 0.0234 - val_accuracy: 1.0000 - 62ms/epoch - 4ms/step\nEpoch 294/500\n14/14 - 0s - loss: 0.0213 - accuracy: 1.0000 - val_loss: 0.0232 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 295/500\n14/14 - 0s - loss: 0.0212 - accuracy: 1.0000 - val_loss: 0.0230 - val_accuracy: 1.0000 - 62ms/epoch - 4ms/step\nEpoch 296/500\n14/14 - 0s - loss: 0.0211 - accuracy: 1.0000 - val_loss: 0.0229 - val_accuracy: 1.0000 - 79ms/epoch - 6ms/step\nEpoch 297/500\n14/14 - 0s - loss: 0.0210 - accuracy: 1.0000 - val_loss: 0.0227 - val_accuracy: 1.0000 - 64ms/epoch - 5ms/step\nEpoch 298/500\n14/14 - 0s - loss: 0.0208 - accuracy: 1.0000 - val_loss: 0.0225 - val_accuracy: 1.0000 - 70ms/epoch - 5ms/step\nEpoch 299/500\n14/14 - 0s - loss: 0.0206 - accuracy: 1.0000 - val_loss: 0.0225 - val_accuracy: 1.0000 - 65ms/epoch - 5ms/step\nEpoch 300/500\n14/14 - 0s - loss: 0.0205 - accuracy: 1.0000 - val_loss: 0.0223 - val_accuracy: 1.0000 - 83ms/epoch - 6ms/step\nEpoch 301/500\n14/14 - 0s - loss: 0.0204 - accuracy: 1.0000 - val_loss: 0.0222 - val_accuracy: 1.0000 - 62ms/epoch - 4ms/step\nEpoch 302/500\n14/14 - 0s - loss: 0.0202 - accuracy: 1.0000 - val_loss: 0.0219 - val_accuracy: 1.0000 - 75ms/epoch - 5ms/step\nEpoch 303/500\n14/14 - 0s - loss: 0.0201 - accuracy: 1.0000 - val_loss: 0.0219 - val_accuracy: 1.0000 - 63ms/epoch - 4ms/step\nEpoch 304/500\n14/14 - 0s - loss: 0.0200 - accuracy: 1.0000 - val_loss: 0.0219 - val_accuracy: 1.0000 - 62ms/epoch - 4ms/step\nEpoch 305/500\n14/14 - 0s - loss: 0.0199 - accuracy: 1.0000 - val_loss: 0.0217 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 306/500\n14/14 - 0s - loss: 0.0201 - accuracy: 1.0000 - val_loss: 0.0215 - val_accuracy: 1.0000 - 58ms/epoch - 4ms/step\nEpoch 307/500\n14/14 - 0s - loss: 0.0197 - accuracy: 1.0000 - val_loss: 0.0214 - val_accuracy: 1.0000 - 52ms/epoch - 4ms/step\nEpoch 308/500\n14/14 - 0s - loss: 0.0195 - accuracy: 1.0000 - val_loss: 0.0213 - val_accuracy: 1.0000 - 64ms/epoch - 5ms/step\nEpoch 309/500\n14/14 - 0s - loss: 0.0193 - accuracy: 1.0000 - val_loss: 0.0211 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 310/500\n14/14 - 0s - loss: 0.0192 - accuracy: 1.0000 - val_loss: 0.0211 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 311/500\n14/14 - 0s - loss: 0.0192 - accuracy: 1.0000 - val_loss: 0.0209 - val_accuracy: 1.0000 - 62ms/epoch - 4ms/step\nEpoch 312/500\n14/14 - 0s - loss: 0.0196 - accuracy: 1.0000 - val_loss: 0.0207 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 313/500\n14/14 - 0s - loss: 0.0189 - accuracy: 1.0000 - val_loss: 0.0207 - val_accuracy: 1.0000 - 62ms/epoch - 4ms/step\nEpoch 314/500\n14/14 - 0s - loss: 0.0188 - accuracy: 1.0000 - val_loss: 0.0206 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 315/500\n14/14 - 0s - loss: 0.0187 - accuracy: 1.0000 - val_loss: 0.0204 - val_accuracy: 1.0000 - 54ms/epoch - 4ms/step\nEpoch 316/500\n14/14 - 0s - loss: 0.0185 - accuracy: 1.0000 - val_loss: 0.0202 - val_accuracy: 1.0000 - 48ms/epoch - 3ms/step\nEpoch 317/500\n14/14 - 0s - loss: 0.0184 - accuracy: 1.0000 - val_loss: 0.0201 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 318/500\n14/14 - 0s - loss: 0.0184 - accuracy: 1.0000 - val_loss: 0.0201 - val_accuracy: 1.0000 - 38ms/epoch - 3ms/step\nEpoch 319/500\n14/14 - 0s - loss: 0.0182 - accuracy: 1.0000 - val_loss: 0.0199 - val_accuracy: 1.0000 - 62ms/epoch - 4ms/step\nEpoch 320/500\n14/14 - 0s - loss: 0.0181 - accuracy: 1.0000 - val_loss: 0.0198 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 321/500\n14/14 - 0s - loss: 0.0180 - accuracy: 1.0000 - val_loss: 0.0197 - val_accuracy: 1.0000 - 62ms/epoch - 4ms/step\nEpoch 322/500\n14/14 - 0s - loss: 0.0179 - accuracy: 1.0000 - val_loss: 0.0196 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 323/500\n14/14 - 0s - loss: 0.0179 - accuracy: 1.0000 - val_loss: 0.0195 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 324/500\n14/14 - 0s - loss: 0.0177 - accuracy: 1.0000 - val_loss: 0.0193 - val_accuracy: 1.0000 - 62ms/epoch - 4ms/step\nEpoch 325/500\n14/14 - 0s - loss: 0.0176 - accuracy: 1.0000 - val_loss: 0.0193 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 326/500\n14/14 - 0s - loss: 0.0177 - accuracy: 1.0000 - val_loss: 0.0192 - val_accuracy: 1.0000 - 63ms/epoch - 4ms/step\nEpoch 327/500\n14/14 - 0s - loss: 0.0174 - accuracy: 1.0000 - val_loss: 0.0190 - val_accuracy: 1.0000 - 45ms/epoch - 3ms/step\nEpoch 328/500\n14/14 - 0s - loss: 0.0174 - accuracy: 1.0000 - val_loss: 0.0189 - val_accuracy: 1.0000 - 61ms/epoch - 4ms/step\nEpoch 329/500\n14/14 - 0s - loss: 0.0173 - accuracy: 1.0000 - val_loss: 0.0188 - val_accuracy: 1.0000 - 48ms/epoch - 3ms/step\nEpoch 330/500\n14/14 - 0s - loss: 0.0171 - accuracy: 1.0000 - val_loss: 0.0188 - val_accuracy: 1.0000 - 63ms/epoch - 4ms/step\nEpoch 331/500\n14/14 - 0s - loss: 0.0171 - accuracy: 1.0000 - val_loss: 0.0187 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 332/500\n14/14 - 0s - loss: 0.0169 - accuracy: 1.0000 - val_loss: 0.0185 - val_accuracy: 1.0000 - 63ms/epoch - 5ms/step\nEpoch 333/500\n14/14 - 0s - loss: 0.0168 - accuracy: 1.0000 - val_loss: 0.0184 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 334/500\n14/14 - 0s - loss: 0.0167 - accuracy: 1.0000 - val_loss: 0.0183 - val_accuracy: 1.0000 - 54ms/epoch - 4ms/step\nEpoch 335/500\n14/14 - 0s - loss: 0.0167 - accuracy: 1.0000 - val_loss: 0.0183 - val_accuracy: 1.0000 - 50ms/epoch - 4ms/step\nEpoch 336/500\n14/14 - 0s - loss: 0.0166 - accuracy: 1.0000 - val_loss: 0.0182 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 337/500\n14/14 - 0s - loss: 0.0164 - accuracy: 1.0000 - val_loss: 0.0180 - val_accuracy: 1.0000 - 52ms/epoch - 4ms/step\nEpoch 338/500\n14/14 - 0s - loss: 0.0166 - accuracy: 1.0000 - val_loss: 0.0179 - val_accuracy: 1.0000 - 48ms/epoch - 3ms/step\nEpoch 339/500\n14/14 - 0s - loss: 0.0164 - accuracy: 1.0000 - val_loss: 0.0179 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 340/500\n14/14 - 0s - loss: 0.0162 - accuracy: 1.0000 - val_loss: 0.0178 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 341/500\n14/14 - 0s - loss: 0.0162 - accuracy: 1.0000 - val_loss: 0.0177 - val_accuracy: 1.0000 - 62ms/epoch - 4ms/step\nEpoch 342/500\n14/14 - 0s - loss: 0.0160 - accuracy: 1.0000 - val_loss: 0.0175 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 343/500\n14/14 - 0s - loss: 0.0160 - accuracy: 1.0000 - val_loss: 0.0174 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 344/500\n14/14 - 0s - loss: 0.0163 - accuracy: 1.0000 - val_loss: 0.0174 - val_accuracy: 1.0000 - 62ms/epoch - 4ms/step\nEpoch 345/500\n14/14 - 0s - loss: 0.0157 - accuracy: 1.0000 - val_loss: 0.0174 - val_accuracy: 1.0000 - 56ms/epoch - 4ms/step\nEpoch 346/500\n14/14 - 0s - loss: 0.0157 - accuracy: 1.0000 - val_loss: 0.0173 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 347/500\n14/14 - 0s - loss: 0.0155 - accuracy: 1.0000 - val_loss: 0.0171 - val_accuracy: 1.0000 - 48ms/epoch - 3ms/step\nEpoch 348/500\n14/14 - 0s - loss: 0.0156 - accuracy: 1.0000 - val_loss: 0.0170 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 349/500\n14/14 - 0s - loss: 0.0155 - accuracy: 1.0000 - val_loss: 0.0169 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 350/500\n14/14 - 0s - loss: 0.0153 - accuracy: 1.0000 - val_loss: 0.0169 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 351/500\n14/14 - 0s - loss: 0.0153 - accuracy: 1.0000 - val_loss: 0.0168 - val_accuracy: 1.0000 - 62ms/epoch - 4ms/step\nEpoch 352/500\n14/14 - 0s - loss: 0.0152 - accuracy: 1.0000 - val_loss: 0.0168 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 353/500\n14/14 - 0s - loss: 0.0151 - accuracy: 1.0000 - val_loss: 0.0167 - val_accuracy: 1.0000 - 54ms/epoch - 4ms/step\nEpoch 354/500\n14/14 - 0s - loss: 0.0150 - accuracy: 1.0000 - val_loss: 0.0166 - val_accuracy: 1.0000 - 63ms/epoch - 4ms/step\nEpoch 355/500\n14/14 - 0s - loss: 0.0149 - accuracy: 1.0000 - val_loss: 0.0165 - val_accuracy: 1.0000 - 54ms/epoch - 4ms/step\nEpoch 356/500\n14/14 - 0s - loss: 0.0150 - accuracy: 1.0000 - val_loss: 0.0165 - val_accuracy: 1.0000 - 50ms/epoch - 4ms/step\nEpoch 357/500\n14/14 - 0s - loss: 0.0148 - accuracy: 1.0000 - val_loss: 0.0164 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 358/500\n14/14 - 0s - loss: 0.0147 - accuracy: 1.0000 - val_loss: 0.0163 - val_accuracy: 1.0000 - 62ms/epoch - 4ms/step\nEpoch 359/500\n14/14 - 0s - loss: 0.0148 - accuracy: 1.0000 - val_loss: 0.0161 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 360/500\n14/14 - 0s - loss: 0.0146 - accuracy: 1.0000 - val_loss: 0.0161 - val_accuracy: 1.0000 - 62ms/epoch - 4ms/step\nEpoch 361/500\n14/14 - 0s - loss: 0.0144 - accuracy: 1.0000 - val_loss: 0.0160 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 362/500\n14/14 - 0s - loss: 0.0144 - accuracy: 1.0000 - val_loss: 0.0160 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 363/500\n14/14 - 0s - loss: 0.0145 - accuracy: 1.0000 - val_loss: 0.0159 - val_accuracy: 1.0000 - 68ms/epoch - 5ms/step\nEpoch 364/500\n14/14 - 0s - loss: 0.0143 - accuracy: 1.0000 - val_loss: 0.0159 - val_accuracy: 1.0000 - 50ms/epoch - 4ms/step\nEpoch 365/500\n14/14 - 0s - loss: 0.0142 - accuracy: 1.0000 - val_loss: 0.0159 - val_accuracy: 1.0000 - 51ms/epoch - 4ms/step\nEpoch 366/500\n14/14 - 0s - loss: 0.0143 - accuracy: 1.0000 - val_loss: 0.0157 - val_accuracy: 1.0000 - 48ms/epoch - 3ms/step\nEpoch 367/500\n14/14 - 0s - loss: 0.0141 - accuracy: 1.0000 - val_loss: 0.0156 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 368/500\n14/14 - 0s - loss: 0.0140 - accuracy: 1.0000 - val_loss: 0.0156 - val_accuracy: 1.0000 - 62ms/epoch - 4ms/step\nEpoch 369/500\n14/14 - 0s - loss: 0.0140 - accuracy: 1.0000 - val_loss: 0.0155 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 370/500\n14/14 - 0s - loss: 0.0140 - accuracy: 1.0000 - val_loss: 0.0154 - val_accuracy: 1.0000 - 62ms/epoch - 4ms/step\nEpoch 371/500\n14/14 - 0s - loss: 0.0138 - accuracy: 1.0000 - val_loss: 0.0153 - val_accuracy: 1.0000 - 51ms/epoch - 4ms/step\nEpoch 372/500\n14/14 - 0s - loss: 0.0138 - accuracy: 1.0000 - val_loss: 0.0153 - val_accuracy: 1.0000 - 64ms/epoch - 5ms/step\nEpoch 373/500\n14/14 - 0s - loss: 0.0136 - accuracy: 1.0000 - val_loss: 0.0152 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 374/500\n14/14 - 0s - loss: 0.0137 - accuracy: 1.0000 - val_loss: 0.0152 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 375/500\n14/14 - 0s - loss: 0.0137 - accuracy: 1.0000 - val_loss: 0.0150 - val_accuracy: 1.0000 - 57ms/epoch - 4ms/step\nEpoch 376/500\n14/14 - 0s - loss: 0.0136 - accuracy: 1.0000 - val_loss: 0.0151 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 377/500\n14/14 - 0s - loss: 0.0135 - accuracy: 1.0000 - val_loss: 0.0150 - val_accuracy: 1.0000 - 64ms/epoch - 5ms/step\nEpoch 378/500\n14/14 - 0s - loss: 0.0135 - accuracy: 1.0000 - val_loss: 0.0148 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 379/500\n14/14 - 0s - loss: 0.0134 - accuracy: 1.0000 - val_loss: 0.0148 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 380/500\n14/14 - 0s - loss: 0.0135 - accuracy: 1.0000 - val_loss: 0.0148 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 381/500\n14/14 - 0s - loss: 0.0132 - accuracy: 1.0000 - val_loss: 0.0147 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 382/500\n14/14 - 0s - loss: 0.0132 - accuracy: 1.0000 - val_loss: 0.0147 - val_accuracy: 1.0000 - 60ms/epoch - 4ms/step\nEpoch 383/500\n14/14 - 0s - loss: 0.0132 - accuracy: 1.0000 - val_loss: 0.0146 - val_accuracy: 1.0000 - 41ms/epoch - 3ms/step\nEpoch 384/500\n14/14 - 0s - loss: 0.0131 - accuracy: 1.0000 - val_loss: 0.0145 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 385/500\n14/14 - 0s - loss: 0.0130 - accuracy: 1.0000 - val_loss: 0.0145 - val_accuracy: 1.0000 - 54ms/epoch - 4ms/step\nEpoch 386/500\n14/14 - 0s - loss: 0.0130 - accuracy: 1.0000 - val_loss: 0.0143 - val_accuracy: 1.0000 - 62ms/epoch - 4ms/step\nEpoch 387/500\n14/14 - 0s - loss: 0.0128 - accuracy: 1.0000 - val_loss: 0.0143 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 388/500\n14/14 - 0s - loss: 0.0128 - accuracy: 1.0000 - val_loss: 0.0142 - val_accuracy: 1.0000 - 62ms/epoch - 4ms/step\nEpoch 389/500\n14/14 - 0s - loss: 0.0128 - accuracy: 1.0000 - val_loss: 0.0143 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 390/500\n14/14 - 0s - loss: 0.0129 - accuracy: 1.0000 - val_loss: 0.0142 - val_accuracy: 1.0000 - 48ms/epoch - 3ms/step\nEpoch 391/500\n14/14 - 0s - loss: 0.0128 - accuracy: 1.0000 - val_loss: 0.0141 - val_accuracy: 1.0000 - 48ms/epoch - 3ms/step\nEpoch 392/500\n14/14 - 0s - loss: 0.0126 - accuracy: 1.0000 - val_loss: 0.0141 - val_accuracy: 1.0000 - 62ms/epoch - 4ms/step\nEpoch 393/500\n14/14 - 0s - loss: 0.0126 - accuracy: 1.0000 - val_loss: 0.0140 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 394/500\n14/14 - 0s - loss: 0.0125 - accuracy: 1.0000 - val_loss: 0.0139 - val_accuracy: 1.0000 - 60ms/epoch - 4ms/step\nEpoch 395/500\n14/14 - 0s - loss: 0.0125 - accuracy: 1.0000 - val_loss: 0.0138 - val_accuracy: 1.0000 - 48ms/epoch - 3ms/step\nEpoch 396/500\n14/14 - 0s - loss: 0.0124 - accuracy: 1.0000 - val_loss: 0.0138 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 397/500\n14/14 - 0s - loss: 0.0124 - accuracy: 1.0000 - val_loss: 0.0138 - val_accuracy: 1.0000 - 62ms/epoch - 4ms/step\nEpoch 398/500\n14/14 - 0s - loss: 0.0123 - accuracy: 1.0000 - val_loss: 0.0137 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 399/500\n14/14 - 0s - loss: 0.0123 - accuracy: 1.0000 - val_loss: 0.0137 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 400/500\n14/14 - 0s - loss: 0.0123 - accuracy: 1.0000 - val_loss: 0.0136 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 401/500\n14/14 - 0s - loss: 0.0121 - accuracy: 1.0000 - val_loss: 0.0136 - val_accuracy: 1.0000 - 53ms/epoch - 4ms/step\nEpoch 402/500\n14/14 - 0s - loss: 0.0122 - accuracy: 1.0000 - val_loss: 0.0136 - val_accuracy: 1.0000 - 46ms/epoch - 3ms/step\nEpoch 403/500\n14/14 - 0s - loss: 0.0120 - accuracy: 1.0000 - val_loss: 0.0134 - val_accuracy: 1.0000 - 62ms/epoch - 4ms/step\nEpoch 404/500\n14/14 - 0s - loss: 0.0120 - accuracy: 1.0000 - val_loss: 0.0134 - val_accuracy: 1.0000 - 53ms/epoch - 4ms/step\nEpoch 405/500\n14/14 - 0s - loss: 0.0120 - accuracy: 1.0000 - val_loss: 0.0134 - val_accuracy: 1.0000 - 52ms/epoch - 4ms/step\nEpoch 406/500\n14/14 - 0s - loss: 0.0119 - accuracy: 1.0000 - val_loss: 0.0133 - val_accuracy: 1.0000 - 62ms/epoch - 4ms/step\nEpoch 407/500\n14/14 - 0s - loss: 0.0119 - accuracy: 1.0000 - val_loss: 0.0133 - val_accuracy: 1.0000 - 63ms/epoch - 4ms/step\nEpoch 408/500\n14/14 - 0s - loss: 0.0120 - accuracy: 1.0000 - val_loss: 0.0131 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 409/500\n14/14 - 0s - loss: 0.0117 - accuracy: 1.0000 - val_loss: 0.0131 - val_accuracy: 1.0000 - 62ms/epoch - 4ms/step\nEpoch 410/500\n14/14 - 0s - loss: 0.0119 - accuracy: 1.0000 - val_loss: 0.0131 - val_accuracy: 1.0000 - 46ms/epoch - 3ms/step\nEpoch 411/500\n14/14 - 0s - loss: 0.0117 - accuracy: 1.0000 - val_loss: 0.0130 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 412/500\n14/14 - 0s - loss: 0.0118 - accuracy: 1.0000 - val_loss: 0.0130 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 413/500\n14/14 - 0s - loss: 0.0118 - accuracy: 1.0000 - val_loss: 0.0129 - val_accuracy: 1.0000 - 45ms/epoch - 3ms/step\nEpoch 414/500\n14/14 - 0s - loss: 0.0118 - accuracy: 1.0000 - val_loss: 0.0129 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 415/500\n14/14 - 0s - loss: 0.0115 - accuracy: 1.0000 - val_loss: 0.0129 - val_accuracy: 1.0000 - 51ms/epoch - 4ms/step\nEpoch 416/500\n14/14 - 0s - loss: 0.0115 - accuracy: 1.0000 - val_loss: 0.0128 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 417/500\n14/14 - 0s - loss: 0.0114 - accuracy: 1.0000 - val_loss: 0.0126 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 418/500\n14/14 - 0s - loss: 0.0114 - accuracy: 1.0000 - val_loss: 0.0126 - val_accuracy: 1.0000 - 62ms/epoch - 4ms/step\nEpoch 419/500\n14/14 - 0s - loss: 0.0115 - accuracy: 1.0000 - val_loss: 0.0124 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 420/500\n14/14 - 0s - loss: 0.0112 - accuracy: 1.0000 - val_loss: 0.0123 - val_accuracy: 1.0000 - 59ms/epoch - 4ms/step\nEpoch 421/500\n14/14 - 0s - loss: 0.0113 - accuracy: 1.0000 - val_loss: 0.0124 - val_accuracy: 1.0000 - 37ms/epoch - 3ms/step\nEpoch 422/500\n14/14 - 0s - loss: 0.0111 - accuracy: 1.0000 - val_loss: 0.0122 - val_accuracy: 1.0000 - 62ms/epoch - 4ms/step\nEpoch 423/500\n14/14 - 0s - loss: 0.0111 - accuracy: 1.0000 - val_loss: 0.0120 - val_accuracy: 1.0000 - 39ms/epoch - 3ms/step\nEpoch 424/500\n14/14 - 0s - loss: 0.0110 - accuracy: 1.0000 - val_loss: 0.0120 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 425/500\n14/14 - 0s - loss: 0.0110 - accuracy: 1.0000 - val_loss: 0.0120 - val_accuracy: 1.0000 - 54ms/epoch - 4ms/step\nEpoch 426/500\n14/14 - 0s - loss: 0.0109 - accuracy: 1.0000 - val_loss: 0.0119 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 427/500\n14/14 - 0s - loss: 0.0110 - accuracy: 1.0000 - val_loss: 0.0119 - val_accuracy: 1.0000 - 55ms/epoch - 4ms/step\nEpoch 428/500\n14/14 - 0s - loss: 0.0109 - accuracy: 1.0000 - val_loss: 0.0119 - val_accuracy: 1.0000 - 50ms/epoch - 4ms/step\nEpoch 429/500\n14/14 - 0s - loss: 0.0108 - accuracy: 1.0000 - val_loss: 0.0118 - val_accuracy: 1.0000 - 46ms/epoch - 3ms/step\nEpoch 430/500\n14/14 - 0s - loss: 0.0108 - accuracy: 1.0000 - val_loss: 0.0117 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 431/500\n14/14 - 0s - loss: 0.0107 - accuracy: 1.0000 - val_loss: 0.0115 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 432/500\n14/14 - 0s - loss: 0.0107 - accuracy: 1.0000 - val_loss: 0.0114 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 433/500\n14/14 - 0s - loss: 0.0107 - accuracy: 1.0000 - val_loss: 0.0115 - val_accuracy: 1.0000 - 46ms/epoch - 3ms/step\nEpoch 434/500\n14/14 - 0s - loss: 0.0106 - accuracy: 1.0000 - val_loss: 0.0113 - val_accuracy: 1.0000 - 62ms/epoch - 4ms/step\nEpoch 435/500\n14/14 - 0s - loss: 0.0106 - accuracy: 1.0000 - val_loss: 0.0113 - val_accuracy: 1.0000 - 54ms/epoch - 4ms/step\nEpoch 436/500\n14/14 - 0s - loss: 0.0106 - accuracy: 1.0000 - val_loss: 0.0114 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\nEpoch 437/500\n14/14 - 0s - loss: 0.0105 - accuracy: 1.0000 - val_loss: 0.0114 - val_accuracy: 1.0000 - 47ms/epoch - 3ms/step\n\n\nOur model appears to have been capable of perfect accuracy on the both the training and validation sets. This is great!\nPlease, however, note that it is very uncommon to get such an accuracy in the real world, and this does not mean that the model will perform perfectly in predicting the species of new data points. All this means is that our model has converged to an optimal solution for the data it was provided with. Which is still a great feat!\nTo make sure nothing funky went on during training, lets visualize our training and validation losses. For this we can use the seaborn library along with matplot lib to produce some aesthetically pleasing plots. We will use the history from when we fit the model to pull our losses from, and plot the values on a lineplot.\n\nplt.figure(figsize=(8,6))\nsns.set_style(style=\"darkgrid\")\nsns.set(font_scale = 1.5)\nsns.lineplot(x = range(1, len(history.history['loss']) + 1),\n             y = history.history['loss'],\n             label = 'Training Loss',\n             linewidth = 2)\nsns.lineplot(x = range(1, len(history.history['val_loss']) + 1),\n             y = history.history['val_loss'],\n             label = 'Validation Loss',\n             linewidth = 2,\n             linestyle = '--')\nplt.title('Training and Validation Loss Over Epochs')\nplt.xlabel('Epoch')\nplt.ylabel('Loss (categorical crossentropy)')\nplt.show()\n\n\n\n\nThe training curve is smooth. This is a good sign. A smooth curve indicates a stable learning process. As we train on further epochs, our loss falls gradually. Also, it is important to note that both the training loss and validation loss are almost equal. Remember that our model was never trained with the validation set. Thus, our model performed just as well on the unseen validation set as it did on the training set. With almost no difference in these two losses, we can be sure that we did not overfit the training set and have built a generalizable model.\n\n\nEvaluations and Metrics\nUsing our newly trained model, we can produce some evaluation metrics and visualizations to interpret the performance of our model. Our first step is to use our trained model to predict on the validation set (which it has not seen yet). This gives us a 2D-array of 3 columns and some class probabilities. Something like: [P1, P2, P3]. From that, we can use np.argmax to return the index of the maximum value in that array. This gets us from probabilities to a label of the class with the highest probability.\nWith classification problems, a popular way to evaluate performance is with a confusion matrix. This identifies any difference between the actual labels and the labels that our model predicted using a matrix. To make the confusion matrix, we just give our predictions and the true values to sklearn’s confusion_matrix() function. Then we make a dataframe and add our species labels for seaborn to plot later.\nAlong with the matrix, we can print the precision, recall, and f1 scores of our model. In binary classification, precision is the ratio of correctly predicted positive samples to the total predicted positive samples. Recall, on the other hand, is the ratio of correctly predicted positive samples to the total actual positive samples. In general, high precision means that when a model predicts a positive class, it is likely correct. High recall would mean that the model is good at identifying most positive class samples. F1 score simply combines these two ideas into one single metric. Unfortunately, since we are doing a multiclass classification, we must do some extra work (not really). We simply must choose how we want to average our precision and recall scores for each class. For our case, we will use macro-averaging. A nice graphic to explain this process is below:\n Here, N is the number of total classes. With this averaging technique, we can report the average precision, recall, and f1 scores for predictions on our validation set.\nThen, with seaborn, we can use a heatmap to visualize our confusion matrix and add some titles and labels to pretty it up.\n\n# Predict\ny_pred = np.argmax(slp_clf.predict(X_val, verbose=0), axis=1)\n\n# Make Confusion Matrix\ncm = sk.metrics.confusion_matrix(np.argmax(y_val, axis=1), y_pred)\ncm_df = pd.DataFrame(cm,\n                     index = data['species'].unique(),\n                     columns = data['species'].unique())\n\n# Print evaluation metrics\nprint(f'Precision: {sk.metrics.precision_score(np.argmax(y_val, axis=1), y_pred, average=\"macro\"):.3f}')\nprint(f'Recall: {sk.metrics.recall_score(np.argmax(y_val, axis=1), y_pred, average=\"macro\"):.3f}')\nprint(f'F1: {sk.metrics.f1_score(np.argmax(y_val, axis=1), y_pred, average=\"macro\"):.3f}')\n\n# Display Confusion Matrix\nplt.figure(figsize=(8,6), dpi=300)\nsns.set(font_scale = 1.1)\nax = sns.heatmap(cm_df, annot=True, fmt='d', linewidths=2, linecolor='#d3d3d3', cmap='Blues')\nax.set_title('SLP Classifier Multilabel Confusion Matrix')\nax.set_xlabel(\"Predicted Label\", fontsize=14, labelpad=20)\nax.set_ylabel(\"Actual Label\", fontsize=14, labelpad=20)\nplt.show()\n\nPrecision: 1.000\nRecall: 1.000\nF1: 1.000\n\n\n\n\n\nSince our model achieved perfect accuracy, we see that all of the actual and predicted labels match up exactly. We did not wrongly predict a single sample! This is also why all of our evaluation metrics are 1. Out of the 49 Adelie samples, we predicted all of them correctly. The same goes for Chinstrap and Gentoo penguins too.\n\n\nConclusion\nTo wrap up, throughout this post we have worked through an entire machine learning classification problem from start to finish. From data to predictions, we dove into some fundamental topics such as handling missing data values, one-hot encoding, as well as building and training a simple neural network model to just name a few.\nBy leveraging state-of-the-art python libraries like NumPy, Keras, and Scikit-Learn, we gained hands-on experience in implementing and training a neural network model on a real dataset. With neural networks, we even covered some crucial concepts like activation and loss functions. While our single-layer perceptron model may have been simple, it serves as a fundamental building block in neural networks and lays the groundwork for understanding more complex architectures down the line. With this in mind, we can move onto bigger and more complex problems and model types in the future. If you’ve gotten this far, I encourage anyone reading this to remember that this is just the starting point on your journey through this hell-scape (it really isn’t that bad when you get into it though) of machine learning in the modern era. After this, the doors are open. For better or for worse.\nSo, to conclude, Good luck out there, soldier!"
  },
  {
    "objectID": "projects/governmentSurveillance/index.html#predictive-policing",
    "href": "projects/governmentSurveillance/index.html#predictive-policing",
    "title": "Ethics of Intelligent Government Surveillance Systems",
    "section": "Predictive Policing",
    "text": "Predictive Policing\nThe rise of big data systems and machine learning has led to an increase in the use of predictive policing techniques [10]. Despite its growing popularity, predictive policing has yielded mixed results due to several challenges, such as the loss of context in predictive analysis, the lack of transparency in the models used, and ethical concerns regarding privacy and data collection [14].\nTwo well-known predictive policing programs, LASER and PredPol, serve as examples. While LASER aimed to identify areas with a high likelihood of gun violence [6], PredPol focused on calculating hotspots for property-related crimes. LASER was shut down in 2019 due to significant consistency issues discovered through an internal audit. A common issue with these programs is the lack of transparency; many police departments do not disclose the information or technology used to generate their predictive policing systems.\nCritics argue that predictive policing could threaten rights protected by the Fourth Amendment, which requires “reasonable suspicion” for law enforcement actions [5]. However, proponents contend that predictive tools make it easier for police to identify reasonably suspicious individuals [8]. Another concern is the phenomenon of “tech-washing,” where racially biased policing methods are given the appearance of objectivity due to the use of computer algorithms. This is especially problematic when algorithms trained on historical data perpetuate existing biases. The ultimate goal of predictive policing is to transition law enforcement from a reactive to a proactive stance, emphasizing crime prevention rather than merely responding to incidents [7]."
  },
  {
    "objectID": "projects/governmentSurveillance/index.html#social-credit-systems",
    "href": "projects/governmentSurveillance/index.html#social-credit-systems",
    "title": "Ethics of Intelligent Government Surveillance Systems",
    "section": "Social Credit Systems",
    "text": "Social Credit Systems\nChina’s Social Credit System (SCS) is a prime example of the ethical challenges associated with Social Credit Systems. The SCS monitors the behavior of all citizens and enterprises, assigning social credit scores based on their actions. Individuals who violate laws are placed on an online “blacklist,” while trustworthy individuals are placed on a “red list.” Both lists are publicly accessible, and various restrictions can be imposed based on social credit scores, such as financial or legal penalties for failing to pay taxes or attend court summons [3].\nThe SCS has a psychological basis; by displaying rule breakers online, it holds them accountable, while those on the red list serve as models for others. However, the system raises concerns about the potential erosion of individual free will, which could lead to citizens feeling constrained or controlled. This, in turn, may result in growing unease and resentment, potentially culminating in civil unrest or even revolution."
  },
  {
    "objectID": "projects/governmentSurveillance/index.html#pasco-country-sheriff-department-florida",
    "href": "projects/governmentSurveillance/index.html#pasco-country-sheriff-department-florida",
    "title": "Ethics of Intelligent Government Surveillance Systems",
    "section": "Pasco Country Sheriff Department (Florida)",
    "text": "Pasco Country Sheriff Department (Florida)\n\nDescription\nThe Pasco County Sheriff Department instituted The Intelligence-Led Policing Section (ILP) on June 14, 2011. Initially established as a group to “advance the agency’s crime fighting initiatives to a modern-day philosophy”, after much controversy the program was discontinued sometime between 2021 and 2022 [13].\nThis program implemented a predictive policing system which attempted to catch potential criminal offenders before they would, supposedly, commit a crime. Under this program, the potential offenders were denoted as prolific offenders. The Sheriff Department maintained a list of said prolific offenders that they deemed were likely to commit crimes in the future. According to a “crude computer algorithm”, members of the community would be placed on this list based on factors including a person’s criminal record, whether they have been suspected of a crime, whether they have witnessed a crime, or even whether they were a victim of a crime [9].\nIn addition to the aforementioned prolific offenders list, an investigation by the Tampa Bay Times has also revealed the use and maintenance of an at-risk students list by the Pasco Country Sheriff Department [9]. According to the agency’s internal intelligence manual, factors such as grade point average, attendance, involvement in a custody dispute, or even, again, being a victim of a personal crime, all played a role in students being placed on said list. The school districts provide much of this sensitive and private data to the Sheriff Department with full access, raising legal concerns of data privacy. The Pasco Sheriff Office was no longer allowed to access student data post May 3, 2021.\n\n\nCommunity Impact\nOnce placed onto the prolific offenders list, people were subject to intimidation and harassment by officers of the police department instructed to monitor and gather information on their subjects. Officers would show up unannounced to homes to even interrogate friends and families of listed peoples, unnecessarily harassing innocent civilians in the process. When met with uncooperative subjects, officers would also cite them egregious amounts, typically in the thousands, for frivolous property code violations such as having tall grass or missing house numbers [15]. A former Pasco County deputy described that the department had ordered them to “make their lives miserable until they move or sue” [15]. Community members are being forced out of their homes by constant fear of harassment from the police department. This is a conscious action by the department, suspected to be in hopes of lowering crime rate in the area. And as seen above, not only are the listed peoples subject to this, but innocent relatives and friends too.\nRegarding the at-risk students list, in the summer following the initial implementation of this list, school resource officers were reported to make “hundreds of home visits to at-risk kids”, offering support to children and families, but also questioning them of local crimes and even making arrests of kids who had violated probation or curfew orders [2]. The intelligence manual also reportedly encouraged officers to work relationships with to students to discover “seeds of criminal activity” and collect information accordingly. Unfortunately, since the students are unable to know whether or not they have been flagged as at-risk it is difficult to fully evaluate the effects of such a system on the student population. Though, if this list is used at all similarly to the prolific offenders list, we can expect that listed students are likely suffering from similar harassment by school resource and correctional officers alike.\n\n\nEthical Implications\nAn ethical issue paramount to such a predictive policing system as described is discrimination. With a predictive policing system based on data from an already discriminatory criminal justice system, the usage of such a system only works to perpetuate the same injustices. The criminal justice system in the United States disproportionately targets those of minority ethnic groups and low income. So, the predictive policing system of Pasco County follows and targets the same marginalized groups of people. Additionally, a further ethical concern arises when even those with no criminal history, but have simply been suspected of a crime, are also subject to unwarranted harassment. By definition, this is discrimination towards those suspected of a crime, even though they may have been found innocent. With regards to the at-risk students list, discrimination towards those involved in custody disputes is also a concern, as it could be seen as discrimination based on family status [2]. Policing should be ethical and lack any forms of discrimination, and these predictive systems undermine those essential characteristics.\nIn addition to discrimination, predictive policing in the United States raises concerns of violating constitutional rights to protection against unreasonable searches and seizures, protected under the Fourth Amendment. Specifically, when Pasco County officers show up unwarranted to homes with the goals of collecting information, homeowners have the right to refuse and ask the officers to leave. When the officers continue to poke and prod them without a warrant or any reasonable suspicion, they violate the homeowner’s Fourth Amendment rights [15]. Also, when harassing family members and friends of those on the prolific offenders list, officers violate the constitutional right to freedom of association, protected by the First Amendment, as well as the right to not be punished for another’s actions, protected under the Due Process Clause of the Fourteenth Amendment [15]. As citizens of the United States, we can reasonable expect government officials to act in accordance with our constitutional rights. However, the predictive policing tactics of Pasco County do exactly the opposite, making it entirely unethical.\n\n\nUniqueness\nThe Pasco County case in predictive policing is unique in its revealed integration of school performance data and abuse history in its policing efforts towards children. According to the Pasco Sheriff’s Office Intelligence Manual, children which have recorded to have witnessed or experienced household violence of even gotten a D or an F in school are factors which make a child much more likely to become a criminal. This was an effort to identify future criminals in the youth population of a school system [2]. This supposedly led to numerous interventions with at-risk students in the summer following the implementation of the at-risk students list. School resource officers have also been reportedly praised for contributing to intelligence briefings and filing field interview reports on interactions with at-risk kids [2]. This form of investigation into children for their likelihood to become criminals is unprecedented, as far as the public knows.\nAlthough identifying students at risk of dropout or failure in school has seen much implementation throughout history, the usage of school data in policing has only recently been brought to light. Because of the intention to keep these programs secret from the public, the only case of such a system brought into the public eye was that of Pasco County, detailed above. There are likely multiple other implementations of a similar system which have not yet been revealed, but to the public’s knowledge, this is the only pertinent case.\n\n\nBroader View\nGenerally, the public has conveyed great disdain towards Pasco County’s implementation and usage of a predictive policing system. Affected members of the community have worked closely with investigative journalists and law corporations in order to publicize their grievances and fight for their constitutional rights. This case has realized the exact fears which the public has already had on algorithm-assisted policing efforts. And the vast number of new outlets which have picked up reporting on the case supports this.\nAdditionally, this case, specifically, has led to the involvement of the federal government in investigating the suspected injustices of such a system, with the Department of Education uptaking an investigation into the usage of school data by the Pasco County Sheriff Office. This conveys a necessity for change as a result of public outrage. Citizens do not want their constitutional rights to be breached, so they are rightfully upset when a case like this arises."
  },
  {
    "objectID": "projects/governmentSurveillance/index.html#chinese-social-credit-system",
    "href": "projects/governmentSurveillance/index.html#chinese-social-credit-system",
    "title": "Ethics of Intelligent Government Surveillance Systems",
    "section": "Chinese Social Credit System",
    "text": "Chinese Social Credit System\n\nDescription\nThe Chinese Social Credit System is a system put in place and developed by the Chinese government that allows for the monitoring and the rating of it’s citizens. This rating system uses artificial intelligence, big data analytics, personal reportings, and surveillance technologies to get information about individuals to rate their level of trustworthiness as citizens [3]. The information they use includes financial information, online activities, social behavior, and even their physical movements.This score affects every area of the individual’s life, from travel, to loans, to job opportunities, and even to their right to express themselves and their opinions online [3]. This system has been in development for almost a decade now but is expected to be fully implemented and operational by 2025.\n\n\nCommunity Impact\nThe Social Credit System’s (SCS) implementation has had a profound impact on Chinese society, both positive and negative. On the one hand, the implementation of the SCS shoots for complete accountability and because of this it has helped to reduce crime rates, promote financial transparency, and even increase social cohesion by rewarding good behavior and punishing bad behavior. An example of this is that “good” citizens, those with a high score which indicates a high level of trustworthiness, get to enjoy discounted services and priority access to public resources while “bad” citizens face travel restrictions, high insurance premiums, and even social stigma. On the other hand, the system is criticized, understandably, for being intrusive, discriminatory, and (maybe most importantly) prone to abuse. There are also fears that this system gives the government the ability to silence dissent, punish minorities, and restrict personal freedoms. Another negative aspect of the SCS is that it creates a culture of fear and mistrust where people are constantly watching their behavior and that of others in order to avoid negative consequences.\n\n\nEthical Implications\nThe Social Credit System raises a number of ethical questions related to privacy, fairness, and accountability. Regarding privacy, there are concerns since the system collects and analyzes a very large amount of personal data without clear consent or oversight [3]. This opens the opportunity to profile and discriminate against individuals based on their beliefs, ethnicity, or even social status. These issues raise concerns about the people’s right to privacy, autonomy, and due process because the citizens graded by this system may not have access to the information used to calculate their score, or the ability to contest it’s accuracy. From the fairness perspective, the system, as all models/algorithms are, is prone to reinforcing existing biases and inequalities. This would lead to giving advantages to those who are already privileged while punishing those who are marginalized or vulnerable. This raises questions about social justice, equality, and even human dignity since citizens can be judged based on factors they cannot control such as their family background, medical history, or political affiliations. From an accountability perspective, the system completely lacks transparency and oversight. The lack of independent oversight makes the system prone to corruption, abuse, and errors.\n\n\nUniqueness\nWhile there are semi-similar systems for generating scores to quantify an individual’s trustworthiness in a specific area, such as the credit score system in the United States, the Social Credit System in China is unique in several ways both in terms of technical features and social implications. Technically, it represents a new paradigm of surveillance and control to reshape individual and collective behavior through a combination of digital technologies, behavioral sciences, and social engineering. It relies on a massive network of data sources which are used to track and monitor citizens in real time and in all aspects of their lives [3]. From a social perspective, it attempts to redefine the relationship between the state it’s citizens. It aims to make social trustworthiness a key criteria for participation in society.\n\n\nBroader View\nThere are many different and well accepted ways of quantifying an individual’s trustworthiness in a specific area. From credit score used for your financial trustworthiness to ride sharing companies like Uber using a scoring system to evaluate drivers, however nothing has come close to attempting to score complete trustworthiness as a citizen. The Chinese Social Credit System attempts to do this because the idea of complete accountability, especially with such a large population, would be very beneficial to the state. The issue with implementing any system this vast with such an effect on people’s lives is that there must be independent oversight as well as transparency on how the score is generated. Any less and you have an all powerful system that is incredibly prone to corruption and abuse. In addition, to collect that data you are taking away all of your citizens’ right to privacy as well as creating a culture and atmosphere of fear among your population since every one of their actions is being observed, recorded, and scored."
  },
  {
    "objectID": "projects/OT-graphTheoretical/index.html",
    "href": "projects/OT-graphTheoretical/index.html",
    "title": "Analysis of A Graph Theoretic Additive Approximation of Optimal Transport",
    "section": "",
    "text": "In this post, I describe and analyze a graph theoretic additive approximation algorithm for approximating Optimal Transport. Nathaniel Lahn, Deepika Mulchandani, and Sharath Raghvendra (my former theory and algorithms professor) submitted this algorithm to the arXiv repository originally on May 28, 2019.\nPlease see the source below for the original paper on this algorithm.\n[A Graph Theoretic Additive Approximation of Optimal Transport]"
  },
  {
    "objectID": "projects/OT-graphTheoretical/index.html#description",
    "href": "projects/OT-graphTheoretical/index.html#description",
    "title": "Analysis of A Graph Theoretic Additive Approximation of Optimal Transport",
    "section": "Description",
    "text": "Description\nThis algorithm applies the traditional framework of augmenting paths for computing an approximate solution to the minimum-cost maximum flow problem to a transport problem. However, in order to this, the problem must only contain integer supplies and demands. So, in order to utilize this algorithm, the input problem must first be transformed to contain only integer demands and supplies. The process of transforming the given problem’s demand and supply values follows:\n\nTransforming Demand and Supply Values\nInitially, let \\(\\varepsilon\\) be some constant value such that \\(0 &lt; \\varepsilon &lt; 1\\), and set \\(\\alpha = \\frac{2nC}{\\varepsilon U \\delta}\\). Here, the variables are defined as follows:\n\n\\(n\\) is the total number of nodes.\n\\(C\\) is the largest cost of any edge in the graph.\n\\(U\\) is the total supply.\n\\(\\delta\\) is the additive error.\n\nNow, let \\(\\mathcal{I}\\) be the input for the transportation problem such that each demand node \\(a \\in A\\) has a demand of \\(d_a\\) and each supply node \\(b \\in B\\) has a supply of \\(s_b\\). We will denote the integer-scaled demand for some node \\(a \\in A\\) as \\(\\overline{d}_a\\). Similarly, the integer-scaled supply for some node \\(b \\in B\\) is denoted as \\(\\overline{s}_b\\). The process to scale the demand and supply values at each node is detailed next. For all demand nodes \\(a \\in A\\), \\(\\overline{d}_a = \\lceil d_a\\alpha\\rceil\\), and for all supply nodes \\(b \\in B\\), \\(\\overline{s}_b = \\lfloor s_b\\alpha \\rfloor\\). Let \\(\\mathcal{I'}\\) denote this scaled input. Next, the solution to the scaled transport problem must be mapped to a feasible solution for the original demand and supply values. Let \\(\\sigma\\) denote any feasible maximum transport plan for the input \\(\\mathcal{I'}\\). A transport plan \\(\\sigma\\) which sets, for each edge \\((a,b),\\sigma(a,b) = \\sigma'(a,b)/\\alpha\\), is not necessarily a feasible or a maximum solution. \\(\\sigma\\) may not be a maximum solution, as there may be an excess supply remaining at a supply node. Similarly, \\(\\sigma\\) is not feasible as there may be excess supply that reaches a demand node. In order to convert \\(\\sigma\\) to a feasible and maximum solution, there are two steps:\n\nFirst, we must convert \\(\\sigma\\) to a feasible solution. To do this, we can iteratively remove excess supply at demand nodes. For instance, let \\(\\mathcal{k}_a\\) denote the excess supply that reaches a demand node \\(a \\in A\\). Now, iteratively select an arbitrary edge incident on \\(a\\). Let \\((a,b)\\) denote this edge. Now, reduce \\(\\sigma(a,b)\\) and \\(\\mathcal{k}_a\\) by \\(min{\\mathcal{k}_a, \\sigma(a,b)}\\). Continue this until \\(\\mathcal{k}_a\\) is reduced to \\(0\\). We can then repeat these steps for all demand nodes \\(a \\in A\\) such that \\(\\mathcal{k}_a &gt; 0\\). After this, \\(\\sigma\\) will have been converted to a feasible solution. At this point the total remaining supply in \\(\\sigma\\) is at most \\(2n/\\alpha\\).\nSecond, we must convert the newly feasible solution, \\(\\sigma\\), to a maximum transport plan. To do this, we just match the \\(2n/\\alpha\\) supplies arbitrarily to any leftover demands. This will incur a cost of at most \\(C\\) per supply unit.\n\nFinally, with a properly scaled input, we can now apply the algorithm.\n\n\nAlgorithm for Scaled Demands and Supplies:\nFirst, initialize \\(\\sigma\\) as a transport plan, such that, for every edge \\((a,b) \\in A \\times B\\), \\(\\sigma(a,b) = 0\\). Now, let the dual weight of some vertex \\(v\\) be denoted as \\(y(v)\\). Set \\(y(v) = 0\\) for all vertices \\(v \\in A \\cup B\\). This algorithm executes in phases, wherein each phase is two steps. Execution will terminate once \\(\\sigma\\) becomes a maximum transport plan. The two steps are detailed below:\n\nHungarian Search:\nIn the first step, we will conduct a Hungarian Search in order to compute at least one augmenting path of admissible edges. Firstly, we will add two arbitrary nodes, \\(s\\) and \\(t\\), to the residual network. Here, \\(s\\) represents a source node and \\(t\\) represents a sink node for the single-source, single-sink maximum flow problem. We attach edges from \\(s\\) to every free supply node and edges from every free demand node to \\(t\\). All edges incident on \\(s\\) or \\(t\\) are given a weight of \\(0\\). The weight of all other edges in the residual network are set the the slack of the respective edge based on the edge’s direction. Let \\(\\mathcal{G}_\\sigma\\) denote the augmented residual network with the attached source and sink vertices. Now, execute Dijkstra’s algorithm from \\(s\\) in the augmented residual network \\(\\mathcal{G}_\\sigma\\). Let \\(\\ell_v\\) be the shortest path in \\(\\mathcal{G}_\\sigma\\) from \\(s\\) to \\(v\\) for any node \\(v \\in A \\cup B\\). Now, we must update the dual weights in the network to maintain feasibility. For any vertex \\(v \\in A \\cup B\\), we will do one of the following:\n\nIf \\(\\ell_v \\geq \\ell_t\\), the dual weight is not updated.\nOtherwise, the dual weight is updated. Now, if \\(v \\in A\\), \\(y(v) \\leftarrow y(v) - \\ell_t + \\ell_v\\). Else, if \\(v \\in B\\), \\(y(v) \\leftarrow y(v) + \\ell_t - \\ell_v\\).\n\nOnce this step reaches completion, \\(\\sigma\\) remains feasible and the admissible graph contains at least one augmenting path.\nPartial DFS:\nIn the second step, we will compute at least one augmenting path and update \\(\\sigma\\) by augmenting it along every computed path. First, let \\(\\mathcal{A}\\) denote the admissible graph. Now, let \\(X\\) denote the set of free supply nodes in the admissible graph \\(\\mathcal{A}\\). A DFS will be iteratively run from every supply node in set \\(X\\). Let \\(b \\in X\\) be the supply node for the current iteration. The steps of the partial DFS from node \\(b\\) are as follows:\n\nBegin a DFS from node \\(b\\).\nDuring the execution, if a free demand node is visited, then an augmenting path is found, and the DFS is terminated. Let \\(P\\) denote this path. Once the DFS is terminated, all edges visited by the DFS, except for the edges of path \\(P\\), are removed. Then, we will augment \\(\\sigma\\) along path \\(P\\) and updates set \\(X\\) to contain only the set of free supply nodes remaining in admissible graph \\(\\mathcal{A}\\).\nIf no augmenting path is found, all vertices and edges visited by the DFS are removed from admissible graph \\(\\mathcal{A}\\), and set \\(X\\) is updated to contain only the set of free supply nodes remaining in \\(\\mathcal{A}\\).\n\nOnce this step reaches completion, set \\(X\\) will be empty.\n\nFinally, at the end of execution, \\(\\sigma\\) is a maximum transport plan for the given transport problem."
  },
  {
    "objectID": "projects/OT-graphTheoretical/index.html#correctness",
    "href": "projects/OT-graphTheoretical/index.html#correctness",
    "title": "Analysis of A Graph Theoretic Additive Approximation of Optimal Transport",
    "section": "Correctness",
    "text": "Correctness\nTo begin assessment of this algorithm’s correctness, we must first prove the following invariant:\nIn each phase of the algorithm, the partial DFS step computes at least one augmenting path. Once the partial DFS step terminates, there is no augmenting path in the admissible graph.\nThe proof is as follows:\nConsider the shortest path from source node \\(s\\) to sink node \\(t\\) in the augmented residual network. Let \\(P'\\) denote this path. Now, let \\(b\\) be the free supply node after \\(s\\) and let \\(a\\) be the free demand node before \\(t\\) along path \\(P'\\). Let \\(P\\) denote the path from node \\(b\\) to node \\(a\\).\nFirst, we must show that path \\(P\\) is an admissible augmenting path after the dual updates conducted by the Hungarian search. Note that, by construction, for any edge \\((u,v)\\) in \\(P\\), \\(\\ell_u \\leq \\ell_t\\) and \\(\\ell_v \\leq \\ell_t\\).\nLet \\(\\tilde{y}(.)\\) denote the updated dual weight for some vertex. Now, the updated dual weights for vertices \\(u\\) and \\(v\\) become:\n\n\\(\\tilde{y}(u) = y(u) + \\ell_t - \\ell_u\\) and \\(\\tilde{y}(v) = y(v) - \\ell_t + \\ell_u\\) for a forward edge, or\n\\(\\tilde{y}(u) = y(u) - \\ell_t + \\ell_u\\) and \\(\\tilde{y}(v) = y(v) + \\ell_t - \\ell_u\\) for a backward edge.\n\nNow, the updated feasibility condition becomes:\n\n\\(\\tilde{y}(u) + \\tilde{y}(v) = y(u) + y(v) + \\ell_v - \\ell_u\\) for a forward edge, or\n\\(\\tilde{y}(u) + \\tilde{y}(v) = y(u) + y(v) - \\ell_v + \\ell_u\\) for a backward edge.\n\nNote that all edges in the shortest path \\(P\\) satisfy the condition that for any directed edge \\((u,v)\\), \\(\\ell_u + s(u,v) \\geq \\ell_v\\) where \\(s(u,v)\\) is the slack of the respective edge. Because this condition holds with equality,\n\n\\(\\tilde{y}(u) + \\tilde{y}(v) = y(v) + y(v) + s(u,v) = \\overline{c}(u,v) + 1\\) if \\((u,v)\\) is a forward edge, or\n\\(\\tilde{y}(u) + \\tilde{y}(v) = y(v) + y(v) - s(u,v) = \\overline{c}(u,v)\\) if \\((u,v)\\) is a backward edge.\n\nIn other words, this means that path \\(P\\) is an admissible augmenting path.\nFinally, since the partial DFS initiates from every free supply vertex, including node \\(b\\), we will discover at least one augmenting path in the admissible graph.\nAfter this, we must next show that once the partial DFS step terminates, there is no augmenting path in the admissible graph. During this step’s execution, graph \\(\\mathcal{A}\\), initialized to the admissible graph, is maintained. For instance, after each execution of DFS, the edges visited by DFS are removed from graph \\(\\mathcal{A}\\) unless they are on the augmenting path \\(P\\). If no augmenting path is found, all edges visited by the DFS are removed from graph \\(\\mathcal{A}\\). The termination condition of this step ensures that graph \\(\\mathcal{A}\\) does not have any free supply vertices remaining at completion.\nHere, it is important to note that, every vertex removed from graph \\(\\mathcal{A}\\) is a vertex in which the DFS backtracked. Since it can be shown that there is no directed cycle consisting of admissible cycles, there is no path of admissible edges from any vertex which had been removed from graph \\(\\mathcal{A}\\) to a free demand node.\nNow, as the partial DFS step ensures that all free supply vertices are deleted from graph \\(\\mathcal{A}\\) on completion, there cannot be admissible paths from any free supply vertex to a free demand vertex in the admissible graph. Now that we have proved this invariant, we can proceed to proving the correctness.\nAs the invariant above declares, in each phase, the algorithm will augment the transport plan by at least one unit of supply. So, it follows that, on termination of the algorithm, we will have computed a feasible maximum transport plan.\nFirstly, we must show that all transport plans maintained by the algorithm will satisfy the following condition:\nConsidering a feasible maximum transport plan such that for every demand node \\(a \\in A\\), the dual weight \\(y(a)\\leq0\\), and \\(y(a) = 0\\) if node \\(a\\) is free.\nDenote this condition as (C).\nInitially, for any node \\(v\\) such that \\(v \\in A\\), dual weights are set to \\(0\\). Within any phase, now suppose that \\(\\ell_v &lt; \\ell_t\\). Because of this, the dual weight of \\(v\\) will be reduced when the Hungarian Search step updates dual weights.\nSo, since \\(y(v)\\) was initially set to \\(0\\), after the update, \\(y(v)\\leq0\\).\nNext up, we must show that all free vertices of \\(A\\) have a dual weight of \\(0\\), as specified by the aforementioned condition. As dual weights are initialized to \\(0\\), this claim is true initially. During execution of the algorithm, any vertex \\(a \\in A\\) whose demand has been met cannot become free through the remaining execution of the algorithm. So, we can argue that no free demand vertex will have its dual weight updated.\nNow, note that, by construction, any directed edges to the sink node \\(t\\) from a demand node in \\(A\\) have zero cost in \\(\\mathcal{G}_\\sigma\\). Thus, it follows that there is a directed edge \\((v,t)\\) with a cost of zero. So, \\(\\ell_t \\leq \\ell_v\\), resulting in the dual weight \\(y(v)\\) not being updated during the phase. Therefore, the algorithm maintains \\(y(v) = 0\\) for every free demand node, and it is proved that the condition (C) is upheld. And so, finally, when the algorithm completes, we will have computed a feasible maximum transport plan which satisfies condition (C)."
  },
  {
    "objectID": "projects/OT-graphTheoretical/index.html#efficiency",
    "href": "projects/OT-graphTheoretical/index.html#efficiency",
    "title": "Analysis of A Graph Theoretic Additive Approximation of Optimal Transport",
    "section": "Efficiency",
    "text": "Efficiency\nBefore beginning an assessment of this algorithm’s efficiency, we must provide some lemmas. Also, at any stage in the algorithm, let \\(B'\\) denote the set of free supply nodes and let \\(A_F\\) denote the set of free demand nodes. The lemmas follow:\nLemma 2.2: The dual weight of any free supply node \\(v \\in B'\\) is at most \\(\\lfloor 2C/\\delta' \\rfloor + 1\\)\n\nProof. Suppose, for the sake of contradiction, that the free supply node \\(b \\in B\\) has a dual weight \\(y(b) \\geq \\lfloor 2C/\\delta' \\rfloor + 2\\). Now, consider the condition (C). Due to this condition, any free demand node \\(a \\in A_f\\) has a dual weight \\(y(a)=0\\). So, \\(y(a) + y(b) \\geq \\lfloor 2C/\\delta' \\rfloor + 2 \\geq \\overline{c}(a,b) + 2\\). However, this violates feasibility, as \\(y(a) + y(b) \\leq \\overline{c}(a,b) + 1\\) is not upheld. Thus, a contradiction arises. ◻\n\nLemma 2.3: The total number of phases in our algorithm is at most \\(\\lfloor 2C/\\delta' \\rfloor + 1\\).\nNote: Here, \\(C\\) is the largest value in the cost matrix, \\(\\delta\\) is the additive error, and \\(\\delta' = (1-\\varepsilon)\\delta\\) where \\(\\varepsilon\\) is some constant such that \\(0 &lt; \\varepsilon &lt; 1\\).\n\nProof. From the invariant proved on the previous page, we can conclude that at the start of a phase, there are no admissible augmenting paths. Because of this, any path from source node \\(s\\) to sink node \\(t\\) in the augmented residual network \\(\\mathcal{G}_\\sigma\\) will have a cost of at least 1. In other words \\(\\ell_t \\geq 1\\). Now, during any phase, let \\(b \\in B'\\) be any free supply node. It is important to note that node \\(b\\) is also a free supply node in all previous phases. By construction, we may conclude that there exists a directed edge from \\(s\\) to \\(b\\) with a cost of \\(0\\) in \\(\\mathcal{A}\\). Therefore, \\(\\ell_b = 0\\). Finally, since we now know that \\(\\ell_t \\geq 1\\), when updating the dual weight of node \\(b\\) for this phase, the dual weight will be increased by at least \\(1\\). After \\(\\lfloor 2C/\\delta' \\rfloor + 2\\) phases, due to the aforementioned increase of at least \\(1\\), \\(y(b) \\geq \\lfloor 2C/\\delta' \\rfloor + 2\\), which contradicts Lemma 2.2 because \\(\\lfloor 2C/\\delta' \\rfloor + 2 &gt; \\lfloor 2C/\\delta' \\rfloor + 1\\). ◻\n\nLemma 2.4: Let \\(\\mathbb{P}\\) be the set of all augmenting paths produced by the algorithm. Then, \\(\\sum_{P\\in\\mathbb{P}} |P| = \\mathcal{O}(\\frac{nC^2}{\\varepsilon(1-\\varepsilon)\\delta^2})\\).\nNote: Here, \\(C\\) is the largest value in the cost matrix, and \\(\\delta\\) is the additive error. Additionally, \\(|P|\\) is the number of edges on path \\(P\\), and \\(\\varepsilon\\) is some constant such that \\(0 &lt; \\varepsilon &lt; 1\\).\nNow that the lemmas are out of the way, let us begin the assessment of this algorithm’s efficiency. First, Let \\(\\mathbb{P}_j\\) denote the set of all augmenting paths computed in phase \\(j\\) of execution. Also, let \\(\\mathbb{P}\\) denote the set of all augmenting paths produced by the algorithm across all phases.\nConsider Lemma 2.3. As described in the lemma, the total number of phases executed by the algorithm is bounded above by \\(\\lfloor 2C/\\delta' \\rfloor + 1\\). So, the total number of phases is \\(\\mathcal{O}(C/\\delta')\\). Now, each phase consists of two steps: the Hungarian Search step and the partial DFS step.\nIn the Hungarian Search step, a single Dijkstra’s search is executed, taking \\(\\mathcal{O}(n^2)\\) time. The dual weight updates are insignificant to the complexity of this step.\nRegarding the partial DFS step, note that during execution, any edge visited by the DFS is removed as long as it does not contribute to an augmenting path. Inversely, edges which do contribute to an augmenting path may be visited multiple times within the same phase. So, the time taken by the partial DFS step during phase \\(j\\) becomes \\(\\mathcal{O}(n^2 + \\sum_{P \\in \\mathbb{P}_j} |P|)\\), where \\(|P|\\) is the number of edges along the augmenting path \\(P\\). Thus, since we know that the number of total phases is \\(\\mathcal{O}(C/\\delta')\\), the total time taken by the algorithm across all phases is \\(\\mathcal{O}((C/\\delta')n^2 + \\sum_{P \\in \\mathbb{P}} |P|)\\). Considering Lemma 2.4, wherein \\(\\sum_{P \\in \\mathbb{P}} |P| = \\mathcal{O}(\\frac{nC^2}{\\varepsilon(1-\\varepsilon)\\delta^2})\\), the total execution time of the algorithm is \\(\\mathcal{O}(\\frac{n^2C}{(1-\\varepsilon)\\delta} + \\frac{nC^2}{\\varepsilon(1-\\varepsilon)\\delta^2})\\)."
  },
  {
    "objectID": "projects/OT-survey/index.html",
    "href": "projects/OT-survey/index.html",
    "title": "What is the Optimal Transport Problem?",
    "section": "",
    "text": "What is the Optimal Transport Problem?\n\nFirst, let us begin with a general, or low-level, definition of the optimal transport problem. The most common explanation for this problem is as follows: Assume that you have dug a hole in the ground of \\(X\\) cubic feet. And you now also have a pile of dirt with an exact volume of \\(X\\) cubic feet. Now, consider a naïve method of filling the hole with all of the dirt you have accumulated in the pile. In order to fill this hole, you would have to move every individual dirt particle a certain distance from some location in the pile to some location in the hole. Once all particles have been moved, you will have filled your hole.\nSo, how would we define the optimality of our solution? To understand optimality in this case, we need to have some notion of cost for our solution. For this simple case, we can define cost quite simply. Let the cost of moving a single dirt particle be the distance this particle moves when it is picked up from the dirt pile and placed in the hole. We can quantify this as the euclidean distance from any given particle’s source location to its destination. So now, with our definition of cost, an optimal method of transporting all particles of dirt from the pile to the hole will be the one which minimizes the total cost incurred by transporting all particles. Thus, to have optimal transport, we have to minimize the total distance traveled by all particles in the set when filling the hole.\nThis is the most basic version of the optimal transport problem. It is essentially an extension of finding a minimum cost matching. Consider this problem as matching every source location of a dirt particle to some destination location in the most cost-optimal manner. Once we have found the minimum cost matching between the sets of source and destination locations, we have found the optimal transport plan.\nNow, some may ask, why do we even worry about such a problem? In another post, I go further in-depth on some possible applications of optimal transport theory. But before I do, here is the gist of it. In mathematics, more specifically statistics, computing the distance between probability distributions intuitively has been a large obstacle. Distance in the statistic sense can take very different forms. One of the most popular definitions of a statistical distance is the Kullback-Leibler divergence. Although the most popular, this divergence is not the most desirable. This divergence is not symmetric and may even result in an infinite divergence in some cases. Therefore, the KL divergence is quite undesirable and unintuitive. With optimal transport theory, researchers look to develop a more intuitive and desirable form of statistical distance. That is why this problem is of such importance.\nFinally, let us wrap up by extending the dirt pile and hole example to probability distributions. What would be the optimal transport plan of transforming a probability distribution \\(A\\) to probability distribution \\(B\\)? Conceptualize \\(A\\) as a 2-dimensional pile of dirt, the shape of which resembles the shape of the respective probability distribution. Additionally, conceptualize \\(B\\) as a 2-dimensional hole, the shape of which resembles the shape of the respective probability distribution. Now, we can “transform” distribution \\(A\\) into distribution \\(B\\) by simply placing the “dirt” from \\(A\\) to the hole from \\(B\\). The cost can be defined the same way as in our dirt pile example. When the total cost of transport is at its minimum, we have found the optimal transport plan. Intuitively, we can see that these are similar problems. Note that, if the two distributions are discrete, this problem can be solved in a manner similar to that mentioned above. However, in the case that the two distributions are continuous, the procedure is slightly different. In this case, we would have to compute the optimal transport between samples of the distributions. For \\(n\\) samples, each sample would be assigned a weight of \\(1/n\\), and then the optimal transport between the samples would be computed. When all sample points are equally weighted, the problem is known as the assignment problem.\n\n\nApproximate Optimal Transport Problems\nOptimal transport can be exactly computed, however, computing an exact solution often takes too much time for any practical usages. Because of this, the development of algorithms which compute exact optimal transport has slowed, and in turn we have seen a rise in popularity of approximation algorithms. These approximation algorithms aim to produce a near-optimal transport plan, usually within some predefined error. For some constant \\(\\varepsilon &gt; 0\\), an \\(\\varepsilon\\)-approximate transport plan is one where the total cost of the transport plan is within \\(\\varepsilon n\\) of the optimum. And often, algorithms which compute exact optimal transport are adapted to more-quickly compute an approximate transport plan. With a much faster execution, these approximation algorithms are more fit for practical usages in the real world."
  },
  {
    "objectID": "projects/probabilityTheory-naiveBayes/index.html",
    "href": "projects/probabilityTheory-naiveBayes/index.html",
    "title": "Harnessing Probability Theory: Naive Bayes for Classification",
    "section": "",
    "text": "Probability theory is a branch of mathematics which works to quantify uncertainty and randomness, providing us with a formal framework for expressing and manipulating randomness in various situations. Probability theory as a whole is quite extensive, covering the study of random variables and events as well as the principles governing the likelihood of different outcomes. Applications of probability theory extend from general statistics all the way to machine learning.\nStrap in, because here we will try and navigate the sophisticated interplay between probability theory and modern machine learning algorithms. Although we may only cover a minuscule portion of what probability theory has to offer to us machine learning engineers, I hope that this project can serve to ease you (the reader) into how we can view machine learning problems through the lens of probabilities.\nWith this project, we will discuss some foundational concepts within probability theory and work through an example of applying these concepts to machine learning, using a Naive Bayes Classifier model to predict on the renowned Iris dataset. We will unravel the Bayesian approach to probability theory and delve into the inner workings of the Naive Bayes model, identifying why this combination of fields serves as a powerful framework for traditional machine learning tasks."
  },
  {
    "objectID": "projects/probabilityTheory-naiveBayes/index.html#frequentist-school",
    "href": "projects/probabilityTheory-naiveBayes/index.html#frequentist-school",
    "title": "Harnessing Probability Theory: Naive Bayes for Classification",
    "section": "Frequentist School",
    "text": "Frequentist School\nIn the frequentist interpretation of probability, probability is defined as the limit of the relative frequency of an event occurring in a large number of trials. Here, probability is seen as an objective property of the physical world. Probability is seen to be based on the idea that as experiments are repeated infinitely, the observed relative frequency of an event converges to its true probability.\nFor instance, consider flipping a coin and trying to understand the probability of landing heads with the frequentist approach. According to the frequentist perspective, the probability of getting heads is the limit of the number of heads divided by the total number of flips as we repeat the experiment infinitely. Therefore, if you were to flip a coin a million times, the ratio of heads to total flips would converge to the true probability of landing heads, or at least that’s what frequentists believe."
  },
  {
    "objectID": "projects/probabilityTheory-naiveBayes/index.html#bayesian-school",
    "href": "projects/probabilityTheory-naiveBayes/index.html#bayesian-school",
    "title": "Harnessing Probability Theory: Naive Bayes for Classification",
    "section": "Bayesian School",
    "text": "Bayesian School\nNow, the Bayesian interpretation involves both initial beliefs (known as priors) as well as updates to this belief. The revised prior is known as a posterior. The Bayesian interpretation views probability as a measure of belief or confidence in the occurrence of an event. Then, the probability is updated based on prior knowledge along with every new piece of evidence we get. It allows for the incorporation of personal beliefs and uncertainties, making it particularly useful in decision-making under uncertainty.\nSo, for the same example of trying to understand the probability of landing heads for a coin flip, the Bayesian thought process would go something like this. Initially, you would simply come up with a probability out of thin air and say something like the probability of landing heads is 0.4. This prior belief can be based on anything. Now, this may seem a bit odd, but, the most important concept here is that this prior should be revised into a posterior based on the new evidence that you gather. So, after a few flips, you would adjust your beliefs with the new data that you gathered. This school of thought proves to be very useful in trying to predict the probability of events which have not occurred yet, such as trying to compute the probability that a brand new sports team will win a championship."
  },
  {
    "objectID": "projects/VAE-Generation/index.html",
    "href": "projects/VAE-Generation/index.html",
    "title": "Synthetic Data Generation with Variational Autoencoders",
    "section": "",
    "text": "In the modern age of big data, robust and diverse datasets are extremely valuable. Data is what fuels the machine learning models that we, as a society, rely on for much of our daily lives. From cellphones to cars, machine learning has found its way into anywhere it could be useful. Despite the almost infinite range of applications for machine learning, there is one major obstacle holding us back. And that is the data. Acquiring a useful dataset is an extremely important part of machine learning. Laden with challenges involving privacy concerns and accessibility, this task is riddled with roadblocks.\nSo, if we don’t have the data we need to build these models, why don’t we just make it ourselves? In recent times, researchers have said just this. They have looked at synthetic data generation to solve this issue of data scarcity directly. With generative AI, we now have the ability to create realistic datasets, circumventing traditional data acquisition methods and their respective problems.\nThroughout this post, I will explore how variational autoencoders can be used to bring forth realistic synthetic data, demystifying their achitecture and decoding their generational abilities."
  },
  {
    "objectID": "projects/VAE-Generation/index.html#encoder",
    "href": "projects/VAE-Generation/index.html#encoder",
    "title": "Synthetic Data Generation with Variational Autoencoders",
    "section": "Encoder",
    "text": "Encoder\nThe encoder’s job is to compress the original data into a latent space representation. Normally, the encoder is built as layers of nodes, where each layer reduces the dimensionality of the data. As the data is constricted, the encoder will learn to extract only the most useful bits of information and discard any redundant or otherwise useless parts of the dataset."
  },
  {
    "objectID": "projects/VAE-Generation/index.html#latent-space",
    "href": "projects/VAE-Generation/index.html#latent-space",
    "title": "Synthetic Data Generation with Variational Autoencoders",
    "section": "Latent Space",
    "text": "Latent Space\nThe output of the encoder is the latent space, a compressed representation of the input data. In an undercomplete autoencoder, this space should be a lower-dimensional form of the data which captures all important characteristics of the original dataset. The latent space is also the input to the decoder."
  },
  {
    "objectID": "projects/VAE-Generation/index.html#decoder",
    "href": "projects/VAE-Generation/index.html#decoder",
    "title": "Synthetic Data Generation with Variational Autoencoders",
    "section": "Decoder",
    "text": "Decoder\nAfter the latent space, we have the decoder. The decoder’s job is to reconstruct the original data from the latent space. The architecture is similar to that of the encoder. Usually, it is simply a slightly modified encoder architecture in reverse. The decoder will attempt to learn to generate the original data from the latent space, minimizing the difference between the reconstructed data and the original input."
  },
  {
    "objectID": "projects/VAE-Generation/index.html#loss-function",
    "href": "projects/VAE-Generation/index.html#loss-function",
    "title": "Synthetic Data Generation with Variational Autoencoders",
    "section": "Loss Function",
    "text": "Loss Function\nAn autoencoder uses the dissimilarity between the original and reconstructed data as its loss function. This dissimilarity is otherwise known as the reconstruction error. Depending on the type of data it attempts to reconstruct, this error can be defined in different ways. Mean Squared Error (MSE) and binary cross-entropy are often used here."
  },
  {
    "objectID": "projects/VAE-Generation/index.html#encoder-1",
    "href": "projects/VAE-Generation/index.html#encoder-1",
    "title": "Synthetic Data Generation with Variational Autoencoders",
    "section": "Encoder",
    "text": "Encoder\nThe structure of the encoder is mostly similar to a traditional autoencoder. The only difference is that instead of mapping the input data points to points in the latent space, the encoder is built to map the input data points to the parameters of the latent distribution, the mean and variance (\\(\\mu\\) and \\(\\sigma^2\\)). This distribution is then randomly sampled to construct the latent space."
  },
  {
    "objectID": "projects/VAE-Generation/index.html#latent-space-1",
    "href": "projects/VAE-Generation/index.html#latent-space-1",
    "title": "Synthetic Data Generation with Variational Autoencoders",
    "section": "Latent Space",
    "text": "Latent Space\nThe learned latent distribution is sampled to contruct the latent space. These samples are the input to the decoder."
  },
  {
    "objectID": "projects/VAE-Generation/index.html#decoder-1",
    "href": "projects/VAE-Generation/index.html#decoder-1",
    "title": "Synthetic Data Generation with Variational Autoencoders",
    "section": "Decoder",
    "text": "Decoder\nThe decoder of a variational autoencoder has no differences to the traditional. It is also neural network whose job is to reconstruct the original data from the latent space."
  },
  {
    "objectID": "projects/VAE-Generation/index.html#loss-function-1",
    "href": "projects/VAE-Generation/index.html#loss-function-1",
    "title": "Synthetic Data Generation with Variational Autoencoders",
    "section": "Loss Function",
    "text": "Loss Function\nVariational autoencoders use a loss function with two main components:\n\nReconstruction Loss: Just like traditional autoencoders, the reconstruction loss captures the difference between the original input data and the reconstructed data from the decoder.\nKL-Divergence Loss: In VAEs, a Kullback-Leibler (KL) divergence loss is added to the loss function in order to enforce the latent space to follow a gaussian distribution. This is a regularizing term."
  },
  {
    "objectID": "projects/VAE-Generation/index.html#a-hypothetical-problem-class-imbalance",
    "href": "projects/VAE-Generation/index.html#a-hypothetical-problem-class-imbalance",
    "title": "Synthetic Data Generation with Variational Autoencoders",
    "section": "A Hypothetical Problem: Class Imbalance",
    "text": "A Hypothetical Problem: Class Imbalance\nConsider a simply binary classification problem where the task is to predict whether any given image is of a dog or a cat. Now, suppose that you have a dataset containing 1000 images of dogs and 10000 images of cats. Therefore, you have ten-fold more instances of cats than dogs in your dataset.\nHere, we run into the issue of class imbalance. In other words, the class distribution is highly weighted in favor of one class. This is a common problem faced by machine learning engineers everyday. Machine learning models will may struggle to identify dogs in this scenario, as the dataset simply does not contain as many examples for the dog class. Additionally, some models may tend to predict the majority class more of the time, leading to biased predictions and innacurate results.\nSo, how can we solve this problem. There are many approaches which you could take, the most simple of which is to reduce the total number of cat images to equal that of dog images. However, with this solution, you risk training a worse model due to the lack of data.\nEnter variational autoencoders and synthetic data generation. In this case, you could utilize VAEs to synthetically generate images of cats, based on the learned latent distribution and thus solve this issue of class imbalance. By balancing the class distribution and providing the model with more examples of the minority class, you can improve its ability to correctly predict labels for the minority class (dogs)."
  },
  {
    "objectID": "projects/VAE-Generation/index.html#imports",
    "href": "projects/VAE-Generation/index.html#imports",
    "title": "Synthetic Data Generation with Variational Autoencoders",
    "section": "Imports",
    "text": "Imports\nFinally, we can get into the code. We are going to start with our imports. We will be building our VAE model with keras, so most of the inputs will be for layer types and such. Also, we will be using numpy for some matrix computations and matplotlib for some visualizations.\n\n# Imports\nimport numpy as np\nimport tensorflow as tf\nfrom matplotlib import pyplot as plt\nfrom tensorflow.keras import layers\nfrom tensorflow.keras.models import Model\nfrom tensorflow.keras.layers import Input\nfrom tensorflow.keras.layers import Dense\nfrom tensorflow.keras.layers import Conv2D\nfrom tensorflow.keras.layers import Conv2DTranspose\nfrom tensorflow.keras.layers import LeakyReLU\nfrom tensorflow.keras.layers import BatchNormalization\nfrom tensorflow.keras.layers import Flatten\nfrom tensorflow.keras.layers import Reshape\nfrom tensorflow.keras.utils import plot_model\nfrom tensorflow.keras.callbacks import EarlyStopping"
  },
  {
    "objectID": "projects/VAE-Generation/index.html#data-preprocessing",
    "href": "projects/VAE-Generation/index.html#data-preprocessing",
    "title": "Synthetic Data Generation with Variational Autoencoders",
    "section": "Data Preprocessing",
    "text": "Data Preprocessing\nWe are going to use the MNIST Handwritten Digist Datset for this project, so we must load this in. Thankfully, the images can be loaded with keras. Here we also want to pull only images of 0’s, since it is the minority class we want to generate samples for. To make sure we have the right images in our set, we will display one of the images from the training set along with its label.\n\n# Load data\n(X_train, y_train), (X_test, y_test) = tf.keras.datasets.mnist.load_data()\n\n# Pull only images of hand-drawn zeroes\nX_train_zeroes = X_train[y_train == 0]\ny_train_zeroes = y_train[y_train == 0]\n\nX_test_zeroes = X_test[y_test == 0]\ny_test_zeroes = y_test[y_test == 0]\n\nimage = X_train_zeroes[100, :].reshape(28, 28)\nplt.imshow(image, cmap='gray')\nprint(f'Label for Image: {y_train_zeroes[100]}')\n\nLabel for Image: 0\n\n\n\n\n\nIn the cell below, we are simply converting the values to 32-bit precision for the model, so the model won’t have to do it later. The division by 255 is for scaling the input features between 0.0 and 1.0. This scaling is recommended for neural networks, especially with default hyperparameters.\n\nX_train_zeroes = X_train_zeroes.astype(\"float32\") / 255\nX_test_zeroes = X_test_zeroes.astype(\"float32\") / 255\n\n# Reshape dataset to shape of (28, 28, 1) expected by the model\nX_train_zeroes = np.reshape(X_train_zeroes, newshape=(X_train_zeroes.shape[0], X_train_zeroes.shape[1], X_train_zeroes.shape[2], 1))\nX_test_zeroes = np.reshape(X_test_zeroes, newshape=(X_test_zeroes.shape[0], X_test_zeroes.shape[1], X_test_zeroes.shape[2], 1))\n\nX_train_zeroes.shape\n\n(5923, 28, 28, 1)"
  },
  {
    "objectID": "projects/VAE-Generation/index.html#sampling-layer",
    "href": "projects/VAE-Generation/index.html#sampling-layer",
    "title": "Synthetic Data Generation with Variational Autoencoders",
    "section": "Sampling Layer",
    "text": "Sampling Layer\nBefore constructing anything else, we have to define a custom keras layer for the sampling layer. The sampling layer is the latent space in the VAE model. The layer will use the mean and variance inputs to trasnform them into a normal distribution. The output of this layer will be the latent space.\n\n# Create sampling layer\nclass Sampling(layers.Layer):\n    \"\"\"Use (z_mean, z_log_var) to sample z (latent space)\"\"\"\n    \n    # Call override\n    def call(self, inputs):\n        z_mean, z_log_var = inputs\n        batch = tf.shape(z_mean)[0]\n        dim = tf.shape(z_mean)[1]\n        epsilon = tf.random.normal(shape=(batch, dim))\n        return z_mean + tf.exp(0.5 * z_log_var) * epsilon"
  },
  {
    "objectID": "projects/VAE-Generation/index.html#encoder-2",
    "href": "projects/VAE-Generation/index.html#encoder-2",
    "title": "Synthetic Data Generation with Variational Autoencoders",
    "section": "Encoder",
    "text": "Encoder\nNow, we can build the encoder. Let’s define a function to make it a bit easier to build when we have to do so later. We use 2D-convolutional layers to capture some spatial patterns and learn features in our images. For a stable training, we use batch normalization layers to normalize the features. Finally, leaky rectified linear unit (LeakyReLU) is used as the activation layer to mitigate vanishing gradients by allowing negative gradients in training.\nThe architecture here is subject to a lot of tuning. You can change any of this to better suit the specific problem you may be working with.\nAfter the primary encoder layers, we will flatten the vectors from shape (7, 7, 64) into a 1D vector for the latent space. Then, since we are attempting to learn a distribution from the data, we will feed the flattened vector is fed into mean and variance layers (representing the distribution’s parameters).\nFinally, the parameters are given to the sampling layer to randomly sample a normal distribution. This sample is the output of the encoder\nHere, img_size and img_channels is used for setting the shape of the input layer. The latent_dim parameter can also be customized here. The encoder model and the pre_flatten_shape, used for reconstruction in the decoder, are returned.\n\n# Define function for building the encoder\ndef build_encoder(img_size, img_channels, latent_dim):\n    \n    # Input layer\n    input_layer = Input(shape=(img_size, img_size, img_channels), name='encoder_in')\n    \n    # Level 1\n    x = Conv2D(filters=1, kernel_size=(3, 3), padding='same', strides=1, name='level_1_conv')(input_layer)\n    x = BatchNormalization(name='level_1_batchnorm')(x)\n    x = LeakyReLU(name='level_1_activation')(x)\n    \n    # Level 2\n    x = Conv2D(filters=32, kernel_size=(3, 3), padding='same', strides=2, name='level_2_conv')(x)\n    x = BatchNormalization(name='level_2_batchnorm')(x)\n    x = LeakyReLU(name='level_2_activation')(x)\n    \n    # Level 3\n    x = Conv2D(filters=64, kernel_size=(3, 3), padding='same', strides=2, name='level_3_conv')(x)\n    x = BatchNormalization(name='level_3_batchnorm')(x)\n    x = LeakyReLU(name='level_3_activation')(x)\n    \n    # Flatten\n    pre_flatten_shape = tf.keras.backend.int_shape(x)[1:]\n    x = Flatten(name='flatten')(x)\n    \n    # Mean and Var\n    z_mean = Dense(latent_dim, name='z_mean')(x)\n    z_log_var = Dense(latent_dim, name='z_log_var')(x)\n    \n    # Latent Space (aka bottleneck)\n    z = Sampling(name='z')([z_mean, z_log_var])\n    \n    # Encoder\n    encoder = Model(input_layer, [z_mean, z_log_var, z], name=\"encoder\")\n    \n    return encoder, pre_flatten_shape\n\nWe have completed the encoder, so let’s build it and take a look at the summary to see it if everything looks good. We will build it with img_size of 28, 1 channel for grey, and a latent dimension of 2.\n\n# build encoder\nencoder, pre_flatten_shape = build_encoder(28, 1, 2)\nencoder.summary()\nplot_model(encoder)\n\nModel: \"encoder\"\n__________________________________________________________________________________________________\n Layer (type)                   Output Shape         Param #     Connected to                     \n==================================================================================================\n encoder_in (InputLayer)        [(None, 28, 28, 1)]  0           []                               \n                                                                                                  \n level_1_conv (Conv2D)          (None, 28, 28, 1)    10          ['encoder_in[0][0]']             \n                                                                                                  \n level_1_batchnorm (BatchNormal  (None, 28, 28, 1)   4           ['level_1_conv[0][0]']           \n ization)                                                                                         \n                                                                                                  \n level_1_activation (LeakyReLU)  (None, 28, 28, 1)   0           ['level_1_batchnorm[0][0]']      \n                                                                                                  \n level_2_conv (Conv2D)          (None, 14, 14, 32)   320         ['level_1_activation[0][0]']     \n                                                                                                  \n level_2_batchnorm (BatchNormal  (None, 14, 14, 32)  128         ['level_2_conv[0][0]']           \n ization)                                                                                         \n                                                                                                  \n level_2_activation (LeakyReLU)  (None, 14, 14, 32)  0           ['level_2_batchnorm[0][0]']      \n                                                                                                  \n level_3_conv (Conv2D)          (None, 7, 7, 64)     18496       ['level_2_activation[0][0]']     \n                                                                                                  \n level_3_batchnorm (BatchNormal  (None, 7, 7, 64)    256         ['level_3_conv[0][0]']           \n ization)                                                                                         \n                                                                                                  \n level_3_activation (LeakyReLU)  (None, 7, 7, 64)    0           ['level_3_batchnorm[0][0]']      \n                                                                                                  \n flatten (Flatten)              (None, 3136)         0           ['level_3_activation[0][0]']     \n                                                                                                  \n z_mean (Dense)                 (None, 2)            6274        ['flatten[0][0]']                \n                                                                                                  \n z_log_var (Dense)              (None, 2)            6274        ['flatten[0][0]']                \n                                                                                                  \n z (Sampling)                   (None, 2)            0           ['z_mean[0][0]',                 \n                                                                  'z_log_var[0][0]']              \n                                                                                                  \n==================================================================================================\nTotal params: 31,762\nTrainable params: 31,568\nNon-trainable params: 194\n__________________________________________________________________________________________________\n\n\n\n\n\nEverything looks good! We can see the the input is a 28x28 image with 1 channel for grey. The final output is a vector of length 2."
  },
  {
    "objectID": "projects/VAE-Generation/index.html#decoder-2",
    "href": "projects/VAE-Generation/index.html#decoder-2",
    "title": "Synthetic Data Generation with Variational Autoencoders",
    "section": "Decoder",
    "text": "Decoder\nNow, onto the decoder. Here, we are essentially reversing the architecture of the encoder, without using the sampling layer.\nThe decoder is fed a latent vector of length 2 (from the encoder), and will try and reconstruct the 28x28 image from it. Here we will use the pre_flatten_shape from above to expand the latent vector to size (7,7,28). Also, instead of using Conv2D layers, we are using Conv2DTranspose layers. This is because the transpose is used for upsampling, while Conv2D is usually used for downsampling or feature extraction.\nFinally, there is no batch normalization done after the last level of the decoder, as it will mess up the output images that we get back. Normalization is not needed here as it is the output of the model.\n\n# Define function for building the decoder\ndef build_decoder(pre_flatten_shape, latent_dim):\n    \n    # Input from decoder\n    latent_input = Input(shape=(latent_dim), name='decoder_in')\n    \n    # Expand latent vector length to pre_flatten_shape\n    x = Dense(np.prod(pre_flatten_shape), name='expand_latent_vector')(latent_input)\n    \n    # Reshape to (7, 7, 64) aka pre_flatten_shape\n    x = Reshape(target_shape=pre_flatten_shape)(x)\n    \n    # Level 1\n    x = Conv2DTranspose(filters=64, kernel_size=(3, 3), padding='same', strides=2, name='level_1_conv')(x)\n    x = BatchNormalization(name='level_1_batchnorm')(x)\n    x = LeakyReLU(name='level_1_activation')(x)\n\n    \n    # Level 2\n    x = Conv2DTranspose(filters=32, kernel_size=(3, 3), padding='same', strides=2, name='level_2_conv')(x)\n    x = BatchNormalization(name='level_2_batchnorm')(x)\n    x = LeakyReLU(name='level_2_activation')(x)\n    \n    # Level 3\n    x = Conv2DTranspose(filters=1, kernel_size=(3, 3), padding='same', strides=1, name='level_3_conv')(x)\n    x = LeakyReLU(name='level_3_activation')(x)\n\n    \n    # Output Layer\n    output_layer = x\n    \n    # Decoder\n    decoder = Model(latent_input, output_layer, name=\"decoder\")\n    \n    return decoder\n\nAgain, like we did with the encoder, let’s build it to make sure we have constructed it correctly.\n\n# build decoder\ndecoder = build_decoder(pre_flatten_shape, 2)\ndecoder.summary()\nplot_model(decoder)\n\nModel: \"decoder\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n decoder_in (InputLayer)     [(None, 2)]               0         \n                                                                 \n expand_latent_vector (Dense  (None, 3136)             9408      \n )                                                               \n                                                                 \n reshape_9 (Reshape)         (None, 7, 7, 64)          0         \n                                                                 \n level_1_conv (Conv2DTranspo  (None, 14, 14, 64)       36928     \n se)                                                             \n                                                                 \n level_1_batchnorm (BatchNor  (None, 14, 14, 64)       256       \n malization)                                                     \n                                                                 \n level_1_activation (LeakyRe  (None, 14, 14, 64)       0         \n LU)                                                             \n                                                                 \n level_2_conv (Conv2DTranspo  (None, 28, 28, 32)       18464     \n se)                                                             \n                                                                 \n level_2_batchnorm (BatchNor  (None, 28, 28, 32)       128       \n malization)                                                     \n                                                                 \n level_2_activation (LeakyRe  (None, 28, 28, 32)       0         \n LU)                                                             \n                                                                 \n level_3_conv (Conv2DTranspo  (None, 28, 28, 1)        289       \n se)                                                             \n                                                                 \n level_3_activation (LeakyRe  (None, 28, 28, 1)        0         \n LU)                                                             \n                                                                 \n=================================================================\nTotal params: 65,473\nTrainable params: 65,281\nNon-trainable params: 192\n_________________________________________________________________\n\n\n\n\n\nThe input and output shapes look correct, so we can move on."
  },
  {
    "objectID": "projects/VAE-Generation/index.html#variational-autoencoder",
    "href": "projects/VAE-Generation/index.html#variational-autoencoder",
    "title": "Synthetic Data Generation with Variational Autoencoders",
    "section": "Variational Autoencoder",
    "text": "Variational Autoencoder\nSince we now have both an encoder and decoder model build, we can put the two together to build the full variational autoencoder.\nWe will be subclassing a keras model and customizing some functions to make this work. The functions are detailed below.\n\nInitializer\nThis is how we will initialize our class. We are setting the encoder and decoder that we passed in to attributes for this model. Loss trackers are set too.\n\n\nMetrics\nThe metrics function will collect metric values and display them during fit. These metrics are also logged by the History object returned after the model fit is complete. We simply list the metrics we want returned.\n\n\nTrain Step\nOur custom training step will first send data into the encoder to be encoder. The mean, variance, and latent vectors are returned. The latent vector is then fed into the decoder to get the reconstructed data.\nAfter we have done all that, we can compute the losses. The reconstruction_loss will use binary crossentropy to evaluate the difference between the original data and the reconstructed data. The kl_loss penalizes deciations from a gaussian distribution, thus enforcing a gaussian structure in the latent space. Finally, the aforementioned two losses are added together to get the total loss.\nThen, we compute and apply the gradients according to the total loss.\nFinally, we update the state of the loss trackers and return the losses.\n\n\nTest Step\nThis step is exactly the same as the train step, without computing and applying gradients.\n\n\nCall\nNothing is important here. We will just override the call method to let us save the model later.\n\n# Subclass Model to build custom VAE model\nclass VAE(Model):\n    def __init__(self, encoder, decoder, **kwargs):\n        \n        super().__init__(**kwargs)\n        \n        self.encoder = encoder\n        self.decoder = decoder\n        self.total_loss_tracker = tf.keras.metrics.Mean(name=\"total_loss\")\n        self.reconstruction_loss_tracker = tf.keras.metrics.Mean( name=\"reconstruction_loss\")\n        self.kl_loss_tracker = tf.keras.metrics.Mean(name=\"kl_loss\")\n        \n    @property\n    def metrics(self):\n        return [self.total_loss_tracker, self.reconstruction_loss_tracker, self.kl_loss_tracker]\n\n    def train_step(self, data):\n        with tf.GradientTape() as tape:\n            # Get Mean, Var, and Latent Space from encoder\n            z_mean, z_log_var, z = self.encoder(data)\n\n            # Reconstruct data from encoder with decoder\n            reconstructed_data = self.decoder(z)\n\n            # Compute losses\n                # BCE as reconstruction loss\n            reconstruction_loss = tf.reduce_mean(\n                tf.reduce_sum(\n                        tf.keras.losses.binary_crossentropy(data, reconstructed_data), axis=(1,2)\n                )\n            )\n            kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n            kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n            total_loss = reconstruction_loss + kl_loss\n\n        # Compute and Apply Gradients\n        grads = tape.gradient(total_loss, self.trainable_weights)\n        self.optimizer.apply_gradients(zip(grads, self.trainable_weights))\n\n        # Update and Return Losses\n        self.total_loss_tracker.update_state(total_loss)\n        self.reconstruction_loss_tracker.update_state(reconstruction_loss)\n        self.kl_loss_tracker.update_state(kl_loss)\n\n        return {\n            \"loss\" : self.total_loss_tracker.result(),\n            \"reconstruction_loss\": self.reconstruction_loss_tracker.result(),\n            \"kl_loss\": self.kl_loss_tracker.result(),\n        }    \n    \n    def test_step(self, data):        \n        if isinstance(data, tuple): data=data[0]\n        \n        # Get Mean, Var, and Latent Space from encoder\n        z_mean, z_log_var, z = self.encoder(data)\n\n        # Reconstruct data from encoder with decoder\n        reconstructed_data = self.decoder(z)\n        \n        # Compute losses\n            # BCE as reconstruction loss\n        reconstruction_loss = tf.reduce_mean(\n            tf.reduce_sum(\n                    tf.keras.losses.binary_crossentropy(data, reconstructed_data), axis=(1,2)\n            )\n        )\n        \n        kl_loss = -0.5 * (1 + z_log_var - tf.square(z_mean) - tf.exp(z_log_var))\n        kl_loss = tf.reduce_mean(tf.reduce_sum(kl_loss, axis=1))\n        total_loss = reconstruction_loss + kl_loss\n\n        return {\n            \"loss\" : total_loss,\n            \"reconstruction_loss\": reconstruction_loss,\n            \"kl_loss\": kl_loss,\n        }\n    \n    \n    # Override call\n    def call(self, inputs):\n        # Encode\n        _,_, encoded = self.encoder(inputs)\n        \n        # Decode\n        decoded = self.decoder(encoded)\n        \n        # Return\n        return decoded\n\nNow, we are finally done building all the models necessary. Just to make sure everything is good for a final time, we can build the variational autoencoder and check the summary. Because we have subclassed a keras Model(), we must build it and supply an input shape before we call summary().\n\nvae = VAE(encoder, decoder, name=\"VAE\")\nvae.build(input_shape=(None, 28, 28, 1))\nvae.summary()\n\nModel: \"VAE\"\n_________________________________________________________________\n Layer (type)                Output Shape              Param #   \n=================================================================\n encoder (Functional)        [(None, 2),               31762     \n                              (None, 2),                         \n                              (None, 2)]                         \n                                                                 \n decoder (Functional)        (None, 28, 28, 1)         65473     \n                                                                 \n=================================================================\nTotal params: 97,241\nTrainable params: 96,849\nNon-trainable params: 392\n_________________________________________________________________"
  },
  {
    "objectID": "projects/VAE-Generation/index.html#vae-training",
    "href": "projects/VAE-Generation/index.html#vae-training",
    "title": "Synthetic Data Generation with Variational Autoencoders",
    "section": "VAE Training",
    "text": "VAE Training\nOur model is built and looks good. We can finally move on to training it. Before training, we must compile the model. We will be using Adam as the optimizer and implementing early stopping for our training run. We will stop the model and restore weights if we see that the validation loss is not improving for 5 epochs. The early stopping can only stop the training after epoch 5 in order to allow for a warm-up period. We do this so that we don’t overfit the training set and lose diversity in our generated samples.\nNext, we can fit the model. Please note that the hyperparameters here should be tuned for your own specific application.\n\n# Compile VAE\nvae.compile(optimizer = 'Adam')\n\n# Implement Early Stopping\nearly_stopping_monitor = EarlyStopping(\n    monitor='val_loss',\n    patience=5,\n    restore_best_weights=True,\n    start_from_epoch=5\n)\n\n# Fit model\nhistory = vae.fit(X_train_zeroes, epochs=50, batch_size=32, validation_data=(X_test_zeroes, X_test_zeroes), callbacks=[early_stopping_monitor], verbose=2)\n\nEpoch 1/50\n186/186 - 9s - loss: 288.5445 - reconstruction_loss: 285.5083 - kl_loss: 3.0362 - val_loss: 237.9042 - val_reconstruction_loss: 234.3238 - val_kl_loss: 3.5803 - 9s/epoch - 47ms/step\nEpoch 2/50\n186/186 - 6s - loss: 213.4632 - reconstruction_loss: 210.3203 - kl_loss: 3.1428 - val_loss: 218.8881 - val_reconstruction_loss: 215.6606 - val_kl_loss: 3.2276 - 6s/epoch - 34ms/step\nEpoch 3/50\n186/186 - 6s - loss: 196.5661 - reconstruction_loss: 192.9294 - kl_loss: 3.6367 - val_loss: 212.2316 - val_reconstruction_loss: 208.1650 - val_kl_loss: 4.0666 - 6s/epoch - 33ms/step\nEpoch 4/50\n186/186 - 6s - loss: 188.3397 - reconstruction_loss: 184.4873 - kl_loss: 3.8525 - val_loss: 199.0696 - val_reconstruction_loss: 194.7719 - val_kl_loss: 4.2977 - 6s/epoch - 33ms/step\nEpoch 5/50\n186/186 - 6s - loss: 185.1732 - reconstruction_loss: 181.3222 - kl_loss: 3.8510 - val_loss: 262.3339 - val_reconstruction_loss: 257.4344 - val_kl_loss: 4.8995 - 6s/epoch - 33ms/step\nEpoch 6/50\n186/186 - 6s - loss: 226.3369 - reconstruction_loss: 223.2303 - kl_loss: 3.1065 - val_loss: 234.8180 - val_reconstruction_loss: 232.5843 - val_kl_loss: 2.2337 - 6s/epoch - 33ms/step\nEpoch 7/50\n186/186 - 6s - loss: 195.5435 - reconstruction_loss: 192.3056 - kl_loss: 3.2379 - val_loss: 203.1790 - val_reconstruction_loss: 199.2196 - val_kl_loss: 3.9594 - 6s/epoch - 33ms/step\nEpoch 8/50\n186/186 - 6s - loss: 180.6178 - reconstruction_loss: 176.7197 - kl_loss: 3.8982 - val_loss: 188.0248 - val_reconstruction_loss: 183.5997 - val_kl_loss: 4.4251 - 6s/epoch - 33ms/step\nEpoch 9/50\n186/186 - 6s - loss: 201.7268 - reconstruction_loss: 197.7177 - kl_loss: 4.0090 - val_loss: 219.0097 - val_reconstruction_loss: 215.5794 - val_kl_loss: 3.4303 - 6s/epoch - 33ms/step\nEpoch 10/50\n186/186 - 6s - loss: 186.3577 - reconstruction_loss: 182.1717 - kl_loss: 4.1859 - val_loss: 187.2178 - val_reconstruction_loss: 183.0271 - val_kl_loss: 4.1908 - 6s/epoch - 33ms/step\nEpoch 11/50\n186/186 - 6s - loss: 175.3111 - reconstruction_loss: 170.8647 - kl_loss: 4.4464 - val_loss: 179.0291 - val_reconstruction_loss: 174.4716 - val_kl_loss: 4.5576 - 6s/epoch - 34ms/step\nEpoch 12/50\n186/186 - 6s - loss: 174.9617 - reconstruction_loss: 170.7765 - kl_loss: 4.1852 - val_loss: 180.2741 - val_reconstruction_loss: 176.2120 - val_kl_loss: 4.0621 - 6s/epoch - 34ms/step\nEpoch 13/50\n186/186 - 6s - loss: 191.4679 - reconstruction_loss: 187.5513 - kl_loss: 3.9167 - val_loss: 213.5826 - val_reconstruction_loss: 211.0571 - val_kl_loss: 2.5255 - 6s/epoch - 34ms/step\nEpoch 14/50\n186/186 - 6s - loss: 187.7171 - reconstruction_loss: 183.6468 - kl_loss: 4.0704 - val_loss: 180.3252 - val_reconstruction_loss: 176.5847 - val_kl_loss: 3.7405 - 6s/epoch - 34ms/step\nEpoch 15/50\n186/186 - 6s - loss: 175.2255 - reconstruction_loss: 170.9659 - kl_loss: 4.2595 - val_loss: 192.9127 - val_reconstruction_loss: 188.5997 - val_kl_loss: 4.3130 - 6s/epoch - 33ms/step\nEpoch 16/50\n186/186 - 6s - loss: 176.7319 - reconstruction_loss: 172.6772 - kl_loss: 4.0547 - val_loss: 177.2155 - val_reconstruction_loss: 173.9822 - val_kl_loss: 3.2333 - 6s/epoch - 33ms/step\nEpoch 17/50\n186/186 - 6s - loss: 174.6873 - reconstruction_loss: 170.5855 - kl_loss: 4.1018 - val_loss: 175.2054 - val_reconstruction_loss: 170.9537 - val_kl_loss: 4.2518 - 6s/epoch - 33ms/step\nEpoch 18/50\n186/186 - 6s - loss: 169.3542 - reconstruction_loss: 165.2892 - kl_loss: 4.0649 - val_loss: 170.4933 - val_reconstruction_loss: 166.4358 - val_kl_loss: 4.0576 - 6s/epoch - 33ms/step\nEpoch 19/50\n186/186 - 6s - loss: 167.7414 - reconstruction_loss: 163.7091 - kl_loss: 4.0321 - val_loss: 170.2082 - val_reconstruction_loss: 165.7858 - val_kl_loss: 4.4224 - 6s/epoch - 33ms/step\nEpoch 20/50\n186/186 - 6s - loss: 208.2969 - reconstruction_loss: 204.9203 - kl_loss: 3.3767 - val_loss: 213.3124 - val_reconstruction_loss: 209.9893 - val_kl_loss: 3.3230 - 6s/epoch - 33ms/step\nEpoch 21/50\n186/186 - 7s - loss: 203.8594 - reconstruction_loss: 200.4045 - kl_loss: 3.4549 - val_loss: 194.6826 - val_reconstruction_loss: 190.7970 - val_kl_loss: 3.8856 - 7s/epoch - 36ms/step\nEpoch 22/50\n186/186 - 7s - loss: 181.2387 - reconstruction_loss: 177.4598 - kl_loss: 3.7789 - val_loss: 175.7598 - val_reconstruction_loss: 172.0283 - val_kl_loss: 3.7314 - 7s/epoch - 37ms/step\nEpoch 23/50\n186/186 - 7s - loss: 178.9578 - reconstruction_loss: 175.2243 - kl_loss: 3.7335 - val_loss: 173.0352 - val_reconstruction_loss: 169.3694 - val_kl_loss: 3.6659 - 7s/epoch - 37ms/step\nEpoch 24/50\n186/186 - 7s - loss: 183.1370 - reconstruction_loss: 178.8514 - kl_loss: 4.2856 - val_loss: 190.2056 - val_reconstruction_loss: 187.0690 - val_kl_loss: 3.1365 - 7s/epoch - 36ms/step"
  },
  {
    "objectID": "projects/VAE-Generation/index.html#vae-visualizations",
    "href": "projects/VAE-Generation/index.html#vae-visualizations",
    "title": "Synthetic Data Generation with Variational Autoencoders",
    "section": "VAE Visualizations",
    "text": "VAE Visualizations\nOnce the model is done training, we can visualize the learning curves by plotting the losses.\n\n# Visualize Learning Curves\nplt.plot(history.history['loss'], label='train_total_loss')\nplt.plot(history.history['kl_loss'], label='train_kl_loss')\nplt.plot(history.history['reconstruction_loss'], label='train_reconstruction_loss')\nplt.plot(history.history['val_loss'], label='val_total_loss')\nplt.plot(history.history['val_kl_loss'], label='val_kl_loss')\nplt.plot(history.history['val_reconstruction_loss'], label='val_reconstruction_loss')\nplt.legend()\nplt.show()\n\n\n\n\nHere, we can see that our training and validation losses do not differ much. This is a good sign that we haven’t overtrained our model. The losses are a bit jumpy, but that is okay since we do not neccesarily want a perfect reconstruction of the data. Remember, we want to generate plausible new sample images for the dataset. They should not be perfect reconstructions of our training set, but should retain the same general structure (aka look like handwritten zeroes).\nGreat! We now have a trained variable autoencoder. Now what?\nThis is the exciting part. We can now sample from the learned latent space to generate new samples.\nWith np.random.normal, we can draw random samples from a normal distribution with mean 0 and standard deviation of 1. Then, we specify an output shape for the samples. Next, we feed the random sample into the decoder to decode the sampled latent vector back into the original data space. After all this, we will have a synthetically generated sample image.\nBelow we generate 10 new sample images and display them.\n\nnum_samples = 10\n\nfig, axes = plt.subplots(1, num_samples)\nfor i, ax in zip(axes, range(num_samples)):\n    latent_sample = np.random.normal(0, 1, (1, 2))\n    decoded_sample = vae.decoder.predict(latent_sample, verbose=0)\n    sample_image = decoded_sample.reshape(28, 28)\n    \n    axes[ax].imshow(sample_image, cmap='gray')\n    axes[ax].axis('off')\n\n\n\n\nWe can see that all the images generated are realistic representations of handwritten zeroes. They all are slightly imperfect, confirming that we have not lost all diversity in our generated samples. This is a good thing as we want diverse samples in order to train a robust classification model later on.\nFor a better understanding of the latent space, we can visualize the space with the images we generate from our latent vector of length 2. I chose a latent dimension of 2 mostly for this purpose.\nBelow, we define a function to generate samples throughout the latent space.\n\ndef latent_space_viz():\n    # Initialize figure as zeroed array\n    figure = np.zeros((28 * 10, 28 * 10))\n    \n    # Evenly spaced vector for x and y values in latent distribution\n    grid_x = np.linspace(-1, 1, 10)\n    grid_y = np.linspace(-1, 1, 10)[::-1]\n    \n    for idx_0, y in enumerate(grid_y):\n        for idx_1, x in enumerate(grid_x):\n            # Generate sample\n            latent_sample = np.array([[x, y]])\n            decoded_sample = vae.decoder.predict(latent_sample, verbose=0)\n            img = decoded_sample[0].reshape(28, 28)\n            \n            # Set image in figure grid\n            figure[idx_0*28 : (idx_0+1)*28, idx_1*28 : (idx_1+1)*28] = img\n            \n    # Setting figsize, ticks, and labels\n    plt.figure(figsize=(8, 8))\n    pixels = np.arange(28//2, 10*28+(28//2), 28)\n    plt.xticks(pixels, np.round(grid_x, 1))\n    plt.yticks(pixels, np.round(grid_y, 1))\n    plt.xlabel('z [0]')\n    plt.ylabel('z [1]')\n    plt.imshow(figure, cmap='gray')\n    plt.show()\n    \nlatent_space_viz()            \n\n\n\n\nFrom this visualization, it is easy to see how the model has mapped variations of handwritten 0’s to the latent space. Some are a bit skewed and others a bit more “perfect”. And since the distribution is continuous, we can draw an infinite number of samples. Between each of the displayed images, there are infinitely many more which are slightly different than the ones we see. This is simply a 10x10 grid of 100 generated samples.\nSimilar to other common forms of data augmentation, we seem to have covered some rotation and some scaling in our generated set. This diversity is great for training robust models with our new data."
  }
]