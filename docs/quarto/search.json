[
  {
    "objectID": "projects/wGAN-stock-predictions/index.html",
    "href": "projects/wGAN-stock-predictions/index.html",
    "title": "Deep Learning Equity Trading Model",
    "section": "",
    "text": "This project seeks to rely on recent deep learning and technical analysis advancements to generate predictions of short term price movements for the Apple stock, $AAPL.\nPlease check out the links below for the full paper and the project website.\nFull Paper\nProject Website"
  },
  {
    "objectID": "projects/wGAN-stock-predictions/index.html#problem-description",
    "href": "projects/wGAN-stock-predictions/index.html#problem-description",
    "title": "Deep Learning Equity Trading Model",
    "section": "Problem Description",
    "text": "Problem Description\nWith the New York Stock Exchange containing a total global market capitalization of $15 trillion, it is no surprise that both retail and institutional investors have relied on every possible advancement in the digital world to generate stronger returns [3]. As such, to remain a competitive investor in such a highly technologically optimized market, it is crucial that new innovative approaches to investing are made. Ultimately, this has led to an open market today where 90% of all short term trades and 50-70% of all trades are completed by stocking trading algorithms [3]. Primarily, these computer generated trades rely on technical analysis, which avoid traditional fundamental methodologies, and instead rely on past trading activity, price changes of a security, and patterns in charts to develop valuable indicators for a given security’s future price movements.\nOur solution proposal seeks to rely on recent deep learning and technical analysis advancements to generate predictions of short term price movements for a specific equity holding. Our model will rely on an ensemble approach by combining analysisfrom a Wasserstein Generative Adversarial Network (wGAN) and the pretrained Valence Aware Dictionary for Sentiment Reasoning (VADER) model to generate our prediction process. The wGAN serves as a stable learning model during gradient descent, while avoiding convergence failures and mode collapse. This model will serve as a generator that will utilize two key data points for prediction, correlated assets and technical analysis. This model will serve in conjunction with VADER, which will offer sentiment analysis of discussion of the chosen security, determining whether posts are positive, neutral, or negative. Ultimately, we seek to rely on each of these models to offer an alternative to human based decisionmaking for short-term trading and instead create automated and successful predictions to stock price movements."
  },
  {
    "objectID": "projects/wGAN-stock-predictions/index.html#motivation",
    "href": "projects/wGAN-stock-predictions/index.html#motivation",
    "title": "Deep Learning Equity Trading Model",
    "section": "Motivation",
    "text": "Motivation\nAccurate forecasting of the stock market can be greatly beneficial for a variety of persons. Politicians and government officials would be able to leverage this forecasting to predict the future health of the economy. And with such predictions, they would be able to enact policy to counteract problematic trends, leading to more stable economic growth on a national level. Investors would be able to better profit from trades, generating wealth and more stable income from the stock market. Businesses and corporations could even use this forecasting information for more accurate quarterly performance reviews to assess projected success.\nThese are just a few of the possible benefits to motivate a project around improving current state-of-the-art stock market prediction models. This just touches the surface of what is possible here. With this data, anyone could be able to achieve a greater understanding of the behavior of the stock market, which up until this point has been one of the largest mysteries in finance."
  },
  {
    "objectID": "projects/wGAN-stock-predictions/index.html#challenges-and-solutions",
    "href": "projects/wGAN-stock-predictions/index.html#challenges-and-solutions",
    "title": "Deep Learning Equity Trading Model",
    "section": "Challenges and Solutions",
    "text": "Challenges and Solutions\nThe primary question is: how can the currently available methods of forecasting the stock market be improved?\nEnter artificial intelligence, and more specifically, machine learning (ML) models. With innate abilities to identify patterns, trends, and mostly unnoticeable correlations between data points, ML models are the perfect candidate for such tasks. Recurrent Neural Network (RNN), Long Short-Term Memory (LSTM) [6], Gated Recurrent Unit (GRU) and models have all been used to accurately predict time series data before, so why not extend the ideas to predicting stock market prices? Unfortunately, the stock market is extremely volatile, and as a result, it is quite difficult to forecast. Because of this, searches for more complicated models which accurately assess the vast and obscure trends and patterns of the stock market have heightened in recent years, leading to improvements in GAN models and similar for this use case specifically [5].\nAlthough we have seen improvements in many of these more complicated stock price prediction models, there always remains a lack of assessment of sentiment directly from the public. In the most basic sense, people are the primary cause of stock market movements. With a public mass movement towards selling a specific stock, there is likely a fall in a stock price to follow. Similarly, if more people are willing to purchase a stock than sell it, then an increase in the price is likely to follow. Supply and demand are primary factors in stock price movement. Researchers attempt solving this challenge by scraping news articles from official sources, but oftentimes, sentiment received from official sources differs substantially from the actual sentiment of the public.\nTo rectify these issues, this project proposes the use of a wGANGP (Gradient Penalty) model fed by analyses of public sentiment gained from social media platforms along with basic and technical indicators associated with a specific stock ($APPL in our case). With data from posts and comments from social media, sentiment can be assessed directly from personal accounts rather than official news outlets, hopefully bypassing media biases which could skew the predicted stock prices. As people more directly influence the stock market, there is definitely a perceived benefit in this approach. Regarding the use of a wGAN-GP model specifically, it has been chosen for its improvements upon a traditional GAN, noting its advantages in training stability and higher likelihood of convergence. This is directly beneficial when using more complex models for the generator and discriminator in the GAN."
  },
  {
    "objectID": "projects/wGAN-stock-predictions/index.html#literature-survey",
    "href": "projects/wGAN-stock-predictions/index.html#literature-survey",
    "title": "Deep Learning Equity Trading Model",
    "section": "Literature Survey",
    "text": "Literature Survey\nThe article by Boris Banushev, Using the Latest Advancements in Deep Learning to Predict Stock Price Movements [1], provided us great insight into how a GAN can be applied to make time series predictions on a stock. In his project, Boris uses an Long-Short Term Memory Recurrent Neural Network for his generator and a Convolutional Neural Network as his discriminator. To supplement his data, he uses various techniques like Fourier transforms, stacked autoencoders, and eigen portfolios to assemble the most information possible on the stock. Although we will not be using most of these techniques, he does conduct sentiment analysis by gathering news articles and inputting their text into a pretrained BERT model. To tune the hyperparameters of the GAN model, Boris uses reinforcement learning and Bayesian optimization.\nIn Generative Adversarial Network for Stock Market Price Prediction by Ricardo Romero [4], the project’s main goal was to see if various deep learning models could predict whether a stock would increase or decrease. A GAN model, an ARIMA model, an LSTM model, and a Deep LSTM model were all trained and tested. The GAN model had the second highest accuracy of 72.68%, which was slightly under the LSTM model’s highest accuracy of 74.16%. For analyzing the GAN’s performance, 20k, 30k, and 50k epoch models were made, with the 50k epochs model performing the best.\nThe paper authored by Labiad, Benabbou, and Berrado titled Improving Stock Market Intraday Prediction by Generative Adversarial Neural Networks [2] attempted to use a GAN to predict intraday stock prices. The initial dataset of intraday prices had noisy variations and inconsistent distance between consecutive points. To remedy this, the team discretized the data to have points spaced by 10 minute intervals, which led to an overall reduction in the number of observations. Additionally, mode-specific normalization was utilized to better capture the complex distribution within the data. As a result, the synthetic data generated has a much closer distribution to the real data, leading to overall better training of the discriminator and improved prediction."
  },
  {
    "objectID": "projects/wGAN-stock-predictions/index.html#limitations-of-existing-approaches",
    "href": "projects/wGAN-stock-predictions/index.html#limitations-of-existing-approaches",
    "title": "Deep Learning Equity Trading Model",
    "section": "Limitations of Existing Approaches",
    "text": "Limitations of Existing Approaches\nCurrently, three primary solutions exist for predictions of time series stock market prices. They are the PCA-LSTM Model, Traditional GAN Model, and the Traditional GAN Model with BERT. Each of these have a series of limitations that our proposed approach seeks to imrpove upon and ultimately resolve entirely. The PCA-LSTM Model currently relies on an extremely trivial LSTM neural network for predictions solely based on technical indicators. We see this as an extreme limitation as the model is extremely simple and has no inclusion of fundamental analysis. To improve upon this simplicity, the Traditional GAN Model was created; however, is still limited by its lack of fundamental analysis and is typically prone to convergence failures and training instability. Finally, this traditional model has attempted to incorporate fundamental analysis by relying on BERT to offer fundamental analysis provided from biased news outlets, without any sentiment directly from the public. As such, we require that our model improves on these shortcomings by developing resistance to training instability and convergence failures as well as incorporate public sentiment from retail investors directly."
  },
  {
    "objectID": "projects/wGAN-stock-predictions/index.html#problem-definition",
    "href": "projects/wGAN-stock-predictions/index.html#problem-definition",
    "title": "Deep Learning Equity Trading Model",
    "section": "Problem Definition",
    "text": "Problem Definition\nLet \\(F={f_1, f_2, \\cdots , f_n}\\) be the set of \\(n\\) features which will serve as input to the model. These features will include basic indicators (high, low, open, close, volume), sentiment analysis scores, and technical indicators (moving averages, relative strength index, etc.).\nLet \\(W={w_1, w_2, \\cdots , w_n}\\) be the set of \\(n\\) weights which will be updated while training the model. The magnitude of \\(n\\) will be determined by the feature engineering described in the following section.\nThe prediction equation will be as follows:\n\\(stk\\_price = F \\odot W\\)\nWhere \\(stk\\_price\\) is the predicted closing stock price outputted by the model and \\(\\odot\\) denotes the Hadamard Product (more commonly described as component-wise multiplication).\n\nProject Pipeline Diagram\n\nPictured above is the complete data pipeline for this project. The data processing and analyses are divided into two subcomponents, one detailing the fundamental analysis and the other detailing the technical analysis. The fundamental analysis includes evaluation of social media sentiment regarding the stock. Posts are scraped from social media outlets, processed via the NLTK Python library, and finally given sentiment scores by the VADER model. The scores for all posts within a day are then averaged. On the technical analysis side, basic indicators are first pulled from TDAmeritrade and used in various ways to compute a large set of technical indicators, including moving averages and similar. The expanded set of indicators will then be fed into an autoencoder model to produce a set of compressed features. After this process, we will have a condensed set of indicators to merge back with the original technical indicator set to feed into the prediction model. Finally, the expanded indicator set and the daily averaged sentiment scores will be merged and fed to the wGAN-GP prediction model. This model will output the predicted closing prices for the stock."
  },
  {
    "objectID": "projects/wGAN-stock-predictions/index.html#the-dataset",
    "href": "projects/wGAN-stock-predictions/index.html#the-dataset",
    "title": "Deep Learning Equity Trading Model",
    "section": "The Dataset",
    "text": "The Dataset\nWith regards to the fundamental analysis side of the pipeline, tweets mentioning “$AAPL” and “AAPL” are scraped from Twitter. For the technical analysis portion, basic financial data for $AAPL and a number of comparative assets are pulled via the TDAmeritrade API. All data within our dataset spans from 05/09/2013 to 03/31/2023. For purposes of tuning the prediction model, the dataset was split 80:10:10, for the training, validation, and testing sets respectively."
  },
  {
    "objectID": "projects/wGAN-stock-predictions/index.html#fundamental-analysis",
    "href": "projects/wGAN-stock-predictions/index.html#fundamental-analysis",
    "title": "Deep Learning Equity Trading Model",
    "section": "Fundamental Analysis",
    "text": "Fundamental Analysis\nFor sentiment analysis the text extracted from the social media webscraper must be cleaned so that such analysis can be conducted smoothly. Using the Python NLTK library, the text is tokenized and converted into lowercase. Stop-words (unimportant words), such as “the” and “is”, as well as any non-alphanumeric characters are stripped. Finally, the tokens are lemmatized.\nThe pre-processed text from the Twitter posts is then given a sentiment score by the VADER model. Sentiment scores for each day are averaged, returning a final set of daily averaged sentiments from Twitter regarding."
  },
  {
    "objectID": "projects/wGAN-stock-predictions/index.html#technical-analysis",
    "href": "projects/wGAN-stock-predictions/index.html#technical-analysis",
    "title": "Deep Learning Equity Trading Model",
    "section": "Technical Analysis",
    "text": "Technical Analysis\nIn this project, we will generate new features from the few basic indicators and score the averaged public sentiment score for each day. The basic indicators - high, low, open, close, and volume - will initially be utilized to compute various technical indicators. These technical indicators may include items such as the moving average, Bollinger Band, relative strength index, average directional index, moving average convergence divergence, and many more. Next, these features will be input into a variational autoencoder in order to produce a compressed representation of the input features, denoted as compressed features. The compressed features will be added to the original features to extend the feature space. The variational autoencoder will not be used for dimensionality reduction, as these compressed features will be served to the final model in addition to the original features. This is done in hopes that the compressed features will better highlight stock price movement patterns."
  },
  {
    "objectID": "projects/wGAN-stock-predictions/index.html#model-architecture-descriptions",
    "href": "projects/wGAN-stock-predictions/index.html#model-architecture-descriptions",
    "title": "Deep Learning Equity Trading Model",
    "section": "Model Architecture Descriptions",
    "text": "Model Architecture Descriptions\n\nSocial Media Webscraper\nA python script was be developed to scrape data from Twitter. The is script was developed using the Scweet Python library. It searches tweets that mention the keywords “$AAPL” or “AAPL” and collects up to 100 of them for each day. Afterwards, the sentiment of each tweet is determined using VADER. The average sentiment amongst all tweets per day is calculated to get the sentiment of one day.\n\n\nVADER for Sentiment Analysis\nVADER from the NLTK Python library will be used to classify our scraped media posts as negative or positive. The VADER model maps lexical features to sentiment scores via a dictionary. It is a rule-based sentiment analysis tool, which is explicitly sensitive to web-based media. Words are identified and marked as positive or negative, and these markings are utilized to compute a polarity score which identifies the overall sentiment of the message. Words with higher negative sentiment are mapped to negative scores of greater magnitude, and vice-versa. The same applies for words viewed as positive.\n\n\nwGAN-GP for Stock Price Prediction\nAn improvement upon a traditional wGAN, which enforces a gradient norm penalty in the discriminator in order to achieve Lipschitz continuity, known as wGAN-GP is used for the prediction of the closing price of a stock. wGANs improve upon traditional gans with the use of the Wasserstein distance as the loss function, promoting stability in model training.\nGANs are composed of two main components: the generator and the discriminator. The generator will be a traditional LSTM (long short-term memory) network with input units equal to the number of features in the final dataset and 512 hidden units. Finally, there will be one linear layer with a single output detailing the closing price for each day.\nThe discriminator is a CNN (convolutional neural network), chosen for its ability to extract complex patterns and trends from the dataset. The architecture is as follows, where \\(RWS\\) denotes the rolling-window size:\n\n1-Dimensional Convolutional Layer: \\(RWS+1 \\rightarrow 32\\)\n1-Dimensional Convolutional Layer: \\(32 \\rightarrow 64\\)\nLeakyReLU Layer\n1-Dimensional Convolutional Layer: \\(64 \\rightarrow 128\\)\nLeakyReLU Layer\n1-Dimensional Convolutional Layer: \\(128 \\rightarrow 256\\)\nLeakyReLU Layer\nLinear Layer: \\(256 \\rightarrow 256\\)\nLeakyReLU Layer\nLinear Layer: \\(256 \\rightarrow 256\\)\nActivation Layer\nLinear Layer: \\(256 \\rightarrow 1\\)\n\nThis model was tuned with a predefined grid search. Initially, a large set of possible parameters was tested on a small subset of the full dataset. For each incremental step afterwards, the ranges of the hyperparameters were decreased and tested with a larger subset of the dataset. In the final iteration, the model was evaluated on the full dataset."
  },
  {
    "objectID": "projects/wGAN-stock-predictions/index.html#dataset-exploration",
    "href": "projects/wGAN-stock-predictions/index.html#dataset-exploration",
    "title": "Deep Learning Equity Trading Model",
    "section": "Dataset Exploration",
    "text": "Dataset Exploration\n\nTechnical Analysis\nThe primary collection of technical data was derived from the TDAmeritrade API. With the Price History tool, TDAmeritrade would generate a JSON file of the price history for a given symbol over a specified period and frequency. For this project, we collected the daily price history over the last 10 years for our primary stock ($AAPL), comparative assets ($MSFT, $META, $AMZN, and $GOOGL), and an industry index ($SP500). This offered the Date, Open, Close, High, Low, and Volume for each security. From here, a series of technical indicators typically used by day and swing traders were calculated for $AAPL with the existing data. This included metrics such as: Stochastic Oscillators, Relative Strength Index, Simple Moving Averages for Close and Volume, and Moving Average Convergence/Divergence.\n\n\nFundamental Analysis\nThe fundamental analysis portion of this project was primarily based on analyzing the public sentiment of the Apple stock. The initial source of the data was Reddit. Using the PRAW Python library, a Reddit webscraper was built, which parsed the top 10 newest posts of the r/apple subreddit, and gathered the top 50 comments and up to 25 replies per comment. The text content of each post, comment, and reply were collected and then cleaned using the NLTK library. Their sentiment was determined using the VADER model. While this script was fully implemented, there was no way to gather data from previous dates, and therefore was not integrated within the pipeline. The second source of sentiment data was Twitter. Using the Scweet Python library, a script was written which collected up to 100 tweets per day and up to 10 replies per tweet. The only tweets gathered were those that had the keywords “$AAPL” or “AAPL”. The data was collected from January 1, 2013 to March 30, 2023. The same process was used to clean and calculate the sentiment of the tweet text content. Once the sentiment of each day was found, the final dataset was merged with the technical indicator dataset.\nSome preliminary visualizations were created to explore the sentiment dataset.\n\nIn general, the number of tweets that mention the keywrods has decreased over time. One possible explanation of this is the novelty of the stock has decreased, and the general public has placed its attention in other stocks and securities, such as cryptocurrency.\n\nOver our time frame, the sentiment of the stock has remained generally positive, staying above 0.0 for the grand majority of the time. It can be stated that Twitter users discussing the stock have an overal positive view of the company.\n\nFinally, we were curious to see if there was any correlation between the sentiment of the stock and the percent change in price of a given day. The scatterplot above illustrates that their is no relationship between the two, and can be said to be independent of each other."
  },
  {
    "objectID": "projects/wGAN-stock-predictions/index.html#prediction-results",
    "href": "projects/wGAN-stock-predictions/index.html#prediction-results",
    "title": "Deep Learning Equity Trading Model",
    "section": "Prediction Results",
    "text": "Prediction Results\n\nRolling-windows and Sentiment Analysis\nDepicted in the tables below are evaluation metrics of the returned prediction sets for running the wGAN-GP model with 3-day, 5-day, 7-day, and 10-day rolling windows. One table details the performance of the models with the inclusion of sentiment analysis as a feature, and the other without. The evaluation metrics included are: root-mean-square error (RMSE), normalized root-mean-square error (NRMSE), mean-absolute error (MAE), and mean-absolute-percentage error (MAPE). Note that the RMSE values are normalized to the range of the actual closing price range to gain the NRMSE values.\n\nEvaluation Metrics (without Sentiment Scores)\n\n\n\nRolling-Window Size\nRMSE\nNRMSE\nMAE\nMAPE\n\n\n\n\n3-day\n4.616\n0.092\n3.632\n0.024\n\n\n5-day\n5.064\n0.101\n3.991\n0.027\n\n\n7-day\n5.463\n0.109\n4.348\n0.029\n\n\n10-day\n5.841\n0.117\n4.611\n0.031\n\n\n\n\n\nEvaluation Metrics (with Sentiment Scores)\n\n\n\nRolling-Window Size\nRMSE\nNRMSE\nMAE\nMAPE\n\n\n\n\n3-day\n4.401\n0.088\n3.447\n0.023\n\n\n5-day\n4.754\n0.095\n3.743\n0.025\n\n\n7-day\n5.271\n0.105\n4.233\n0.028\n\n\n10-day\n5.411\n0.108\n4.329\n0.029\n\n\n\nFrom an initial evaluation, it appears that the inclusion of daily sentiment score averages as a feature to the prediction model resulted in a slight increase in the accuracy of the predicted closing price. With some more calculations, we can quantify this increased accuracy as a percentage, seen in the table below.\n\n\nPercentage Change in Evaluation Metrics\n\n\n\nRolling-Window Size\nRMSE\nNRMSE\nMAE\nMAPE\n\n\n\n\n3-day\n-4.668\n-4.668\n-5.09\n-4.763\n\n\n5-day\n-6.117\n-6.117\n-6.205\n-6.741\n\n\n7-day\n-3.511\n-3.511\n-2.633\n-2.400\n\n\n10-day\n-7.366\n-7.366\n-6.112\n-5.627\n\n\n\nSo, with some final averaging, this gives us an average percentage change of −5.415% for RMSE and NRMSE, −5.010% for MAE, and −4.882% for MAPE when including sentiment scores as a feature in the prediction model.\nConclusively, we see around a 5% more accurate prediction when sentiment analysis is included in the prediction process. This follows with our initial assumptions that public sentiment is beneficial to accurately predicting the stock market’s daily movements. Though it is only a slight increase, it is believed that with better scraping and data-cleaning capabilities, the model could better harness public sentiment for even more accurate predictions. For instance, removing or filtering out more insignificant or misdirecting media posts could better gauge public sentiment for these purposes, likely leading to increased accuracy in the prediction model.\n\n\n\nBest Predictions\nThe best prediction set returned by the model was with a 3-day sliding window, including sentiment scored from Twitter. The model takes 3 days of basic indicators as input and outputs the predicted closing price of the 4th day. After tuning, the following hyperparameters were chosen for their performance on the validation set:\n\nBatch Size = 65\nLearning Rate = 0.00005\nCritic Training Iterations per Generator Iteration = 5\nEpochs = 150\n\nAdditionally, RMSProp, or root-mean-square propagation, was used as the optimization algorithm for the model. Visualizations of the generator and discriminator losses follow.\n\n\nThe discriminator loss appears to converge approximately to 0, while the generator loss appears to converge approximately to 5. These exact values are mostly unimportant. However, it is important to note that, for this dataset, the losses do appear to converge. In prior versions of the model, the losses did not ever converge. This improvement is thought to be due to the implementation of the gradient penalty, in order to enforce the Lipschitz contraint in the discriminator. The usage of the wGAN-GP model’s improvements to training stability and convergence is likely to credit for this result. Evaluation of the model’s stock price predictions are detailed below.\n\n\nFor the training dataset, the model returned predictions with an RMSE of 0.779. The model seems capable of identifying the movement of the stock price and accurately predicting the magnitude of said movement. The inclusion of various technical indicators and sentiment analysis scores improved this greatly from previous iterations of the model.\nRegarding the validation and testing datasets, the model returned predictions with an averaged RMSE of 4.401 or 0.088 when normalized to the range of the true closing price values. This is a bit higher than that on the training set, indicating a bit of overfitting in the model. Further tuning may be requiring to hone in on a bettergeneralized model. Despite this, the model still performs well in identifying stock price movements and even comes fairly close to the magnitude of the movement, returning an averaged MAE of 3.447 on these sets.\nTaking in the scale of the predicted values, the RMSE appears quite good. Conclusively, the model achieved predictions within an error of about $3.50 or 2.3% off of the actual stock price on average. Although not entirely useful for day trading, this model could prove useful in longer-termed swing trades."
  },
  {
    "objectID": "projects/post-with-code/index.html",
    "href": "projects/post-with-code/index.html",
    "title": "Post With Code",
    "section": "",
    "text": "This is a post with executable code. (Python Added)\n1 + 1\n\n[1] 2"
  },
  {
    "objectID": "projects/post-with-code/index.html#section-1",
    "href": "projects/post-with-code/index.html#section-1",
    "title": "Post With Code",
    "section": "1.1",
    "text": "1.1\nLorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ac auctor augue mauris augue. Velit euismod in pellentesque massa placerat duis ultricies lacus sed. Egestas sed sed risus pretium quam. Morbi tristique senectus et netus. Cursus metus aliquam eleifend mi in. Justo donec enim diam vulputate. Adipiscing elit duis tristique sollicitudin nibh sit amet commodo. In fermentum et sollicitudin ac orci phasellus. Ut morbi tincidunt augue interdum. Nisl vel pretium lectus quam id leo in. Tincidunt dui ut ornare lectus sit amet est placerat in. Cursus sit amet dictum sit amet justo. Egestas integer eget aliquet nibh praesent.\nTempor id eu nisl nunc mi ipsum faucibus vitae aliquet. Eu tincidunt tortor aliquam nulla facilisi cras fermentum odio eu. Convallis aenean et tortor at risus viverra adipiscing. Fringilla est ullamcorper eget nulla facilisi. Porta non pulvinar neque laoreet suspendisse interdum consectetur libero. In mollis nunc sed id semper risus in hendrerit gravida. Quam id leo in vitae turpis massa sed elementum. Nullam non nisi est sit amet facilisis. Porttitor rhoncus dolor purus non enim praesent elementum facilisis leo. Imperdiet dui accumsan sit amet nulla facilisi. Ac felis donec et odio pellentesque.\n\n1.1.1\nLorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ac auctor augue mauris augue. Velit euismod in pellentesque massa placerat duis ultricies lacus sed. Egestas sed sed risus pretium quam. Morbi tristique senectus et netus. Cursus metus aliquam eleifend mi in. Justo donec enim diam vulputate. Adipiscing elit duis tristique sollicitudin nibh sit amet commodo. In fermentum et sollicitudin ac orci phasellus. Ut morbi tincidunt augue interdum. Nisl vel pretium lectus quam id leo in. Tincidunt dui ut ornare lectus sit amet est placerat in. Cursus sit amet dictum sit amet justo. Egestas integer eget aliquet nibh praesent.\nTempor id eu nisl nunc mi ipsum faucibus vitae aliquet. Eu tincidunt tortor aliquam nulla facilisi cras fermentum odio eu. Convallis aenean et tortor at risus viverra adipiscing. Fringilla est ullamcorper eget nulla facilisi. Porta non pulvinar neque laoreet suspendisse interdum consectetur libero. In mollis nunc sed id semper risus in hendrerit gravida. Quam id leo in vitae turpis massa sed elementum. Nullam non nisi est sit amet facilisis. Porttitor rhoncus dolor purus non enim praesent elementum facilisis leo. Imperdiet dui accumsan sit amet nulla facilisi. Ac felis donec et odio pellentesque."
  },
  {
    "objectID": "projects/post-with-code/index.html#section-4",
    "href": "projects/post-with-code/index.html#section-4",
    "title": "Post With Code",
    "section": "2.1",
    "text": "2.1\nLorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ac auctor augue mauris augue. Velit euismod in pellentesque massa placerat duis ultricies lacus sed. Egestas sed sed risus pretium quam. Morbi tristique senectus et netus. Cursus metus aliquam eleifend mi in. Justo donec enim diam vulputate. Adipiscing elit duis tristique sollicitudin nibh sit amet commodo. In fermentum et sollicitudin ac orci phasellus. Ut morbi tincidunt augue interdum. Nisl vel pretium lectus quam id leo in. Tincidunt dui ut ornare lectus sit amet est placerat in. Cursus sit amet dictum sit amet justo. Egestas integer eget aliquet nibh praesent.\nTempor id eu nisl nunc mi ipsum faucibus vitae aliquet. Eu tincidunt tortor aliquam nulla facilisi cras fermentum odio eu. Convallis aenean et tortor at risus viverra adipiscing. Fringilla est ullamcorper eget nulla facilisi. Porta non pulvinar neque laoreet suspendisse interdum consectetur libero. In mollis nunc sed id semper risus in hendrerit gravida. Quam id leo in vitae turpis massa sed elementum. Nullam non nisi est sit amet facilisis. Porttitor rhoncus dolor purus non enim praesent elementum facilisis leo. Imperdiet dui accumsan sit amet nulla facilisi. Ac felis donec et odio pellentesque.Lorem ipsum dolor sit amet, consectetur adipiscing elit, sed do eiusmod tempor incididunt ut labore et dolore magna aliqua. Ac auctor augue mauris augue. Velit euismod in pellentesque massa placerat duis ultricies lacus sed. Egestas sed sed risus pretium quam. Morbi tristique senectus et netus. Cursus metus aliquam eleifend mi in. Justo donec enim diam vulputate. Adipiscing elit duis tristique sollicitudin nibh sit amet commodo. In fermentum et sollicitudin ac orci phasellus. Ut morbi tincidunt augue interdum. Nisl vel pretium lectus quam id leo in. Tincidunt dui ut ornare lectus sit amet est placerat in. Cursus sit amet dictum sit amet justo. Egestas integer eget aliquet nibh praesent.\nTempor id eu nisl nunc mi ipsum faucibus vitae aliquet. Eu tincidunt tortor aliquam nulla facilisi cras fermentum odio eu. Convallis aenean et tortor at risus viverra adipiscing. Fringilla est ullamcorper eget nulla facilisi. Porta non pulvinar neque laoreet suspendisse interdum consectetur libero. In mollis nunc sed id semper risus in hendrerit gravida. Quam id leo in vitae turpis massa sed elementum. Nullam non nisi est sit amet facilisis. Porttitor rhoncus dolor purus non enim praesent elementum facilisis leo. Imperdiet dui accumsan sit amet nulla facilisi. Ac felis donec et odio pellentesque."
  },
  {
    "objectID": "projects/OT-survey/index.html",
    "href": "projects/OT-survey/index.html",
    "title": "What is the Optimal Transport Problem?",
    "section": "",
    "text": "What is the Optimal Transport Problem?\n\nFirst, let us begin with a general, or low-level, definition of the optimal transport problem. The most common explanation for this problem is as follows: Assume that you have dug a hole in the ground of \\(X\\) cubic feet. And you now also have a pile of dirt with an exact volume of \\(X\\) cubic feet. Now, consider a naïve method of filling the hole with all of the dirt you have accumulated in the pile. In order to fill this hole, you would have to move every individual dirt particle a certain distance from some location in the pile to some location in the hole. Once all particles have been moved, you will have filled your hole.\nSo, how would we define the optimality of our solution? To understand optimality in this case, we need to have some notion of cost for our solution. For this simple case, we can define cost quite simply. Let the cost of moving a single dirt particle be the distance this particle moves when it is picked up from the dirt pile and placed in the hole. We can quantify this as the euclidean distance from any given particle’s source location to its destination. So now, with our definition of cost, an optimal method of transporting all particles of dirt from the pile to the hole will be the one which minimizes the total cost incurred by transporting all particles. Thus, to have optimal transport, we have to minimize the total distance traveled by all particles in the set when filling the hole.\nThis is the most basic version of the optimal transport problem. It is essentially an extension of finding a minimum cost matching. Consider this problem as matching every source location of a dirt particle to some destination location in the most cost-optimal manner. Once we have found the minimum cost matching between the sets of source and destination locations, we have found the optimal transport plan.\nNow, some may ask, why do we even worry about such a problem? In another post, I go further in-depth on some possible applications of optimal transport theory. But before I do, here is the gist of it. In mathematics, more specifically statistics, computing the distance between probability distributions intuitively has been a large obstacle. Distance in the statistic sense can take very different forms. One of the most popular definitions of a statistical distance is the Kullback-Leibler divergence. Although the most popular, this divergence is not the most desirable. This divergence is not symmetric and may even result in an infinite divergence in some cases. Therefore, the KL divergence is quite undesirable and unintuitive. With optimal transport theory, researchers look to develop a more intuitive and desirable form of statistical distance. That is why this problem is of such importance.\nFinally, let us wrap up by extending the dirt pile and hole example to probability distributions. What would be the optimal transport plan of transforming a probability distribution \\(A\\) to probability distribution \\(B\\)? Conceptualize \\(A\\) as a 2-dimensional pile of dirt, the shape of which resembles the shape of the respective probability distribution. Additionally, conceptualize \\(B\\) as a 2-dimensional hole, the shape of which resembles the shape of the respective probability distribution. Now, we can “transform” distribution \\(A\\) into distribution \\(B\\) by simply placing the “dirt” from \\(A\\) to the hole from \\(B\\). The cost can be defined the same way as in our dirt pile example. When the total cost of transport is at its minimum, we have found the optimal transport plan. Intuitively, we can see that these are similar problems. Note that, if the two distributions are discrete, this problem can be solved in a manner similar to that mentioned above. However, in the case that the two distributions are continuous, the procedure is slightly different. In this case, we would have to compute the optimal transport between samples of the distributions. For \\(n\\) samples, each sample would be assigned a weight of \\(1/n\\), and then the optimal transport between the samples would be computed. When all sample points are equally weighted, the problem is known as the assignment problem.\n\n\nApproximate Optimal Transport Problems\nOptimal transport can be exactly computed, however, computing an exact solution often takes too much time for any practical usages. Because of this, the development of algorithms which compute exact optimal transport has slowed, and in turn we have seen a rise in popularity of approximation algorithms. These approximation algorithms aim to produce a near-optimal transport plan, usually within some predefined error. For some constant \\(\\varepsilon &gt; 0\\), an \\(\\varepsilon\\)-approximate transport plan is one where the total cost of the transport plan is within \\(\\varepsilon n\\) of the optimum. And often, algorithms which compute exact optimal transport are adapted to more-quickly compute an approximate transport plan. With a much faster execution, these approximation algorithms are more fit for practical usages in the real world."
  },
  {
    "objectID": "projects/OT-graphTheoretical/index.html",
    "href": "projects/OT-graphTheoretical/index.html",
    "title": "Analysis of A Graph Theoretic Additive Approximation of Optimal Transport",
    "section": "",
    "text": "In this post, I describe and analyze a graph theoretic additive approximation algorithm for approximating Optimal Transport. Nathaniel Lahn, Deepika Mulchandani, and Sharath Raghvendra (my former theory and algorithms professor) submitted this algorithm to the arXiv repository originally on May 28, 2019.\nPlease see the source below for the original paper on this algorithm.\n[A Graph Theoretic Additive Approximation of Optimal Transport]"
  },
  {
    "objectID": "projects/OT-graphTheoretical/index.html#description",
    "href": "projects/OT-graphTheoretical/index.html#description",
    "title": "Analysis of A Graph Theoretic Additive Approximation of Optimal Transport",
    "section": "Description",
    "text": "Description\nThis algorithm applies the traditional framework of augmenting paths for computing an approximate solution to the minimum-cost maximum flow problem to a transport problem. However, in order to this, the problem must only contain integer supplies and demands. So, in order to utilize this algorithm, the input problem must first be transformed to contain only integer demands and supplies. The process of transforming the given problem’s demand and supply values follows:\n\nTransforming Demand and Supply Values\nInitially, let \\(\\varepsilon\\) be some constant value such that \\(0 &lt; \\varepsilon &lt; 1\\), and set \\(\\alpha = \\frac{2nC}{\\varepsilon U \\delta}\\). Here, the variables are defined as follows:\n\n\\(n\\) is the total number of nodes.\n\\(C\\) is the largest cost of any edge in the graph.\n\\(U\\) is the total supply.\n\\(\\delta\\) is the additive error.\n\nNow, let \\(\\mathcal{I}\\) be the input for the transportation problem such that each demand node \\(a \\in A\\) has a demand of \\(d_a\\) and each supply node \\(b \\in B\\) has a supply of \\(s_b\\). We will denote the integer-scaled demand for some node \\(a \\in A\\) as \\(\\overline{d}_a\\). Similarly, the integer-scaled supply for some node \\(b \\in B\\) is denoted as \\(\\overline{s}_b\\). The process to scale the demand and supply values at each node is detailed next. For all demand nodes \\(a \\in A\\), \\(\\overline{d}_a = \\lceil d_a\\alpha\\rceil\\), and for all supply nodes \\(b \\in B\\), \\(\\overline{s}_b = \\lfloor s_b\\alpha \\rfloor\\). Let \\(\\mathcal{I'}\\) denote this scaled input. Next, the solution to the scaled transport problem must be mapped to a feasible solution for the original demand and supply values. Let \\(\\sigma\\) denote any feasible maximum transport plan for the input \\(\\mathcal{I'}\\). A transport plan \\(\\sigma\\) which sets, for each edge \\((a,b),\\sigma(a,b) = \\sigma'(a,b)/\\alpha\\), is not necessarily a feasible or a maximum solution. \\(\\sigma\\) may not be a maximum solution, as there may be an excess supply remaining at a supply node. Similarly, \\(\\sigma\\) is not feasible as there may be excess supply that reaches a demand node. In order to convert \\(\\sigma\\) to a feasible and maximum solution, there are two steps:\n\nFirst, we must convert \\(\\sigma\\) to a feasible solution. To do this, we can iteratively remove excess supply at demand nodes. For instance, let \\(\\mathcal{k}_a\\) denote the excess supply that reaches a demand node \\(a \\in A\\). Now, iteratively select an arbitrary edge incident on \\(a\\). Let \\((a,b)\\) denote this edge. Now, reduce \\(\\sigma(a,b)\\) and \\(\\mathcal{k}_a\\) by \\(min{\\mathcal{k}_a, \\sigma(a,b)}\\). Continue this until \\(\\mathcal{k}_a\\) is reduced to \\(0\\). We can then repeat these steps for all demand nodes \\(a \\in A\\) such that \\(\\mathcal{k}_a &gt; 0\\). After this, \\(\\sigma\\) will have been converted to a feasible solution. At this point the total remaining supply in \\(\\sigma\\) is at most \\(2n/\\alpha\\).\nSecond, we must convert the newly feasible solution, \\(\\sigma\\), to a maximum transport plan. To do this, we just match the \\(2n/\\alpha\\) supplies arbitrarily to any leftover demands. This will incur a cost of at most \\(C\\) per supply unit.\n\nFinally, with a properly scaled input, we can now apply the algorithm.\n\n\nAlgorithm for Scaled Demands and Supplies:\nFirst, initialize \\(\\sigma\\) as a transport plan, such that, for every edge \\((a,b) \\in A \\times B\\), \\(\\sigma(a,b) = 0\\). Now, let the dual weight of some vertex \\(v\\) be denoted as \\(y(v)\\). Set \\(y(v) = 0\\) for all vertices \\(v \\in A \\cup B\\). This algorithm executes in phases, wherein each phase is two steps. Execution will terminate once \\(\\sigma\\) becomes a maximum transport plan. The two steps are detailed below:\n\nHungarian Search:\nIn the first step, we will conduct a Hungarian Search in order to compute at least one augmenting path of admissible edges. Firstly, we will add two arbitrary nodes, \\(s\\) and \\(t\\), to the residual network. Here, \\(s\\) represents a source node and \\(t\\) represents a sink node for the single-source, single-sink maximum flow problem. We attach edges from \\(s\\) to every free supply node and edges from every free demand node to \\(t\\). All edges incident on \\(s\\) or \\(t\\) are given a weight of \\(0\\). The weight of all other edges in the residual network are set the the slack of the respective edge based on the edge’s direction. Let \\(\\mathcal{G}_\\sigma\\) denote the augmented residual network with the attached source and sink vertices. Now, execute Dijkstra’s algorithm from \\(s\\) in the augmented residual network \\(\\mathcal{G}_\\sigma\\). Let \\(\\ell_v\\) be the shortest path in \\(\\mathcal{G}_\\sigma\\) from \\(s\\) to \\(v\\) for any node \\(v \\in A \\cup B\\). Now, we must update the dual weights in the network to maintain feasibility. For any vertex \\(v \\in A \\cup B\\), we will do one of the following:\n\nIf \\(\\ell_v \\geq \\ell_t\\), the dual weight is not updated.\nOtherwise, the dual weight is updated. Now, if \\(v \\in A\\), \\(y(v) \\leftarrow y(v) - \\ell_t + \\ell_v\\). Else, if \\(v \\in B\\), \\(y(v) \\leftarrow y(v) + \\ell_t - \\ell_v\\).\n\nOnce this step reaches completion, \\(\\sigma\\) remains feasible and the admissible graph contains at least one augmenting path.\nPartial DFS:\nIn the second step, we will compute at least one augmenting path and update \\(\\sigma\\) by augmenting it along every computed path. First, let \\(\\mathcal{A}\\) denote the admissible graph. Now, let \\(X\\) denote the set of free supply nodes in the admissible graph \\(\\mathcal{A}\\). A DFS will be iteratively run from every supply node in set \\(X\\). Let \\(b \\in X\\) be the supply node for the current iteration. The steps of the partial DFS from node \\(b\\) are as follows:\n\nBegin a DFS from node \\(b\\).\nDuring the execution, if a free demand node is visited, then an augmenting path is found, and the DFS is terminated. Let \\(P\\) denote this path. Once the DFS is terminated, all edges visited by the DFS, except for the edges of path \\(P\\), are removed. Then, we will augment \\(\\sigma\\) along path \\(P\\) and updates set \\(X\\) to contain only the set of free supply nodes remaining in admissible graph \\(\\mathcal{A}\\).\nIf no augmenting path is found, all vertices and edges visited by the DFS are removed from admissible graph \\(\\mathcal{A}\\), and set \\(X\\) is updated to contain only the set of free supply nodes remaining in \\(\\mathcal{A}\\).\n\nOnce this step reaches completion, set \\(X\\) will be empty.\n\nFinally, at the end of execution, \\(\\sigma\\) is a maximum transport plan for the given transport problem."
  },
  {
    "objectID": "projects/OT-graphTheoretical/index.html#correctness",
    "href": "projects/OT-graphTheoretical/index.html#correctness",
    "title": "Analysis of A Graph Theoretic Additive Approximation of Optimal Transport",
    "section": "Correctness",
    "text": "Correctness\nTo begin assessment of this algorithm’s correctness, we must first prove the following invariant:\nIn each phase of the algorithm, the partial DFS step computes at least one augmenting path. Once the partial DFS step terminates, there is no augmenting path in the admissible graph.\nThe proof is as follows:\nConsider the shortest path from source node \\(s\\) to sink node \\(t\\) in the augmented residual network. Let \\(P'\\) denote this path. Now, let \\(b\\) be the free supply node after \\(s\\) and let \\(a\\) be the free demand node before \\(t\\) along path \\(P'\\). Let \\(P\\) denote the path from node \\(b\\) to node \\(a\\).\nFirst, we must show that path \\(P\\) is an admissible augmenting path after the dual updates conducted by the Hungarian search. Note that, by construction, for any edge \\((u,v)\\) in \\(P\\), \\(\\ell_u \\leq \\ell_t\\) and \\(\\ell_v \\leq \\ell_t\\).\nLet \\(\\tilde{y}(.)\\) denote the updated dual weight for some vertex. Now, the updated dual weights for vertices \\(u\\) and \\(v\\) become:\n\n\\(\\tilde{y}(u) = y(u) + \\ell_t - \\ell_u\\) and \\(\\tilde{y}(v) = y(v) - \\ell_t + \\ell_u\\) for a forward edge, or\n\\(\\tilde{y}(u) = y(u) - \\ell_t + \\ell_u\\) and \\(\\tilde{y}(v) = y(v) + \\ell_t - \\ell_u\\) for a backward edge.\n\nNow, the updated feasibility condition becomes:\n\n\\(\\tilde{y}(u) + \\tilde{y}(v) = y(u) + y(v) + \\ell_v - \\ell_u\\) for a forward edge, or\n\\(\\tilde{y}(u) + \\tilde{y}(v) = y(u) + y(v) - \\ell_v + \\ell_u\\) for a backward edge.\n\nNote that all edges in the shortest path \\(P\\) satisfy the condition that for any directed edge \\((u,v)\\), \\(\\ell_u + s(u,v) \\geq \\ell_v\\) where \\(s(u,v)\\) is the slack of the respective edge. Because this condition holds with equality,\n\n\\(\\tilde{y}(u) + \\tilde{y}(v) = y(v) + y(v) + s(u,v) = \\overline{c}(u,v) + 1\\) if \\((u,v)\\) is a forward edge, or\n\\(\\tilde{y}(u) + \\tilde{y}(v) = y(v) + y(v) - s(u,v) = \\overline{c}(u,v)\\) if \\((u,v)\\) is a backward edge.\n\nIn other words, this means that path \\(P\\) is an admissible augmenting path.\nFinally, since the partial DFS initiates from every free supply vertex, including node \\(b\\), we will discover at least one augmenting path in the admissible graph.\nAfter this, we must next show that once the partial DFS step terminates, there is no augmenting path in the admissible graph. During this step’s execution, graph \\(\\mathcal{A}\\), initialized to the admissible graph, is maintained. For instance, after each execution of DFS, the edges visited by DFS are removed from graph \\(\\mathcal{A}\\) unless they are on the augmenting path \\(P\\). If no augmenting path is found, all edges visited by the DFS are removed from graph \\(\\mathcal{A}\\). The termination condition of this step ensures that graph \\(\\mathcal{A}\\) does not have any free supply vertices remaining at completion.\nHere, it is important to note that, every vertex removed from graph \\(\\mathcal{A}\\) is a vertex in which the DFS backtracked. Since it can be shown that there is no directed cycle consisting of admissible cycles, there is no path of admissible edges from any vertex which had been removed from graph \\(\\mathcal{A}\\) to a free demand node.\nNow, as the partial DFS step ensures that all free supply vertices are deleted from graph \\(\\mathcal{A}\\) on completion, there cannot be admissible paths from any free supply vertex to a free demand vertex in the admissible graph. Now that we have proved this invariant, we can proceed to proving the correctness.\nAs the invariant above declares, in each phase, the algorithm will augment the transport plan by at least one unit of supply. So, it follows that, on termination of the algorithm, we will have computed a feasible maximum transport plan.\nFirstly, we must show that all transport plans maintained by the algorithm will satisfy the following condition:\nConsidering a feasible maximum transport plan such that for every demand node \\(a \\in A\\), the dual weight \\(y(a)\\leq0\\), and \\(y(a) = 0\\) if node \\(a\\) is free.\nDenote this condition as (C).\nInitially, for any node \\(v\\) such that \\(v \\in A\\), dual weights are set to \\(0\\). Within any phase, now suppose that \\(\\ell_v &lt; \\ell_t\\). Because of this, the dual weight of \\(v\\) will be reduced when the Hungarian Search step updates dual weights.\nSo, since \\(y(v)\\) was initially set to \\(0\\), after the update, \\(y(v)\\leq0\\).\nNext up, we must show that all free vertices of \\(A\\) have a dual weight of \\(0\\), as specified by the aforementioned condition. As dual weights are initialized to \\(0\\), this claim is true initially. During execution of the algorithm, any vertex \\(a \\in A\\) whose demand has been met cannot become free through the remaining execution of the algorithm. So, we can argue that no free demand vertex will have its dual weight updated.\nNow, note that, by construction, any directed edges to the sink node \\(t\\) from a demand node in \\(A\\) have zero cost in \\(\\mathcal{G}_\\sigma\\). Thus, it follows that there is a directed edge \\((v,t)\\) with a cost of zero. So, \\(\\ell_t \\leq \\ell_v\\), resulting in the dual weight \\(y(v)\\) not being updated during the phase. Therefore, the algorithm maintains \\(y(v) = 0\\) for every free demand node, and it is proved that the condition (C) is upheld. And so, finally, when the algorithm completes, we will have computed a feasible maximum transport plan which satisfies condition (C)."
  },
  {
    "objectID": "projects/OT-graphTheoretical/index.html#efficiency",
    "href": "projects/OT-graphTheoretical/index.html#efficiency",
    "title": "Analysis of A Graph Theoretic Additive Approximation of Optimal Transport",
    "section": "Efficiency",
    "text": "Efficiency\nBefore beginning an assessment of this algorithm’s efficiency, we must provide some lemmas. Also, at any stage in the algorithm, let \\(B'\\) denote the set of free supply nodes and let \\(A_F\\) denote the set of free demand nodes. The lemmas follow:\nLemma 2.2: The dual weight of any free supply node \\(v \\in B'\\) is at most \\(\\lfloor 2C/\\delta' \\rfloor + 1\\)\n\nProof. Suppose, for the sake of contradiction, that the free supply node \\(b \\in B\\) has a dual weight \\(y(b) \\geq \\lfloor 2C/\\delta' \\rfloor + 2\\). Now, consider the condition (C). Due to this condition, any free demand node \\(a \\in A_f\\) has a dual weight \\(y(a)=0\\). So, \\(y(a) + y(b) \\geq \\lfloor 2C/\\delta' \\rfloor + 2 \\geq \\overline{c}(a,b) + 2\\). However, this violates feasibility, as \\(y(a) + y(b) \\leq \\overline{c}(a,b) + 1\\) is not upheld. Thus, a contradiction arises. ◻\n\nLemma 2.3: The total number of phases in our algorithm is at most \\(\\lfloor 2C/\\delta' \\rfloor + 1\\).\nNote: Here, \\(C\\) is the largest value in the cost matrix, \\(\\delta\\) is the additive error, and \\(\\delta' = (1-\\varepsilon)\\delta\\) where \\(\\varepsilon\\) is some constant such that \\(0 &lt; \\varepsilon &lt; 1\\).\n\nProof. From the invariant proved on the previous page, we can conclude that at the start of a phase, there are no admissible augmenting paths. Because of this, any path from source node \\(s\\) to sink node \\(t\\) in the augmented residual network \\(\\mathcal{G}_\\sigma\\) will have a cost of at least 1. In other words \\(\\ell_t \\geq 1\\). Now, during any phase, let \\(b \\in B'\\) be any free supply node. It is important to note that node \\(b\\) is also a free supply node in all previous phases. By construction, we may conclude that there exists a directed edge from \\(s\\) to \\(b\\) with a cost of \\(0\\) in \\(\\mathcal{A}\\). Therefore, \\(\\ell_b = 0\\). Finally, since we now know that \\(\\ell_t \\geq 1\\), when updating the dual weight of node \\(b\\) for this phase, the dual weight will be increased by at least \\(1\\). After \\(\\lfloor 2C/\\delta' \\rfloor + 2\\) phases, due to the aforementioned increase of at least \\(1\\), \\(y(b) \\geq \\lfloor 2C/\\delta' \\rfloor + 2\\), which contradicts Lemma 2.2 because \\(\\lfloor 2C/\\delta' \\rfloor + 2 &gt; \\lfloor 2C/\\delta' \\rfloor + 1\\). ◻\n\nLemma 2.4: Let \\(\\mathbb{P}\\) be the set of all augmenting paths produced by the algorithm. Then, \\(\\sum_{P\\in\\mathbb{P}} |P| = \\mathcal{O}(\\frac{nC^2}{\\varepsilon(1-\\varepsilon)\\delta^2})\\).\nNote: Here, \\(C\\) is the largest value in the cost matrix, and \\(\\delta\\) is the additive error. Additionally, \\(|P|\\) is the number of edges on path \\(P\\), and \\(\\varepsilon\\) is some constant such that \\(0 &lt; \\varepsilon &lt; 1\\).\nNow that the lemmas are out of the way, let us begin the assessment of this algorithm’s efficiency. First, Let \\(\\mathbb{P}_j\\) denote the set of all augmenting paths computed in phase \\(j\\) of execution. Also, let \\(\\mathbb{P}\\) denote the set of all augmenting paths produced by the algorithm across all phases.\nConsider Lemma 2.3. As described in the lemma, the total number of phases executed by the algorithm is bounded above by \\(\\lfloor 2C/\\delta' \\rfloor + 1\\). So, the total number of phases is \\(\\mathcal{O}(C/\\delta')\\). Now, each phase consists of two steps: the Hungarian Search step and the partial DFS step.\nIn the Hungarian Search step, a single Dijkstra’s search is executed, taking \\(\\mathcal{O}(n^2)\\) time. The dual weight updates are insignificant to the complexity of this step.\nRegarding the partial DFS step, note that during execution, any edge visited by the DFS is removed as long as it does not contribute to an augmenting path. Inversely, edges which do contribute to an augmenting path may be visited multiple times within the same phase. So, the time taken by the partial DFS step during phase \\(j\\) becomes \\(\\mathcal{O}(n^2 + \\sum_{P \\in \\mathbb{P}_j} |P|)\\), where \\(|P|\\) is the number of edges along the augmenting path \\(P\\). Thus, since we know that the number of total phases is \\(\\mathcal{O}(C/\\delta')\\), the total time taken by the algorithm across all phases is \\(\\mathcal{O}((C/\\delta')n^2 + \\sum_{P \\in \\mathbb{P}} |P|)\\). Considering Lemma 2.4, wherein \\(\\sum_{P \\in \\mathbb{P}} |P| = \\mathcal{O}(\\frac{nC^2}{\\varepsilon(1-\\varepsilon)\\delta^2})\\), the total execution time of the algorithm is \\(\\mathcal{O}(\\frac{n^2C}{(1-\\varepsilon)\\delta} + \\frac{nC^2}{\\varepsilon(1-\\varepsilon)\\delta^2})\\)."
  },
  {
    "objectID": "projects/governmentSurveillance/index.html#predictive-policing",
    "href": "projects/governmentSurveillance/index.html#predictive-policing",
    "title": "Ethics of Intelligent Government Surveillance Systems",
    "section": "Predictive Policing",
    "text": "Predictive Policing\nThe rise of big data systems and machine learning has led to an increase in the use of predictive policing techniques [10]. Despite its growing popularity, predictive policing has yielded mixed results due to several challenges, such as the loss of context in predictive analysis, the lack of transparency in the models used, and ethical concerns regarding privacy and data collection [14].\nTwo well-known predictive policing programs, LASER and PredPol, serve as examples. While LASER aimed to identify areas with a high likelihood of gun violence [6], PredPol focused on calculating hotspots for property-related crimes. LASER was shut down in 2019 due to significant consistency issues discovered through an internal audit. A common issue with these programs is the lack of transparency; many police departments do not disclose the information or technology used to generate their predictive policing systems.\nCritics argue that predictive policing could threaten rights protected by the Fourth Amendment, which requires “reasonable suspicion” for law enforcement actions [5]. However, proponents contend that predictive tools make it easier for police to identify reasonably suspicious individuals [8]. Another concern is the phenomenon of “tech-washing,” where racially biased policing methods are given the appearance of objectivity due to the use of computer algorithms. This is especially problematic when algorithms trained on historical data perpetuate existing biases. The ultimate goal of predictive policing is to transition law enforcement from a reactive to a proactive stance, emphasizing crime prevention rather than merely responding to incidents [7]."
  },
  {
    "objectID": "projects/governmentSurveillance/index.html#social-credit-systems",
    "href": "projects/governmentSurveillance/index.html#social-credit-systems",
    "title": "Ethics of Intelligent Government Surveillance Systems",
    "section": "Social Credit Systems",
    "text": "Social Credit Systems\nChina’s Social Credit System (SCS) is a prime example of the ethical challenges associated with Social Credit Systems. The SCS monitors the behavior of all citizens and enterprises, assigning social credit scores based on their actions. Individuals who violate laws are placed on an online “blacklist,” while trustworthy individuals are placed on a “red list.” Both lists are publicly accessible, and various restrictions can be imposed based on social credit scores, such as financial or legal penalties for failing to pay taxes or attend court summons [3].\nThe SCS has a psychological basis; by displaying rule breakers online, it holds them accountable, while those on the red list serve as models for others. However, the system raises concerns about the potential erosion of individual free will, which could lead to citizens feeling constrained or controlled. This, in turn, may result in growing unease and resentment, potentially culminating in civil unrest or even revolution."
  },
  {
    "objectID": "projects/governmentSurveillance/index.html#pasco-country-sheriff-department-florida",
    "href": "projects/governmentSurveillance/index.html#pasco-country-sheriff-department-florida",
    "title": "Ethics of Intelligent Government Surveillance Systems",
    "section": "Pasco Country Sheriff Department (Florida)",
    "text": "Pasco Country Sheriff Department (Florida)\n\nDescription\nThe Pasco County Sheriff Department instituted The Intelligence-Led Policing Section (ILP) on June 14, 2011. Initially established as a group to “advance the agency’s crime fighting initiatives to a modern-day philosophy”, after much controversy the program was discontinued sometime between 2021 and 2022 [13].\nThis program implemented a predictive policing system which attempted to catch potential criminal offenders before they would, supposedly, commit a crime. Under this program, the potential offenders were denoted as prolific offenders. The Sheriff Department maintained a list of said prolific offenders that they deemed were likely to commit crimes in the future. According to a “crude computer algorithm”, members of the community would be placed on this list based on factors including a person’s criminal record, whether they have been suspected of a crime, whether they have witnessed a crime, or even whether they were a victim of a crime [9].\nIn addition to the aforementioned prolific offenders list, an investigation by the Tampa Bay Times has also revealed the use and maintenance of an at-risk students list by the Pasco Country Sheriff Department [9]. According to the agency’s internal intelligence manual, factors such as grade point average, attendance, involvement in a custody dispute, or even, again, being a victim of a personal crime, all played a role in students being placed on said list. The school districts provide much of this sensitive and private data to the Sheriff Department with full access, raising legal concerns of data privacy. The Pasco Sheriff Office was no longer allowed to access student data post May 3, 2021.\n\n\nCommunity Impact\nOnce placed onto the prolific offenders list, people were subject to intimidation and harassment by officers of the police department instructed to monitor and gather information on their subjects. Officers would show up unannounced to homes to even interrogate friends and families of listed peoples, unnecessarily harassing innocent civilians in the process. When met with uncooperative subjects, officers would also cite them egregious amounts, typically in the thousands, for frivolous property code violations such as having tall grass or missing house numbers [15]. A former Pasco County deputy described that the department had ordered them to “make their lives miserable until they move or sue” [15]. Community members are being forced out of their homes by constant fear of harassment from the police department. This is a conscious action by the department, suspected to be in hopes of lowering crime rate in the area. And as seen above, not only are the listed peoples subject to this, but innocent relatives and friends too.\nRegarding the at-risk students list, in the summer following the initial implementation of this list, school resource officers were reported to make “hundreds of home visits to at-risk kids”, offering support to children and families, but also questioning them of local crimes and even making arrests of kids who had violated probation or curfew orders [2]. The intelligence manual also reportedly encouraged officers to work relationships with to students to discover “seeds of criminal activity” and collect information accordingly. Unfortunately, since the students are unable to know whether or not they have been flagged as at-risk it is difficult to fully evaluate the effects of such a system on the student population. Though, if this list is used at all similarly to the prolific offenders list, we can expect that listed students are likely suffering from similar harassment by school resource and correctional officers alike.\n\n\nEthical Implications\nAn ethical issue paramount to such a predictive policing system as described is discrimination. With a predictive policing system based on data from an already discriminatory criminal justice system, the usage of such a system only works to perpetuate the same injustices. The criminal justice system in the United States disproportionately targets those of minority ethnic groups and low income. So, the predictive policing system of Pasco County follows and targets the same marginalized groups of people. Additionally, a further ethical concern arises when even those with no criminal history, but have simply been suspected of a crime, are also subject to unwarranted harassment. By definition, this is discrimination towards those suspected of a crime, even though they may have been found innocent. With regards to the at-risk students list, discrimination towards those involved in custody disputes is also a concern, as it could be seen as discrimination based on family status [2]. Policing should be ethical and lack any forms of discrimination, and these predictive systems undermine those essential characteristics.\nIn addition to discrimination, predictive policing in the United States raises concerns of violating constitutional rights to protection against unreasonable searches and seizures, protected under the Fourth Amendment. Specifically, when Pasco County officers show up unwarranted to homes with the goals of collecting information, homeowners have the right to refuse and ask the officers to leave. When the officers continue to poke and prod them without a warrant or any reasonable suspicion, they violate the homeowner’s Fourth Amendment rights [15]. Also, when harassing family members and friends of those on the prolific offenders list, officers violate the constitutional right to freedom of association, protected by the First Amendment, as well as the right to not be punished for another’s actions, protected under the Due Process Clause of the Fourteenth Amendment [15]. As citizens of the United States, we can reasonable expect government officials to act in accordance with our constitutional rights. However, the predictive policing tactics of Pasco County do exactly the opposite, making it entirely unethical.\n\n\nUniqueness\nThe Pasco County case in predictive policing is unique in its revealed integration of school performance data and abuse history in its policing efforts towards children. According to the Pasco Sheriff’s Office Intelligence Manual, children which have recorded to have witnessed or experienced household violence of even gotten a D or an F in school are factors which make a child much more likely to become a criminal. This was an effort to identify future criminals in the youth population of a school system [2]. This supposedly led to numerous interventions with at-risk students in the summer following the implementation of the at-risk students list. School resource officers have also been reportedly praised for contributing to intelligence briefings and filing field interview reports on interactions with at-risk kids [2]. This form of investigation into children for their likelihood to become criminals is unprecedented, as far as the public knows.\nAlthough identifying students at risk of dropout or failure in school has seen much implementation throughout history, the usage of school data in policing has only recently been brought to light. Because of the intention to keep these programs secret from the public, the only case of such a system brought into the public eye was that of Pasco County, detailed above. There are likely multiple other implementations of a similar system which have not yet been revealed, but to the public’s knowledge, this is the only pertinent case.\n\n\nBroader View\nGenerally, the public has conveyed great disdain towards Pasco County’s implementation and usage of a predictive policing system. Affected members of the community have worked closely with investigative journalists and law corporations in order to publicize their grievances and fight for their constitutional rights. This case has realized the exact fears which the public has already had on algorithm-assisted policing efforts. And the vast number of new outlets which have picked up reporting on the case supports this.\nAdditionally, this case, specifically, has led to the involvement of the federal government in investigating the suspected injustices of such a system, with the Department of Education uptaking an investigation into the usage of school data by the Pasco County Sheriff Office. This conveys a necessity for change as a result of public outrage. Citizens do not want their constitutional rights to be breached, so they are rightfully upset when a case like this arises."
  },
  {
    "objectID": "projects/governmentSurveillance/index.html#chinese-social-credit-system",
    "href": "projects/governmentSurveillance/index.html#chinese-social-credit-system",
    "title": "Ethics of Intelligent Government Surveillance Systems",
    "section": "Chinese Social Credit System",
    "text": "Chinese Social Credit System\n\nDescription\nThe Chinese Social Credit System is a system put in place and developed by the Chinese government that allows for the monitoring and the rating of it’s citizens. This rating system uses artificial intelligence, big data analytics, personal reportings, and surveillance technologies to get information about individuals to rate their level of trustworthiness as citizens [3]. The information they use includes financial information, online activities, social behavior, and even their physical movements.This score affects every area of the individual’s life, from travel, to loans, to job opportunities, and even to their right to express themselves and their opinions online [3]. This system has been in development for almost a decade now but is expected to be fully implemented and operational by 2025.\n\n\nCommunity Impact\nThe Social Credit System’s (SCS) implementation has had a profound impact on Chinese society, both positive and negative. On the one hand, the implementation of the SCS shoots for complete accountability and because of this it has helped to reduce crime rates, promote financial transparency, and even increase social cohesion by rewarding good behavior and punishing bad behavior. An example of this is that “good” citizens, those with a high score which indicates a high level of trustworthiness, get to enjoy discounted services and priority access to public resources while “bad” citizens face travel restrictions, high insurance premiums, and even social stigma. On the other hand, the system is criticized, understandably, for being intrusive, discriminatory, and (maybe most importantly) prone to abuse. There are also fears that this system gives the government the ability to silence dissent, punish minorities, and restrict personal freedoms. Another negative aspect of the SCS is that it creates a culture of fear and mistrust where people are constantly watching their behavior and that of others in order to avoid negative consequences.\n\n\nEthical Implications\nThe Social Credit System raises a number of ethical questions related to privacy, fairness, and accountability. Regarding privacy, there are concerns since the system collects and analyzes a very large amount of personal data without clear consent or oversight [3]. This opens the opportunity to profile and discriminate against individuals based on their beliefs, ethnicity, or even social status. These issues raise concerns about the people’s right to privacy, autonomy, and due process because the citizens graded by this system may not have access to the information used to calculate their score, or the ability to contest it’s accuracy. From the fairness perspective, the system, as all models/algorithms are, is prone to reinforcing existing biases and inequalities. This would lead to giving advantages to those who are already privileged while punishing those who are marginalized or vulnerable. This raises questions about social justice, equality, and even human dignity since citizens can be judged based on factors they cannot control such as their family background, medical history, or political affiliations. From an accountability perspective, the system completely lacks transparency and oversight. The lack of independent oversight makes the system prone to corruption, abuse, and errors.\n\n\nUniqueness\nWhile there are semi-similar systems for generating scores to quantify an individual’s trustworthiness in a specific area, such as the credit score system in the United States, the Social Credit System in China is unique in several ways both in terms of technical features and social implications. Technically, it represents a new paradigm of surveillance and control to reshape individual and collective behavior through a combination of digital technologies, behavioral sciences, and social engineering. It relies on a massive network of data sources which are used to track and monitor citizens in real time and in all aspects of their lives [3]. From a social perspective, it attempts to redefine the relationship between the state it’s citizens. It aims to make social trustworthiness a key criteria for participation in society.\n\n\nBroader View\nThere are many different and well accepted ways of quantifying an individual’s trustworthiness in a specific area. From credit score used for your financial trustworthiness to ride sharing companies like Uber using a scoring system to evaluate drivers, however nothing has come close to attempting to score complete trustworthiness as a citizen. The Chinese Social Credit System attempts to do this because the idea of complete accountability, especially with such a large population, would be very beneficial to the state. The issue with implementing any system this vast with such an effect on people’s lives is that there must be independent oversight as well as transparency on how the score is generated. Any less and you have an all powerful system that is incredibly prone to corruption and abuse. In addition, to collect that data you are taking away all of your citizens’ right to privacy as well as creating a culture and atmosphere of fear among your population since every one of their actions is being observed, recorded, and scored."
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "Finley Malloc",
    "section": "",
    "text": "Finley Malloc is the Chief Data Scientist at Wengo Analytics. When not innovating on data platforms, Finley enjoys spending time unicycling and playing with her pet iguana.\n\n\nUniversity of California, San Diego | San Diego, CA PhD in Mathematics | Sept 2011 - June 2015\nMacalester College | St. Paul MA B.A in Economics | Sept 2007 - June 2011\n\n\n\nWengo Analytics | Head Data Scientist | April 2018 - present\nGeoScynce | Chief Analyst | Sept 2012 - April 2018\n\n \n  \n   \n  \n    \n     twitter\n  \n  \n    \n     Github"
  },
  {
    "objectID": "about.html#education",
    "href": "about.html#education",
    "title": "Finley Malloc",
    "section": "",
    "text": "University of California, San Diego | San Diego, CA PhD in Mathematics | Sept 2011 - June 2015\nMacalester College | St. Paul MA B.A in Economics | Sept 2007 - June 2011"
  },
  {
    "objectID": "about.html#experience",
    "href": "about.html#experience",
    "title": "Finley Malloc",
    "section": "",
    "text": "Wengo Analytics | Head Data Scientist | April 2018 - present\nGeoScynce | Chief Analyst | Sept 2012 - April 2018"
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Shayne Biagi",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n         \n          Author\n        \n     \n  \n\n\n\n\n  \n\n\n\n\nPost With Code\n\n\n\n\n\n\n\nnews\n\n\ncode\n\n\nanalysis\n\n\n\n\n\n\n\n\n\n\n\nAug 31, 2023\n\n\nHarlow Malloc\n\n\n\n\n\n\n  \n\n\n\n\nWelcome To My Blog\n\n\n\n\n\n\n\nnews\n\n\n\n\n\n\n\n\n\n\n\nAug 28, 2023\n\n\nTristan O’Malley\n\n\n\n\n\n\n  \n\n\n\n\nEthics of Intelligent Government Surveillance Systems\n\n\n\n\n\n\n\nethics\n\n\nmachine learning\n\n\ndata analytics\n\n\n\n\n\n\n\n\n\n\n\nMay 23, 2023\n\n\nPierre Sarabamoun, Shaunak Juvekar, Shayne Biagi, Shrikanth Upadhayaya, Srujan Vithalani\n\n\n\n\n\n\n  \n\n\n\n\nDeep Learning Equity Trading Model\n\n\n\n\n\n\n\nmachine learning\n\n\ndata analytics\n\n\nneural networks\n\n\n\n\n\n\n\n\n\n\n\nMay 4, 2023\n\n\nShayne Biagi, Andrew Istfan, Franco Medrano\n\n\n\n\n\n\n  \n\n\n\n\nAnalysis of A Push-Relabel Based Additive Approximation for Optimal Transport\n\n\n\n\n\n\n\noptimal transport\n\n\ntransport theory\n\n\nmachine learning\n\n\nmathematics\n\n\nalgorithms\n\n\n\n\n\n\n\n\n\n\n\nApr 10, 2023\n\n\nShayne Biagi\n\n\n\n\n\n\n  \n\n\n\n\nAnalysis of A Graph Theoretic Additive Approximation of Optimal Transport\n\n\n\n\n\n\n\noptimal transport\n\n\ntransport theory\n\n\nmachine learning\n\n\nmathematics\n\n\nalgorithms\n\n\n\n\n\n\n\n\n\n\n\nApr 7, 2023\n\n\nShayne Biagi\n\n\n\n\n\n\n  \n\n\n\n\nHypothetical Applications of Optimal Transport\n\n\n\n\n\n\n\noptimal transport\n\n\ntransport theory\n\n\n\n\n\n\n\n\n\n\n\nJan 18, 2023\n\n\nShayne Biagi\n\n\n\n\n\n\n  \n\n\n\n\nWhat is the Optimal Transport Problem?\n\n\n\n\n\n\n\noptimal transport\n\n\ntransport theory\n\n\nmathematics\n\n\nstatistics\n\n\n\n\n\n\n\n\n\n\n\nDec 16, 2022\n\n\nShayne Biagi\n\n\n\n\n\n\n  \n\n\n\n\nPandoc Guide\n\n\n\n\n\n\n\ncode\n\n\ndocumentation\n\n\n\n\n\n\n\n\n\n\n\nMar 3, 2022\n\n\nMason Gelletly, Lucy Paul, Shayne Biagi, Ashlyn East, Theodore Gunn\n\n\n\n\n\n\nNo matching items"
  },
  {
    "objectID": "projects/OT-applications/index.html",
    "href": "projects/OT-applications/index.html",
    "title": "Hypothetical Applications of Optimal Transport",
    "section": "",
    "text": "Here, I discuss some possible hypothetical applications of the Optimal Transport algorithms.\n\n\n\nOptimal transport theory has been shown countless times to be used in interpolating the transition from one image to another. With such, we can gain a set of intermediary images to develop a seamless transition. Consider the same concept of a seamless transition from one state to another, but regard transitioning from one sound wave to another. This is the idea I would like to describe.\nWith music streaming services such as Spotify, seamless playback is available with a few different methods, but are any of them “really” seamless? The first available method of seamless playback offered by Spotify utilizes a crossfade, wherein the first track fades out (decreases in volume) as the next track fades in (increases in volume). This is not ideal, as the ending of the first track is lost in the introduction of the new track. Despite this, it is still the most popular method to transitioning between sound clips as it removes abrupt ends in most circumstances. Sometimes, however, if two consecutive tracks differ greatly in pitch or the like, crossfading between them will not remove the abruptness of the transition, worsening the listening experience greatly. Also, with crossfade, a reduction in volume is noticable during the transition.\nAnother available transition method offered by Spotify is their “Automix” feature. This is an improvement over the crossfade feature, but is not available on all platforms nor does it actually regard the pitch of the notes from either track. Spotify defines the feature on their website as follows:\n“(Automix) Allows beat-matched seamless transitions between songs on select Spotify playlists. Works even on Shuffle.”\nIt essentially mixes the previous track with the next track at a specific time, determined by some algorithm, in order to maintain seamless beat transitions. But again, this is not actually seamless, as the transition is still quite noticeable by the human ear. And I, for one, absolutely hate that there is not yet a widely available way to have this seamless transition.\nSo, how do we make a transition seamless? With optimal transport theory, obviously! We can use optimal transport to determine the most efficient transportation plan between the ending notes of the previous track and the beginning of the next track. This way, we will not experience any abrupt transitions between two songs. One song will simply “become” the other without a loss in volume or a noticeable gap in pitch! The sounds can be split into frequencies and moved in such a manner to produce a completely different song. Transitioning directly from the notes of one track to that of another with an optimal transport plan will provide us with a set of intermediary sounds in the transition. These intermediary sounds can then be placed between the two tracks to provide fully seamless sound transitions between tracks!\nIn addition to this use case, we can extend this method of seamlessly transitioning between sound clips to many other aspects of sound engineering as well. For instance, DJs constantly shift between tracks on a set list while performing. They would benefit largely from the ability to rid their performances of abrupt song changes. Additionally, music artists could engineer this into music on their albums and/or performances. Film score composers could even use this in movies to further immerse viewers.\n\n\n\n\nFor this next application, I wish to further continue on the topic of optimal transport-based interpolation. The topic I wish to address here, more specifically, is level transitions in 2D-platformer video games. Consider possibly the most well-known platform game ever, Super Mario Bros. From one stage to the next, the user must transition abruptly between levels. This involves many stoppages. Mario first becomes unplayable after riding the flagpole at the end of a stage. Then the user is sent to a black screen detailing the level number and the number of lives left. Then, Mario will return to being playable once the next stage is loaded to the screen. Can we make this level transition seamless? If it were seamless, the game would have a much different and possibly more cohesive playing experience.\nEnter optimal transport theory.\nFirstly, we should define what it means for a transition to be seamless in this case. Visually, a transition would appear seamless, if one stage’s environment morphs into the next stage without abrupt visual stoppages. In the case with Super Mario Bros, visually, the black screen between stages is the only contributing factor to the transition’s abruptness. By simply using optimal transport-based image interpolation between the environment at the end of the previous stage, where Mario is no longer playable, to that of the next stage, we would gain a set of intermediary environments. Then, placing these intermediary environments in place of the black screen would provide us with a visually seamless transition. Next, we can draw the collision boxes around the new sprites and blocks for the next level. Once that is done, we may return control of Mario back to the user.\nBut, I do not believe that this transition would be truly seamless, as Mario is still unactionable during this transition. So, using simple image interpolation would still yield a somewhat “seam-ful” transition between levels. In other words, user-control is disrupted and hence the transition is still not truly seamless. So, let us keep Mario actionable during this transition. If Mario is actionable, however, we must correctly implement the collision boxes of the environment in real time so that the playability of the next level is left unaltered by our different method of transitioning.\nA trivial solution to this follows. Consider, every individual frame in the transition between two stages, using optimal transport-based image interpolation. In the intermediary images, collisionable objects from the first stage will begin to disappear as new collisionable objects from the new stage appear. For each frame, we must redraw collision boxes to fit correctly over the current environment. And, since redrawing collision boxes is possible during playtime, evident when Mario eats a mushroom and gets larger, our solution is plausible. With this, all frames within the transition would be correctly drawn, and thus Mario would be correctly playable for the entire transition.\nAlso, it should be noted that I am assuming a locked viewbox when transitioning between levels. In other words, the level will not “scroll” during this transition. Therefore, we are only interpolating between static images. Static in this sense is only regarding the environment, and not the playable character (Mario). And although this may seem like an oversight, it is a condition which is easily enforced in a manner that leaves the player unaffected. For instance, we can speed up and slow down the transition so that it is physically impossible for Mario to reach a position which initiates the “scrolling” before the next level has been wholly reproduced. Once the transition is over, we can unlock the viewbox and the next level will continue normally.\nFinally, we have reached a point of seamless transitions between levels in a platformer. The same concept can be extended to 3-dimensional games with level transitions as well. It would require some more severe modifications, however. Instead of image interpolation, we could interpolate a transition between surfaces on 3-dimensional mesh objects in the environment, and redraw the collision boxes over the intermediary models. The applications are limitless!\n\n\n\n\nFinally, with this last application of optimal transport theory, I would like to harp on the main goal of optimal transport. That is, the desire to develop a better method of describing the statistic distance between distributions, or how different two distributions are. With utilization of the Wasserstein distance, we can do this with optimal transport theory.\nBegin by considering wave energy. Wave energy, simply put, is a method of producing renewable energy from the motion of ocean waves. Unfortunately, this form of energy production is mostly defunct nowadays. And this is due to the inconsistency of waves and the expensive costs associated with developing and operating such a power plant. The amount of power produced by a wave energy plant is primarily dependent on the sheer size of the wave and the kinetic energy it encompasses. Often, the operational costs of the plant will outweigh the price of the energy it produces, and so the entire venture becomes unmanageable from a financial sense.\nNow, one solution to this problem would be to intermittently run the power plant only at times where the price of the energy produced is greater than the current operational costs. This way, operational costs are not incurred when power output is too low (i.e. the waves are not large enough). But, with this solution, we raise another important question: When exactly should the power plant be run? To put this solution into effect, we would need some way to decide when the ideal times to activate the power plant are. That is where optimal transport theory comes in.\nTo apply optimal transport theory here, there is something we must know first. Assume that we know the optimal wave shape to produce the most energy at any given time. Although time-consuming, finding the optimal wave shape would be plausible. For instance, over the course of a year, researchers could chart waveforms along with the amount of energy produced by the plant. Then, they could assign the optimal waveform as that which produced the most energy in that year. This waveform may differ from year to year, so I will refer to the optimal waveform as the current optimal waveform. With optimal transport, we can determine the Wasserstein distance between two waves, denoting how different the two waves are. At any given time, we are able to compute this distance between the optimal waveform and the current waveform. So, in realtime, we could compute how “different” our current waveform is from the optimal waveform.\nNow, over the course of another year, suppose we charted the Wasserstein distances between the optimal waveform and the current waveform. In addition to this, we would keep track of when the power plant becomes profitable (i.e. the operational costs are outweighed by the energy production). To conceptualize this further, consider the fact that at some point, any given waveform may be too “different” from the optimal waveform to produce enough energy to offset the current operational costs. This is why we keep track of when the plant becomes profitable. With the strategy above, we can determine an upper limit to the Wasserstein distance \\(W\\) such that any distance greater than \\(W\\) causes the plant to become unprofitable. With this upper limit, we can now know at which times we should activate the generator to garner the greatest profit margins. And finally with greater profit margins, running a wave energy plant may finally become a financially manageable venture.\nIt is important to note that I do not know definitively if there is any correlation between the Wasserstein distance between two wave forms and the amount of kinetic energy these waves contain. For example, does a large Wasserstein distance between some waveform and the optimal waveform mean that the two waveforms contain largely different amounts of kinetic energy? I am not sure. I do, however, know that the kinetic energy of a wave is directly correlated with a wave’s amplitude, frequency, and wavelength. And all such factors which contribute to a wave’s kinetic energy are also influential to its shape. I just am not educated enough on this topic to definitively understand whether variations in frequency and wavelength contribute largely to a variation in Wasserstein distance. Or even if ocean waves vary enough in frequency and wavelength to consider these values at all."
  },
  {
    "objectID": "projects/OT-applications/index.html#seamless-music-transitions",
    "href": "projects/OT-applications/index.html#seamless-music-transitions",
    "title": "Hypothetical Applications of Optimal Transport",
    "section": "",
    "text": "Optimal transport theory has been shown countless times to be used in interpolating the transition from one image to another. With such, we can gain a set of intermediary images to develop a seamless transition. Consider the same concept of a seamless transition from one state to another, but regard transitioning from one sound wave to another. This is the idea I would like to describe.\nWith music streaming services such as Spotify, seamless playback is available with a few different methods, but are any of them “really” seamless? The first available method of seamless playback offered by Spotify utilizes a crossfade, wherein the first track fades out (decreases in volume) as the next track fades in (increases in volume). This is not ideal, as the ending of the first track is lost in the introduction of the new track. Despite this, it is still the most popular method to transitioning between sound clips as it removes abrupt ends in most circumstances. Sometimes, however, if two consecutive tracks differ greatly in pitch or the like, crossfading between them will not remove the abruptness of the transition, worsening the listening experience greatly. Also, with crossfade, a reduction in volume is noticable during the transition.\nAnother available transition method offered by Spotify is their “Automix” feature. This is an improvement over the crossfade feature, but is not available on all platforms nor does it actually regard the pitch of the notes from either track. Spotify defines the feature on their website as follows:\n“(Automix) Allows beat-matched seamless transitions between songs on select Spotify playlists. Works even on Shuffle.”\nIt essentially mixes the previous track with the next track at a specific time, determined by some algorithm, in order to maintain seamless beat transitions. But again, this is not actually seamless, as the transition is still quite noticeable by the human ear. And I, for one, absolutely hate that there is not yet a widely available way to have this seamless transition.\nSo, how do we make a transition seamless? With optimal transport theory, obviously! We can use optimal transport to determine the most efficient transportation plan between the ending notes of the previous track and the beginning of the next track. This way, we will not experience any abrupt transitions between two songs. One song will simply “become” the other without a loss in volume or a noticeable gap in pitch! The sounds can be split into frequencies and moved in such a manner to produce a completely different song. Transitioning directly from the notes of one track to that of another with an optimal transport plan will provide us with a set of intermediary sounds in the transition. These intermediary sounds can then be placed between the two tracks to provide fully seamless sound transitions between tracks!\nIn addition to this use case, we can extend this method of seamlessly transitioning between sound clips to many other aspects of sound engineering as well. For instance, DJs constantly shift between tracks on a set list while performing. They would benefit largely from the ability to rid their performances of abrupt song changes. Additionally, music artists could engineer this into music on their albums and/or performances. Film score composers could even use this in movies to further immerse viewers."
  },
  {
    "objectID": "projects/OT-applications/index.html#game-level-transitions",
    "href": "projects/OT-applications/index.html#game-level-transitions",
    "title": "Hypothetical Applications of Optimal Transport",
    "section": "",
    "text": "For this next application, I wish to further continue on the topic of optimal transport-based interpolation. The topic I wish to address here, more specifically, is level transitions in 2D-platformer video games. Consider possibly the most well-known platform game ever, Super Mario Bros. From one stage to the next, the user must transition abruptly between levels. This involves many stoppages. Mario first becomes unplayable after riding the flagpole at the end of a stage. Then the user is sent to a black screen detailing the level number and the number of lives left. Then, Mario will return to being playable once the next stage is loaded to the screen. Can we make this level transition seamless? If it were seamless, the game would have a much different and possibly more cohesive playing experience.\nEnter optimal transport theory.\nFirstly, we should define what it means for a transition to be seamless in this case. Visually, a transition would appear seamless, if one stage’s environment morphs into the next stage without abrupt visual stoppages. In the case with Super Mario Bros, visually, the black screen between stages is the only contributing factor to the transition’s abruptness. By simply using optimal transport-based image interpolation between the environment at the end of the previous stage, where Mario is no longer playable, to that of the next stage, we would gain a set of intermediary environments. Then, placing these intermediary environments in place of the black screen would provide us with a visually seamless transition. Next, we can draw the collision boxes around the new sprites and blocks for the next level. Once that is done, we may return control of Mario back to the user.\nBut, I do not believe that this transition would be truly seamless, as Mario is still unactionable during this transition. So, using simple image interpolation would still yield a somewhat “seam-ful” transition between levels. In other words, user-control is disrupted and hence the transition is still not truly seamless. So, let us keep Mario actionable during this transition. If Mario is actionable, however, we must correctly implement the collision boxes of the environment in real time so that the playability of the next level is left unaltered by our different method of transitioning.\nA trivial solution to this follows. Consider, every individual frame in the transition between two stages, using optimal transport-based image interpolation. In the intermediary images, collisionable objects from the first stage will begin to disappear as new collisionable objects from the new stage appear. For each frame, we must redraw collision boxes to fit correctly over the current environment. And, since redrawing collision boxes is possible during playtime, evident when Mario eats a mushroom and gets larger, our solution is plausible. With this, all frames within the transition would be correctly drawn, and thus Mario would be correctly playable for the entire transition.\nAlso, it should be noted that I am assuming a locked viewbox when transitioning between levels. In other words, the level will not “scroll” during this transition. Therefore, we are only interpolating between static images. Static in this sense is only regarding the environment, and not the playable character (Mario). And although this may seem like an oversight, it is a condition which is easily enforced in a manner that leaves the player unaffected. For instance, we can speed up and slow down the transition so that it is physically impossible for Mario to reach a position which initiates the “scrolling” before the next level has been wholly reproduced. Once the transition is over, we can unlock the viewbox and the next level will continue normally.\nFinally, we have reached a point of seamless transitions between levels in a platformer. The same concept can be extended to 3-dimensional games with level transitions as well. It would require some more severe modifications, however. Instead of image interpolation, we could interpolate a transition between surfaces on 3-dimensional mesh objects in the environment, and redraw the collision boxes over the intermediary models. The applications are limitless!"
  },
  {
    "objectID": "projects/OT-applications/index.html#wave-shape-analysis-for-tidal-power-estimation",
    "href": "projects/OT-applications/index.html#wave-shape-analysis-for-tidal-power-estimation",
    "title": "Hypothetical Applications of Optimal Transport",
    "section": "",
    "text": "Finally, with this last application of optimal transport theory, I would like to harp on the main goal of optimal transport. That is, the desire to develop a better method of describing the statistic distance between distributions, or how different two distributions are. With utilization of the Wasserstein distance, we can do this with optimal transport theory.\nBegin by considering wave energy. Wave energy, simply put, is a method of producing renewable energy from the motion of ocean waves. Unfortunately, this form of energy production is mostly defunct nowadays. And this is due to the inconsistency of waves and the expensive costs associated with developing and operating such a power plant. The amount of power produced by a wave energy plant is primarily dependent on the sheer size of the wave and the kinetic energy it encompasses. Often, the operational costs of the plant will outweigh the price of the energy it produces, and so the entire venture becomes unmanageable from a financial sense.\nNow, one solution to this problem would be to intermittently run the power plant only at times where the price of the energy produced is greater than the current operational costs. This way, operational costs are not incurred when power output is too low (i.e. the waves are not large enough). But, with this solution, we raise another important question: When exactly should the power plant be run? To put this solution into effect, we would need some way to decide when the ideal times to activate the power plant are. That is where optimal transport theory comes in.\nTo apply optimal transport theory here, there is something we must know first. Assume that we know the optimal wave shape to produce the most energy at any given time. Although time-consuming, finding the optimal wave shape would be plausible. For instance, over the course of a year, researchers could chart waveforms along with the amount of energy produced by the plant. Then, they could assign the optimal waveform as that which produced the most energy in that year. This waveform may differ from year to year, so I will refer to the optimal waveform as the current optimal waveform. With optimal transport, we can determine the Wasserstein distance between two waves, denoting how different the two waves are. At any given time, we are able to compute this distance between the optimal waveform and the current waveform. So, in realtime, we could compute how “different” our current waveform is from the optimal waveform.\nNow, over the course of another year, suppose we charted the Wasserstein distances between the optimal waveform and the current waveform. In addition to this, we would keep track of when the power plant becomes profitable (i.e. the operational costs are outweighed by the energy production). To conceptualize this further, consider the fact that at some point, any given waveform may be too “different” from the optimal waveform to produce enough energy to offset the current operational costs. This is why we keep track of when the plant becomes profitable. With the strategy above, we can determine an upper limit to the Wasserstein distance \\(W\\) such that any distance greater than \\(W\\) causes the plant to become unprofitable. With this upper limit, we can now know at which times we should activate the generator to garner the greatest profit margins. And finally with greater profit margins, running a wave energy plant may finally become a financially manageable venture.\nIt is important to note that I do not know definitively if there is any correlation between the Wasserstein distance between two wave forms and the amount of kinetic energy these waves contain. For example, does a large Wasserstein distance between some waveform and the optimal waveform mean that the two waveforms contain largely different amounts of kinetic energy? I am not sure. I do, however, know that the kinetic energy of a wave is directly correlated with a wave’s amplitude, frequency, and wavelength. And all such factors which contribute to a wave’s kinetic energy are also influential to its shape. I just am not educated enough on this topic to definitively understand whether variations in frequency and wavelength contribute largely to a variation in Wasserstein distance. Or even if ocean waves vary enough in frequency and wavelength to consider these values at all."
  },
  {
    "objectID": "projects/OT-pushRelabel/index.html",
    "href": "projects/OT-pushRelabel/index.html",
    "title": "Analysis of A Push-Relabel Based Additive Approximation for Optimal Transport",
    "section": "",
    "text": "Optimal transport is quite expensive to compute exactly, so there have been recent desires for fast and reliable approximation algorithms. Here I analyze and describe a push-relabel based additive approximation algorithm for Optimal Transport. The paper I reference for this algorithm was originally submitted to the arXiv repository on Mar 7, 2022 by Nathaniel Lahn, Sharath Raghvendra (my former theory and algorithms professor), and Kaiyi Zhang. It is based on the push-relabel framework for min-cost flow problems. And unlike many other approaches, this algorithm can be parallelized for an even quicker execution time.\nPlease see the source below to check out the original paper.\n[A Push-Relabel Based Additive Approximation for Optimal Transport]"
  },
  {
    "objectID": "projects/OT-pushRelabel/index.html#description",
    "href": "projects/OT-pushRelabel/index.html#description",
    "title": "Analysis of A Push-Relabel Based Additive Approximation for Optimal Transport",
    "section": "Description",
    "text": "Description\nThe provided paper details a simple combinatorial approach to find an \\(\\varepsilon\\)-approximation of the Optimal Transport distance. The algorithm is based on the push-relabel framework for min-cost flow problems. This algorithm aims to reduce the numerical instabilities present in the Sinkhorn method of approximating optimal transport. Similar to before, in order to apply this algorithm, the input problem must be transformed so that all edge costs equal a \\(\\varepsilon\\) integer multiple and all nodes are given a dual label. The transformation procedure is detailed below:\n\nTransformation Procedure\nInitially, the input will be a bipartite graph. Let \\(G\\) be this input graph, with two sets of nodes \\(A\\) and \\(B\\). Nodes in set \\(A\\) are referred to as demand nodes, and nodes in set \\(B\\) are referred to as supply nodes.\nFirst, we must convert edge costs to integer multiples of \\(\\varepsilon\\), where \\(\\varepsilon\\) is some constant such that \\(\\varepsilon&gt;0\\). Let \\(c(u,v)\\) denote the cost of edge \\((u,v)\\), and \\(\\overline{c}(u,v)\\) denote the transformed edge cost. For every edge \\((u,v) \\in A \\times B\\), we must transform the edge cost such that, \\(\\overline{c}(u,v) = \\varepsilon\\lfloor c(u,v) / \\varepsilon \\rfloor\\)\nIn addition to this, we must assign dual weights to every vertex \\(v \\in A \\cup B\\). These dual weights must satisfy a set of feasibility conditions. A matching \\(M\\) is considered \\(\\varepsilon\\)-feasible, if for every edge \\((a,b) \\in A \\times B\\):\n\n\\(y(a) + y(b) \\leq \\overline{c}(a,b) + \\varepsilon\\) if edge \\((a,b)\\) is not in the matching \\(M\\).\nor \\(y(a) + y(b) = \\overline{c}(a,b)\\) if edge \\((a,b)\\) is in the matching \\(M\\)."
  },
  {
    "objectID": "projects/OT-pushRelabel/index.html#algorithm",
    "href": "projects/OT-pushRelabel/index.html#algorithm",
    "title": "Analysis of A Push-Relabel Based Additive Approximation for Optimal Transport",
    "section": "Algorithm",
    "text": "Algorithm\nDual weights for every supply node \\(b\\in B\\) are initialized such that \\(y(b) = \\varepsilon\\). Similarly, dual weights for every demand node \\(a\\in A\\) are initialized such that \\(y(a) = 0\\). Finally, we will initialize the matching \\(M\\) to be empty. In other words, \\(M = \\emptyset\\).\nThis algorithm executes in phases, wherein each phase consists of three steps. In each phase, the algorithm will construct a set of free supply nodes, consisting of all free nodes from set \\(B\\). Let \\(B'\\) denote this set of free supply nodes. If \\(|B'| \\leq \\varepsilon n\\), then \\(M\\) is an \\(\\varepsilon\\)-feasible matching and the algorithm will arbitrarily match the remaining free vertices and return the new matching. In the case that \\(|B'| &gt; \\varepsilon n\\), the algorithm will continue by computing a subset of admissible edges with at least least one end point in the free supply node set \\(B'\\).\nLet \\(E\\) denote the full set of admissible edges in the graph, and let \\(E'\\) denote the set of admissible edges with at least least one end point in the free supply node set \\(B'\\). It follows that \\(E' \\subseteq E\\). Finally, let \\(A'\\) denote the set of demand nodes that participate in at least on edge in \\(E'\\). In other words \\(A' = \\{a|a \\in A\\) and \\((a,b) \\in E' \\}\\). Now, the following steps are executed by the algorithm:\n\nGreedy Step: Computes a maximal matching \\(M'\\) in the graph \\(G'(A' \\cup B', E')\\).\nMatching Update: Let \\(A''\\) denote the set of nodes in \\(A'\\) which are present in both \\(M\\) and \\(M'\\). Also, let \\(M''\\) denote the set of edges in \\(M\\) that are incident on some node in \\(A''\\). The algorithm will add the edges of \\(M'\\) to \\(M\\) and remove the edges of \\(M''\\) from \\(M\\).\nDual Weight Update: Let \\(\\overline{y}(v)\\) denote the updated dual weight of some node \\(v\\). For every edge \\((a,b) \\in M'\\), the dual weight of node \\(a\\in A\\) will be updated such that \\(\\overline{y}(a) = y(a) - \\varepsilon\\). Additionally, for every node \\(b\\in B\\) in the set \\(B'\\) such that \\(b\\) is free with respect to \\(M'\\), the dual weight of node \\(b\\) will be updated such that \\(\\overline{y}(b) = y(b) + \\varepsilon\\).\n\n\nExtending the Algorithm to the Optimal Transport Problem\nWith some instance \\(\\mathcal{I}\\) of the optimal transport problem, we can scale the supplies and demands at each node by some multiplicative factor of \\(\\Theta\\). Doing so will not delegitimize the optimal transport plan, as the cost of the optimal transport plan will simply be increased by a factor of \\(\\Theta\\).\nNow, we can choose \\(\\Theta = 4n/\\varepsilon\\), and transform supply and demand values to integer values. To transform the supply and demand values, we can just round supply values down and round demand values up to the nearest integers.\nNext, we create a matching instance by replacing each node with a group of nodes, such that the total of number of replacement nodes equals the respective supply or demand value of the original node. Each node in the replacement group will be set to have a supply or demand value of exactly 1.\nThen, let \\(\\mathcal{A}\\) and \\(\\mathcal{B}\\) be the multi-set of the demand and supply nodes we just produced. Let this new instance of the unbalanced matching problem be denoted by \\(\\mathcal{I'}\\). Now, we can solve the problem instance \\(\\mathcal{I'}\\) with the aforementioned algorithm."
  },
  {
    "objectID": "projects/OT-pushRelabel/index.html#correctness",
    "href": "projects/OT-pushRelabel/index.html#correctness",
    "title": "Analysis of A Push-Relabel Based Additive Approximation for Optimal Transport",
    "section": "Correctness",
    "text": "Correctness\nBefore beginning an assessment of this algorithm’s correctness, we must define some invariants which the algorithm maintains. The invariants and their respective proofs follow:\n(Invariant 1): For every node \\(b\\in B\\), \\(y(b) \\geq 0\\). In other words, every node in set \\(B\\) has a non-negative dual weight. Additionally, for every node \\(a\\in A\\), \\(y(a) \\leq 0\\). In other words, every node in set \\(A\\) is associated with a non-positive dual weight. Furthermore, every free node in set \\(A\\) is associated with a dual weight of 0.\n\nProof. Note that the invariant is true at the start of the algorithm. This is true because dual weights for all nodes in set \\(B\\) are initialized to \\(\\varepsilon\\) and are thus non-negative, and dual weights for all nodes in set \\(A\\) are initialized to \\(0\\) and are thus non-positive. Also, all nodes in set \\(A\\) are initially free as the matching \\(M\\) is initialized to the empty set. Now, inductively assume that the invariant holds at the start of every phase. Next, we must show that the invariant continues to hold at the end of any given phase. Firstly, consider the dual weight update step. When updated, nodes from set \\(B\\) are increased by a factor of \\(\\varepsilon\\), while nodes from set \\(A\\) are decreased by the same factor. Thus, it follows that, as execution progresses, dual weights for nodes in set \\(B\\) strictly increase and dual weights for nodes in set \\(A\\) strictly decrease. So, dual weights for nodes in \\(B\\) will remain non-negative, and dual weights for nodes in \\(A\\) will remain non-positive. Now, we must show that every free node in set \\(A\\) with respect to the matching \\(M\\) will continue to retain a dual weight of \\(0\\). Computing the maximal matching in the greedy step will not affect this condition. Regarding the matching update step, we can conclude that after the matching \\(M\\) has been updated, any node of set \\(A\\) which was matched before the update will remain matched post-update. Because of this, we can further conclude that dual weights for every free node in set \\(A\\) retain zero values. Finally, regarding the dual weight update step, dual weights of nodes from set \\(A\\) are only reduced if said node it matched in \\(M'\\). And, since the previous step will have added edges of \\(M'\\) to \\(M\\), every node in \\(A\\) whose dual weight is updated will be matched in \\(M\\) post-update. Thus, dual weights for all free nodes in set \\(A\\) will retain a zero value. ◻\n\n(Invariant 2): The matching \\(M\\) and a set of dual weights maintains \\(\\varepsilon\\)-feasibility.\n\nProof. Note that the invariant is true at the start of the algorithm, as all \\(\\varepsilon\\)-feasibility conditions are maintained in initializing \\(M\\) and the set of dual weights. Now, inductively assume that the invariant holds at the start of every phase. Next, we must show that the invariant continues to hold at the end of any given phase. So, we must show that edges remain feasible through a phase. To do this, we need to show that the slack of all edges remains non-negative and all edges in the matching \\(M\\) have zero slack. Let \\(s(a,b)\\) denote the slack of edge \\((a,b)\\). Computing the maximal matching in the greedy step will not affect this condition. Only the matching update and dual weight update steps may violate feasibility. Consider an edge \\((a,b)\\) which is feasible but not admissible at the beginning of a phase. In other words \\(s(a,b) \\geq \\varepsilon\\). As the matching \\(M'\\) contains only admissible edges, edge \\((a,b)\\) cannot be in \\(M'\\) nor \\(M\\). Now, the dual weights of nodes \\(a\\) and \\(b\\) may be updated. If the dual weight \\(y(a)\\) is updated, \\(y(a)\\) will only be reduced by \\(\\varepsilon\\), in turn increasing \\(s(a,b)\\) by \\(\\varepsilon\\). Here, \\(s(a,b)\\) will retain a non-negative slack, and so feasibility is maintained. If the dual weight \\(y(b)\\) is updated, \\(y(b)\\) will only be increased by \\(\\varepsilon\\), in turn decreasing \\(s(a,b)\\) by \\(\\varepsilon\\). As, \\(s(a,b) \\geq \\varepsilon\\), \\(s(a,b)\\) will still be non-negative, and so feasibility is maintained. So, in the case that edge \\((a,b)\\) is not admissible at the beginning of a phase, feasibility is maintained. Next, we must consider the case where edge \\((a,b)\\) is admissible at the start of the phase. At the end of the phase, either \\((a,b)\\) is or is not present in the updated matching \\(M\\). Both of these cases are detailed below:\n\nIf \\((a,b)\\) is present in \\(M\\), there are two possible cases. Either, \\((a,b)\\) was in \\(M\\) before the update or \\((a,b)\\) is an edge in \\(M'\\) which was added to the matching and thus exists in \\(M\\) post-update. These further cases are detailed below.\n\nIf \\((a,b)\\) was in \\(M\\) before the update, dual weights for nodes \\(a\\) and \\(b\\) remain unchanged. Thus, feasibility is maintained.\nIf \\((a,b)\\) is an edge in \\(M'\\) which was added to the matching during the update, then \\((a,b) \\in E'\\) was a non-matching admissible edge prior to the update. So, \\(y(a)+y(b) = \\overline{c}(a,b) + \\varepsilon\\). When the dual weights for both nodes are updated, \\(y(a)\\) will be reduced by \\(\\varepsilon\\) and \\(y(b)\\) will remain unchanged, maintaining feasibility. So, once the phase completes, every edge of \\(M\\) retains feasibility.\n\nIf \\((a,b)\\) is not present in \\(M\\), there are two possible cases to explore. Either node \\(b\\) is not present in set \\(B'\\) or b is present in set \\(B'\\). These further cases are detailed below.\n\nIn the case that node \\(b\\) is not present in set \\(B'\\), the edge \\((a,b)\\) satisfies feasibility at the start of a phase. When updating the dual weights of these nodes, only nodes which are present inset \\(B'\\) are updated. And so, since node \\(b\\) is not in \\(B'\\), \\(y(b)\\) is left unchanged. Regarding node \\(a\\), \\(y(a)\\) may be decreased by \\(\\varepsilon\\), in turn increasing \\(s(a,b)\\). Thus, feasibility is always maintained.\nIn the case that node \\(b\\) is present in set \\(B'\\), since \\((a,b)\\) is admissible and is not in \\(M\\) post-update, \\((a,b)\\) cannot be in \\(M'\\). Now, consider the fact that \\(M'\\) is a maximal matching. Because of this, node \\(a\\) is matched with some other node \\(b' \\in M'\\). So, if \\(y(b)\\) is updated, and is thus increased by \\(\\varepsilon\\), \\(y(a)\\) will also updated, and thus will be decreased by \\(\\varepsilon\\). Due to this, \\(s(a,b)\\) remains non-negative, and feasibility is maintained.\n\n\n ◻\n\nNow, finally that the invariants have been proven, we can continue to assess this algorithm’s correctness. Rounding edge costs in the manner described previously will introduce an error of \\(\\varepsilon n\\). Then, after computing a matching of size at least \\((1-\\varepsilon)n\\), when arbitrarily matching the last \\(\\varepsilon n\\) vertices, we incur an error no greater than \\(\\varepsilon n\\). Finally, with reference to the following lemma (Lemma 3.1), we can conclude that the total error in the computed matching is no greater than \\(+3\\varepsilon n\\). And thus, the matching is \\(\\varepsilon\\)-feasible.\nLemma 3.1: The \\(\\varepsilon\\)-feasible matching of size at least \\((1-\\varepsilon)n\\) that is computed by the algorithm is within an additive error of \\(\\varepsilon n\\) from the optimal matching with respect to the rounded edge costs.\n\nProof. Let \\(M\\) be the matching computed by the algorithm. Furthermore, let \\(M_{OPT}\\) be the optimal matching with respect to the rounded edge costs. Considering the first feasibility condition and the fact that dual weights for all free vertices with respect to \\(M\\) are non-negative we can conclude that: \\(\\sum_{(a,b)\\in M} \\overline{c}(a,b) = \\sum_{(a,b)\\in M}(y(a) + y(b)) \\leq \\sum_{v\\in A\\cup B} y(v)\\). Now, as \\(M_{OPT}\\) is a perfect matching, we can further conclude from the second feasibility condition that: \\(\\sum_{v\\in A\\cup B} y(v) = \\sum_{(a,b)\\in M_{OPT}}(y(a) + y(b)) \\leq \\sum_{(a,b)\\in M_{OPT}}\\overline{c}(a,b) + \\varepsilon n\\). And so, \\(\\sum_{(a,b)\\in M} \\overline{c}(a,b) \\leq \\sum_{(a,b)\\in M_{OPT}}\\overline{c}(a,b) + \\varepsilon n\\). Thus, conclusively, the matching \\(M\\) is within an additive error of \\(\\varepsilon n\\) from the optimal matching with respect to rounded edge costs. ◻"
  },
  {
    "objectID": "projects/OT-pushRelabel/index.html#efficiency",
    "href": "projects/OT-pushRelabel/index.html#efficiency",
    "title": "Analysis of A Push-Relabel Based Additive Approximation for Optimal Transport",
    "section": "Efficiency",
    "text": "Efficiency\nBefore beginning an assessment of this algorithm’s efficiency, we must provide some lemmas. The lemmas and their respective proofs follow:\nLemma 3.2: For any node \\(v \\in A \\cup B\\), \\(|y(v)| \\leq (1+2\\varepsilon\\).\n\nProof. Note that the algorithm only ever increases the magnitude of any given dual weight. So, it is sufficient to show that the claim holds at the end of the algorithm. For all nodes in \\(A \\cup B\\), there are only two cases to consider. Either, a node is present in set \\(A\\) or in set \\(B\\). The two cases are described below:\n\nLet \\(a\\) be some node such that \\(a \\in A\\). If node \\(a\\) is free, then the claim holds true due to Invariant 1. Now, if node \\(a\\) is not free, and is thus matched to some node \\(b \\in B\\), edge \\((a,b)\\) is feasible. Because edge \\((a,b)\\) is feasible, we can conclude that \\(y(a) = \\overline{c}(a,b) - y(b) \\geq -y(b) \\geq -(1+2\\varepsilon)\\). So, \\(|y(a)| \\leq (1+2\\varepsilon)\\).\nLet \\(a\\) be some free node at the start of the final phase of the algorithm such that \\(a \\in A\\). We know, from Invariant 1, that \\(y(a) = 0\\). Now, for every vertex \\(b \\in B\\), edge \\((a,b)\\) satisfies feasibility. And so, \\(y(b) \\leq \\overline{c}(a,b) + \\varepsilon - y(a) = \\overline{c}(a,b) + \\varepsilon \\leq 1 + \\varepsilon\\). So, since dual weights for nodes in set \\(B\\) are only ever updated adding \\(\\varepsilon\\), the dual weight for any node \\(b \\in B\\) when the algorithm reaches completion is at most \\(1 + \\varepsilon + \\varepsilon = (1 + 2\\varepsilon)\\).\n\n ◻\n\nLemma 3.3: The sum of the magnitude of the dual weights increases by at least \\(\\varepsilon n_i\\) in each phase.\n\nProof. Let \\(b\\) be some node at the beginning of some phase such that \\(b\\in B'\\). Now, once the phase completes, there are two cases we must consider. Either, node \\(b\\) is still free or it has been matched. Both cases are detailed below:\n\nIn the case that node \\(b\\) is still free once the phase completes, \\(|y(b)|\\) increases by \\(\\varepsilon\\).\nIn the case that \\(b\\) is no longer free once the phase completes, and thus has been matched to some arbitrary node \\(a \\in M'\\), \\(|y(a)|\\) increases by \\(\\varepsilon\\) as well.\n\nSo, conclusively, we can see that each node in set \\(B'\\) causes the dual weight magnitude of some vertex to increase by exactly \\(\\varepsilon\\). Thus, the total dual weight increase in each phase is at least \\(\\varepsilon n_i\\). ◻\n\nLemma 3.4: The execution time of each phase is \\(O(n \\times n_i)\\).\n\nProof. Note that computing the set of free vertices at the start of each phase can be done in \\(O(n)\\) time. Now, consider the greedy step. Initially, the algorithm must compute a maximal matching in the graph \\(G'(A' \\cup B', E')\\). To construct this graph, we can scan over all edges incident to nodes in \\(B'\\), taking \\(O(n \\times n_i)\\) time. Then, the algorithm will find the maximal matching by processing each free node \\(b \\in B'\\). The algorithm will then attempt to match node \\(b\\) by finding the first edge \\((a,b)\\) in the graph such that \\(a\\) is not present in \\(M'\\). If such an edge is found, it is then added to \\(M'\\). If such an edge cannot be found, then node \\(b\\) cannot be matched in \\(M'\\). Once every node in \\(B'\\) has been processed, \\(M'\\) is maximal. In this step, the only procedure significant to the execution time is that which constructs graph \\(G'\\) in \\(O(n \\times n_i)\\) time. So, this step executes in \\(O(n \\times n_i)\\) time.\nNext, consider the matching update and dual weight update steps. Since any matching has a size which is \\(O(n)\\) and each node’s dual weight is updated at most once within any given phase, executing both of these steps will cost \\(O(n)\\) time. Finally, the execution time of the entire phase comes out to be \\(O( (n \\times n_i) + n ) = O(n \\times n_i)\\). ◻\n\nNow, given the lemmas, we can continue assessing the overall execution time of the algorithm. Suppose that the algorithm completes \\(t\\) phases during runtime. Let \\(n_i\\) denote the size of \\(B'\\) in phase \\(i\\) where \\(i\\) is some integer such that \\(1 &lt; i \\leq t\\). From the termination condition, we can conclude that phase \\(i\\) is only executed if \\(n_i &gt; \\varepsilon n\\). In other words, if the size of \\(B'\\) is more than \\(\\varepsilon n\\), the algorithm terminates. Now, from Lemma 3.2, we know that the magnitude of the dual weight of any node is at most \\((1+2\\varepsilon)\\). So, we use this as an upper bound for the magnitude of the dual weight of any given node. Furthering this concept, we can see that the sum total of the dual weight magnitudes over all nodes is at most \\(n(1+2\\varepsilon)\\). Also, from Lemma 3.3, we know that for some phase \\(i\\), the sum total of the dual weight magnitudes over all nodes increases by at least \\(\\varepsilon n_i\\).\nWith these observations, it follows that: \\(\\sum_{i=1}^{t} n_i \\leq n(1+2\\varepsilon)/\\varepsilon\\). And, \\(n(1+2\\varepsilon)/\\varepsilon\\) is \\(O(n/\\varepsilon)\\). Now, from Lemma 3.4, we know that the execution time of any given phase is \\(O(n \\times n_i)\\). So, the total sequential execution time of the algorithm becomes \\(O(n \\cdot \\sum_{i=1}^{t} n_i) = O(n \\cdot \\frac{n}{\\varepsilon}) = O (n^2/\\varepsilon)\\).\nRegarding the execution time of the algorithm when run in parallel, note that the matching update and dual weight update steps are trivially parallelizable, making them execute in constant time. Thus, with a classical parallel program to find maximal matchings for the greedy step in \\(O(\\log n)\\) time, the execution time of the program becomes \\(O(\\log n/\\varepsilon^2)\\)."
  },
  {
    "objectID": "projects/pandoc-guide/index.html",
    "href": "projects/pandoc-guide/index.html",
    "title": "Pandoc Guide",
    "section": "",
    "text": "This is a detailed, topic-based guide to Pandoc aimed at students in Creating User Documentation (ENGL3814) at Virginia Tech.\n\n\nThis task includes instructions on how to download and install Pandoc on your computer. There are two sections. One is for Windows users the other is for macOS users. There are also optional steps in each section to install a LaTeX program. Installing a LaTeX program is highly recommended, as Pandoc creates PDFs using LaTeX. The LaTeX program we will be installing is MiKTeX.\n\n\nThis task includes instructions on how to download and install Pandoc for Windows users. There are also optional instructions for installing MiKTeX.\n\n\n\nComputer\nWindows Operating System\nInternet Access\n\n\n\n\n\nGo to this webpage.\n\nNote: The webpage should look like this.\n\n\nDownload the pandoc-X.XX.X.X-windows-x86.msi file by clicking on the file name. This is the installer file.\n\nNote: X.XX.X.X refers to the version number. It is 2.17.1.1 in the image above.\n\nRun the installer by double-clicking on the downloaded file.\n\nNote: A new window should pop up. This is what it should look like.\n\n\nPress Next to continue.\nFollow the directions given by the Setup Wizard to complete your installation.\n\nNote: This program automatically installs Pandoc for you.\n\nLastly, to check if Pandoc was installed, type “pandoc -v” in your terminal. If Pandoc was installed correctly, you should see that the next line in the terminal is: “pandoc.exe X.XX.X.X” where the X’s represent the version number. If an error occurs, return to step 1 and try again. &gt; Note: You can access the Windows PowerShell terminal with Windows key + X. Select Windows PowerShell from the menu that pops up. (See image below for reference).\n\n\n\n\n\n\nGo to this webpage.\n\nNote: The webpage should look like this.\n\n\nUnder the Windows tab, select the Installer tab.\nPress the Download button to download the installer.\nRun the installer by double-clicking on the downloaded file.\n\nNote: A new window should pop up. This is what it should look like.\n\n\nRead the conditions and check the “I accept the MiKTeX copying conditions.” option.\nPress Next to continue.\nFollow the instructions in the installer to complete the installation.\nOnce you have reached this page in the installer, you are done. MiKTeX has been installed!\n\n\n\n\n\n\nThis task includes instructions on how to download and install Pandoc for macOS users. There are also optional instructions for installing MiKTeX.\n\n\n\nComputer\nmacOS Operating System\nInternet Access\n\n\n\n\n\nGo to this webpage.\n\nNote: The webpage should look like this.\n\n\nDownload the pandoc-X.XX.X.X-macOS.pkg file by clicking on the file name. This is the installer file.\n\nNote: X.XX.X.X refers to the version number. It is 2.17.1.1 in the image above.\n\nRun the installer by double-clicking on the downloaded file.\n\nNote: A new window should pop up. This is what it should look like.\n\n\nPress Continue to continue.\nFollow the directions given by the Installer to complete your installation.\n\nNote: This program automatically installs Pandoc for you.\n\nLastly, to check if Pandoc was installed, type “pandoc -v” in your terminal. If Pandoc was installed correctly, you should see that the next line in the terminal is: “pandoc.exe X.XX.X.X” where the X’s represent the version number. If an error occurs, return to step 1 and try again.\n\nNote: You can access the terminal on a Mac by opening the Launchpad, typing Terminal in the search field, and clicking Terminal.\n\n\n\n\n\n\nGo to this webpage.\n\nNote: The webpage should look like this.\n\n\nSelect the Mac tab.\nPress the Download button to download the disk image (.dmg) file.\nDouble-click on the downloaded disk image (.dmg) file.\n\nNote: A new window should pop up. This is what it should look like.\n\n\nDrag the MiKTeX icon onto the Applications folder icon.\n\nNote: This will install the MiKTeX Console application and all required files.\n\nRun the MiKTeX Console application from Launchpad.\n\nNote: A new window should pop up. This is what it should look like.\n\n\nSelect the Finish private setup option.\nMiKTeX has been installed!\n\n\n\n\n\n\nTo easily convert files we’ll be using a writing and publishing environment. Visual Studio Code is a popular coding editor and we’ll be using it with Pandoc to convert files. These steps will walk you through downloading, installing, and setting up Visual Studio Code.\n\n\n\nComputer running either Windows or Mac\n\n\n\n\n\nGo to the Visual Studio Code website.\nDownload Visual Studio code for your machine.\n\nInstall the package.\nOpen the program.\nNavigate to the extensions tab on the left of the screen.\n\nInstall any useful extensions you might need.\nNote helpful extensions for markdown are markdownlint and Markdown All in One.\nCustomize with themes (Optional).\nOpen the folder your files will be in or create a new folder.\nAdd a new markdown file.\nAdd content.\n\n\n\n\n\nWhen writing documents for different occasions, it is often necessary to know how to convert a document from one form to another. While some professors may require PDF files, others may require Markdown files. So, with the help of Pandoc converting documents becomes a simple task.\nPandoc is a tool that can be installed on your computer to convert files from one format to another. Pandoc works with formats such as HTML, ebooks, documentation formats, bibliography formats and many more.\nOne example of how students have used Pandoc in the past was for a project. This project required converting a Microsoft Word file (.docx) to a Markdown file (.md). After downloading and checking for installation, by typing the correct conversion code into the computer terminal, Pandoc converted the students’ files from Microsoft Word to Markdown.\nSome benefits of Pandoc include:\n\nIt can convert formats from and to a a variety of different formats\nIts ability to convert citations into a properly formatted citation\nThe option to customize using a template system and writing filters\nWorks with slide show formats as well as text and data formats\n\n\n\n\nThis is a general tutorial for writing in Markdown syntax. It will detail the use of basic Markdown syntax elements. Additionally, since Pandoc understands an extended version of Markdown syntax, some useful extensions of traditional Markdown syntax will be sectioned at the end for use with Pandoc.\n\n\nHeadings are created by beginning a line with hashtags (#). The heading name must be separated from the hashtags by a blank space. The number of hashtags represents the level of the heading. For instance, a level one heading would be written as “# Level 1 Heading”.\n\n\n\nHeading Level\nMarkdown Syntax\n\n\n\n\nLevel 1\n# Level 1 Heading\n\n\nLevel 2\n## Level 2 Heading\n\n\nLevel 3\n### Level 3 Heading\n\n\nLevel 4\n#### Level 4 Heading\n\n\nLevel 5\n##### Level 5 Heading\n\n\nLevel 6\n###### Level 6 Heading\n\n\n\n\nThe table above illustrates syntax for various level headings. Note that markdown only supports six levels of headings.\n\n\n\n\n\nParagraphs can be created by using a blank line to separate blocks of text.\nFor instance, these two paragraphs have been separated by a blank space and thus will be rendered as such.\n\nNote: If you must indent a paragraph use “&nbsp;&nbsp;&nbsp;&nbsp;”. This bit of text will add 4 empty spaces.\n\n     This sentence has been indented using this technique.\n\n\n\n\nLine breaks can be used by adding two or more blank spaces at the end of a line and then pressing return.\nThis line has been separated with a line break.\n\n\n\n\nTo bold text, surround it with two asterisks. To italicize text, surround it with one asterisk. To both bold and italicize a piece of text, surround it with three asterisks.\n\n\n\n\n\n\n\nMarkdown Syntax\nOutput\n\n\n\n\n*This* is italicized.\nThis is italicized.\n\n\n**This** is in bold.\nThis is in bold.\n\n\n***This*** is in bold and italicized.\nThis is in bold and italicized.\n\n\n\n\nThe table above illustrates various ways to emphasize text.\n\n\n\n\n\nTo create block quotes, begin paragraph with &gt;.\nFor example, in markdown\n&gt; This is a block quote.\nFor a block quotes with blank line, begin the blank line with &gt;. Also, block quotes can be nested by adding &gt;&gt; in front of the nested quote. See the example below.\n&gt; This is a block quote\n&gt;\n&gt; with a blank line\n&gt;&gt; and a nested quote.\n\n\n\n\nBoth unordered lists and ordered lists can be created in Markdown. Lists can also be indented to create nested lists.\n\n\nIn Markdown, to create an ordered list, you must begin each list element with a number and a period. The numbering must begin with number one, but the order following number one does not matter.\nAn ordered list in Markdown syntax is as follows:\n1. One\n2. Two\n3. Three\nOR\n1. One\n1. Two\n1. Three\nOR\n1. One\n75. Two\n35. Three\nWhen rendered, the lists all look like this:\n\nOne\nTwo\nThree\n\n\n\n\nUnordered lists can be created by adding dashes (-), asterisks (*), or plus signs (+) before each list element.\nAn unordered list in Markdown syntax is as follows:\n- One\n- Two\n- Three\nOR\n* One\n* Two\n* Three\nOR\n+ One\n+ Two\n+ Three\nWhen rendered, the lists all look like this:\n\nOne\nTwo\nThree\n\nHere is an example of a nested list.\nMarkdown Syntax:\n1. One\n2. Two\n    - Indented One\n    - Indented Two\n3. Three\nOutput:\n\nOne\nTwo\n\nIndented One\nIndented Two\n\nThree\n\n\n\n\n\n\nTo escape any Markdown rendering for bits of code, surround your text with backticks (`).\nFor example: *This* will not be in italics since I surrounded it with backticks.\nYou can also use backslash (\\) to escape other characters as:\n\nbackslash \\\nbacktick `\nasterisk *\nunderscore _\ncurly braces { }\nbrackets [ ]\nangle brackets &lt; &gt;\nparentheses ( )\nhashtag #\nplus sign +\ndash -\nperiod .\nexclamation mark !\npipe |\n\nMarkdown Syntax: \\# Without a backslash, this would be a heading.\nOutput: # Without a backslash, this would be a heading.\n\n\n\n\nIn order to insert a horizontal rule in Markdown, you must use three or more asterisks, dashes, or underscores on a line, like so:\n***\n---\n___\n\nA horizontal rule is below this.\n\n\n\n\n\nFor links, you can enclose link text in brackets and follow it with the URL in parentheses.\nFor instance: [Google](google.com)\nwill look like: Google\n\nYou can also link to other files by replacing the link address with a file name. Make sure the file is accessible in your current working directory.\n\n\n\n\n\nFor images, you can add an exclamation mark (!), followed by image alt text enclosed by brackets, and then followed by the image file path or URL enclosed by parentheses. You can also add a title to the image with quotations marks besides your path or URL. See below for an example.\nMarkdown Syntax:\n![Markdown Logo](/media/markdown.jpg \"Markdown\")\nOutput: \nFinally, you can link images to a URL by adding the link address enclosed by parentheses immediately following the image. You must also surround the image code with brackets. See the following example.\nMarkdown Syntax:\n[![Markdown Logo](/media/markdown.jpg \"Markdown\")](https://www.markdownguide.org/getting-started/)\nOutput: \n\n\n\n\nYou can strikethrough text by surrounding the text with two tildes.\nFor example: ~~This~~ is using strikethrough.\nOutput: This is using strikethrough.\n\n\n\n\nTo create a table, use three or more hyphens to create the column headers, and use pipes to create each column. See below for an example.\nMarkdown Syntax:\n| Header 1 | Header 2 |\n| -------- | -------- |\n| Element1 | Element2 |\n| Element3 | Element4 |\nOutput:\n\n\n\nHeader 1\nHeader 2\n\n\n\n\nElement1\nElement2\n\n\nElement3\nElement4\n\n\n\n\n\n\n\nPandoc understands an extended version of Markdown, so here are some useful extensions of the syntax which will work with Pandoc.\n\n\nYou can create headings in setext-style by following a heading with a line of equal signs (for level 1 headings) or dashes (for level 2 headings).\nFor example:\nThis is a level 1 heading\n=========================\nThis is a level 2 heading\n-------------------------\n\n\n\nPandoc allows for definition lists with the following syntax.\nTerm 1\n\n:   Definition 1\n\nTerm 2\n\n:   Definition 2\n\n    Second paragraph of definition 2.\n\n\n\nPandoc also supports multiline tables. Table rows may span multiple lines of text. See below for an example on how you may format one.\n-------------------------------------------------------------\n Centered   Default           Right Left\n  Header    Aligned         Aligned Aligned\n----------- ------- --------------- -------------------------\nFirst       row                12.0 Example of a row that\n                                    spans multiple lines.\n\nSecond      row                5.0 Here's another one. Note\n                                    the blank line between\n                                    rows.\n-------------------------------------------------------------\n\nTable: Here's the caption. It, too, may span\nmultiple lines.\n\n\n\nGrid Tables can also be created with Pandoc. A row of equal signs must separate the headers, and plus signs must indicate corners of cells. See below for an example.\n: Sample grid table.\n\n+---------------+---------------+--------------------+\n| Fruit         | Price         | Advantages         |\n+===============+===============+====================+\n| Bananas       | $1.34         | - built-in wrapper |\n|               |               | - bright color     |\n+---------------+---------------+--------------------+\n| Oranges       | $2.10         | - cures scurvy     |\n|               |               | - tasty            |\n+---------------+---------------+--------------------+\nCell elements may contain multiple paragraphs, code blocks, lists, etc. However, cannot span multiple columns or rows.\n\n\n\nSuperscripts can be created by surrounding the superscript with carets (^). Subscripts can be creating by surounding the subscript with tildes (~).\nFor example: H~2~O is a liquid. 2^10^ is 1024.\n\n\n\nAny text surrounded by two dollar signs ($) will be treated as TeX math. The output will depend on the output format. There must not be a whitespace after the first dollar sign or before the second one.\nExample: $2+2=4$\nFor example, in Docx format, the text will be rendered using OMML math markup.\n\n\n\n\n\nFor more information about Markdown syntax, a good resource is the official Markdown Guide.\nFor more information about Pandoc’s extended syntax, a good resource is the official Pandoc Manual.\n\n\n\n\nThis section is a comprehensive guide on how to convert Markdown files to EPUB, HTML, Word, and PDF files. At the end of this process you will have the knowledge necessary to effortlessly make this conversion in the future.\n\n\nThe following are assumed to have been installed/obtained before beginning the guide\n\nPandoc\nMarkdown File\npdfLatex FOR PDF PUBLISHES ONLY\n\n\n\n\n\nOpen your File Explorer and locate the Markdown file that you would like to convert. Your screen should look similar to the image below (where the “convertMe” file is YOUR file).\n\nClick on the Path of your current directory in your file explorer. This can be located directly above the files shown, and is shown highlighted in the image below.\n\nType cmd over your highlighted path. Once it is highlighted from the previous step, all you must do is begin typing. Once this is complete, your screen should look similar to the image below.\n\nPress enter. This will open the command prompt at the directory that you were in during the File Explorer session. Once this is complete, your screen should look similar to the image below.\n\nType the appropriate command into the command prompt. The generalized formula for your command is the following\npandoc -s YOURFILE.md -o YOUROUTPUT.FILETYPE\nThe YOURFILE.md should be renamed based upon the name of the Markdown file you are attempting to convert. In our examples below, this file is named “convertMe”.\nThe YOUROUTPUT.FILETYPE should be renamed based upon the desired name of your output file and the type of file you are converting to. In our examples below, the file is named “convertedFile” and is being converted to Word.\nFor EPUB\npandoc -s convertMe.md -o convertedFile.epub\nFor HTML\npandoc -s convertMe.md -o convertedFile.html\nFor Word\npandoc -s convertMe.md -o convertedFile.docx\nFor PDF must have pdfLatex installed\npandoc -s convertMe.md -o convertedFile.pdf\nOnce this is complete, your screen’s command should look similar to the image below.\n\nPress enter. This will make Pandoc execute the command. The output file will now be present in the folder of your original Markdown file.\n\nIf you are receiving an error upon hitting enter you have made some small mistake in the previous steps. Carefully retrace your steps and be certain you are typing everything exactly as expected into the command prompt.\n\nOnce this is complete, your screen should look similar to the image below.\n\n\n\n\n\nYou have successfully converted your file."
  },
  {
    "objectID": "projects/pandoc-guide/index.html#installing-pandoc",
    "href": "projects/pandoc-guide/index.html#installing-pandoc",
    "title": "Pandoc Guide",
    "section": "",
    "text": "This task includes instructions on how to download and install Pandoc on your computer. There are two sections. One is for Windows users the other is for macOS users. There are also optional steps in each section to install a LaTeX program. Installing a LaTeX program is highly recommended, as Pandoc creates PDFs using LaTeX. The LaTeX program we will be installing is MiKTeX.\n\n\nThis task includes instructions on how to download and install Pandoc for Windows users. There are also optional instructions for installing MiKTeX.\n\n\n\nComputer\nWindows Operating System\nInternet Access\n\n\n\n\n\nGo to this webpage.\n\nNote: The webpage should look like this.\n\n\nDownload the pandoc-X.XX.X.X-windows-x86.msi file by clicking on the file name. This is the installer file.\n\nNote: X.XX.X.X refers to the version number. It is 2.17.1.1 in the image above.\n\nRun the installer by double-clicking on the downloaded file.\n\nNote: A new window should pop up. This is what it should look like.\n\n\nPress Next to continue.\nFollow the directions given by the Setup Wizard to complete your installation.\n\nNote: This program automatically installs Pandoc for you.\n\nLastly, to check if Pandoc was installed, type “pandoc -v” in your terminal. If Pandoc was installed correctly, you should see that the next line in the terminal is: “pandoc.exe X.XX.X.X” where the X’s represent the version number. If an error occurs, return to step 1 and try again. &gt; Note: You can access the Windows PowerShell terminal with Windows key + X. Select Windows PowerShell from the menu that pops up. (See image below for reference).\n\n\n\n\n\n\nGo to this webpage.\n\nNote: The webpage should look like this.\n\n\nUnder the Windows tab, select the Installer tab.\nPress the Download button to download the installer.\nRun the installer by double-clicking on the downloaded file.\n\nNote: A new window should pop up. This is what it should look like.\n\n\nRead the conditions and check the “I accept the MiKTeX copying conditions.” option.\nPress Next to continue.\nFollow the instructions in the installer to complete the installation.\nOnce you have reached this page in the installer, you are done. MiKTeX has been installed!\n\n\n\n\n\n\nThis task includes instructions on how to download and install Pandoc for macOS users. There are also optional instructions for installing MiKTeX.\n\n\n\nComputer\nmacOS Operating System\nInternet Access\n\n\n\n\n\nGo to this webpage.\n\nNote: The webpage should look like this.\n\n\nDownload the pandoc-X.XX.X.X-macOS.pkg file by clicking on the file name. This is the installer file.\n\nNote: X.XX.X.X refers to the version number. It is 2.17.1.1 in the image above.\n\nRun the installer by double-clicking on the downloaded file.\n\nNote: A new window should pop up. This is what it should look like.\n\n\nPress Continue to continue.\nFollow the directions given by the Installer to complete your installation.\n\nNote: This program automatically installs Pandoc for you.\n\nLastly, to check if Pandoc was installed, type “pandoc -v” in your terminal. If Pandoc was installed correctly, you should see that the next line in the terminal is: “pandoc.exe X.XX.X.X” where the X’s represent the version number. If an error occurs, return to step 1 and try again.\n\nNote: You can access the terminal on a Mac by opening the Launchpad, typing Terminal in the search field, and clicking Terminal.\n\n\n\n\n\n\nGo to this webpage.\n\nNote: The webpage should look like this.\n\n\nSelect the Mac tab.\nPress the Download button to download the disk image (.dmg) file.\nDouble-click on the downloaded disk image (.dmg) file.\n\nNote: A new window should pop up. This is what it should look like.\n\n\nDrag the MiKTeX icon onto the Applications folder icon.\n\nNote: This will install the MiKTeX Console application and all required files.\n\nRun the MiKTeX Console application from Launchpad.\n\nNote: A new window should pop up. This is what it should look like.\n\n\nSelect the Finish private setup option.\nMiKTeX has been installed!"
  },
  {
    "objectID": "projects/pandoc-guide/index.html#setting-up-a-writing-and-publishing-environment",
    "href": "projects/pandoc-guide/index.html#setting-up-a-writing-and-publishing-environment",
    "title": "Pandoc Guide",
    "section": "",
    "text": "To easily convert files we’ll be using a writing and publishing environment. Visual Studio Code is a popular coding editor and we’ll be using it with Pandoc to convert files. These steps will walk you through downloading, installing, and setting up Visual Studio Code.\n\n\n\nComputer running either Windows or Mac\n\n\n\n\n\nGo to the Visual Studio Code website.\nDownload Visual Studio code for your machine.\n\nInstall the package.\nOpen the program.\nNavigate to the extensions tab on the left of the screen.\n\nInstall any useful extensions you might need.\nNote helpful extensions for markdown are markdownlint and Markdown All in One.\nCustomize with themes (Optional).\nOpen the folder your files will be in or create a new folder.\nAdd a new markdown file.\nAdd content."
  },
  {
    "objectID": "projects/pandoc-guide/index.html#what-is-pandoc",
    "href": "projects/pandoc-guide/index.html#what-is-pandoc",
    "title": "Pandoc Guide",
    "section": "",
    "text": "When writing documents for different occasions, it is often necessary to know how to convert a document from one form to another. While some professors may require PDF files, others may require Markdown files. So, with the help of Pandoc converting documents becomes a simple task.\nPandoc is a tool that can be installed on your computer to convert files from one format to another. Pandoc works with formats such as HTML, ebooks, documentation formats, bibliography formats and many more.\nOne example of how students have used Pandoc in the past was for a project. This project required converting a Microsoft Word file (.docx) to a Markdown file (.md). After downloading and checking for installation, by typing the correct conversion code into the computer terminal, Pandoc converted the students’ files from Microsoft Word to Markdown.\nSome benefits of Pandoc include:\n\nIt can convert formats from and to a a variety of different formats\nIts ability to convert citations into a properly formatted citation\nThe option to customize using a template system and writing filters\nWorks with slide show formats as well as text and data formats"
  },
  {
    "objectID": "projects/pandoc-guide/index.html#how-to-write-in-markdown",
    "href": "projects/pandoc-guide/index.html#how-to-write-in-markdown",
    "title": "Pandoc Guide",
    "section": "",
    "text": "This is a general tutorial for writing in Markdown syntax. It will detail the use of basic Markdown syntax elements. Additionally, since Pandoc understands an extended version of Markdown syntax, some useful extensions of traditional Markdown syntax will be sectioned at the end for use with Pandoc.\n\n\nHeadings are created by beginning a line with hashtags (#). The heading name must be separated from the hashtags by a blank space. The number of hashtags represents the level of the heading. For instance, a level one heading would be written as “# Level 1 Heading”.\n\n\n\nHeading Level\nMarkdown Syntax\n\n\n\n\nLevel 1\n# Level 1 Heading\n\n\nLevel 2\n## Level 2 Heading\n\n\nLevel 3\n### Level 3 Heading\n\n\nLevel 4\n#### Level 4 Heading\n\n\nLevel 5\n##### Level 5 Heading\n\n\nLevel 6\n###### Level 6 Heading\n\n\n\n\nThe table above illustrates syntax for various level headings. Note that markdown only supports six levels of headings.\n\n\n\n\n\nParagraphs can be created by using a blank line to separate blocks of text.\nFor instance, these two paragraphs have been separated by a blank space and thus will be rendered as such.\n\nNote: If you must indent a paragraph use “&nbsp;&nbsp;&nbsp;&nbsp;”. This bit of text will add 4 empty spaces.\n\n     This sentence has been indented using this technique.\n\n\n\n\nLine breaks can be used by adding two or more blank spaces at the end of a line and then pressing return.\nThis line has been separated with a line break.\n\n\n\n\nTo bold text, surround it with two asterisks. To italicize text, surround it with one asterisk. To both bold and italicize a piece of text, surround it with three asterisks.\n\n\n\n\n\n\n\nMarkdown Syntax\nOutput\n\n\n\n\n*This* is italicized.\nThis is italicized.\n\n\n**This** is in bold.\nThis is in bold.\n\n\n***This*** is in bold and italicized.\nThis is in bold and italicized.\n\n\n\n\nThe table above illustrates various ways to emphasize text.\n\n\n\n\n\nTo create block quotes, begin paragraph with &gt;.\nFor example, in markdown\n&gt; This is a block quote.\nFor a block quotes with blank line, begin the blank line with &gt;. Also, block quotes can be nested by adding &gt;&gt; in front of the nested quote. See the example below.\n&gt; This is a block quote\n&gt;\n&gt; with a blank line\n&gt;&gt; and a nested quote.\n\n\n\n\nBoth unordered lists and ordered lists can be created in Markdown. Lists can also be indented to create nested lists.\n\n\nIn Markdown, to create an ordered list, you must begin each list element with a number and a period. The numbering must begin with number one, but the order following number one does not matter.\nAn ordered list in Markdown syntax is as follows:\n1. One\n2. Two\n3. Three\nOR\n1. One\n1. Two\n1. Three\nOR\n1. One\n75. Two\n35. Three\nWhen rendered, the lists all look like this:\n\nOne\nTwo\nThree\n\n\n\n\nUnordered lists can be created by adding dashes (-), asterisks (*), or plus signs (+) before each list element.\nAn unordered list in Markdown syntax is as follows:\n- One\n- Two\n- Three\nOR\n* One\n* Two\n* Three\nOR\n+ One\n+ Two\n+ Three\nWhen rendered, the lists all look like this:\n\nOne\nTwo\nThree\n\nHere is an example of a nested list.\nMarkdown Syntax:\n1. One\n2. Two\n    - Indented One\n    - Indented Two\n3. Three\nOutput:\n\nOne\nTwo\n\nIndented One\nIndented Two\n\nThree\n\n\n\n\n\n\nTo escape any Markdown rendering for bits of code, surround your text with backticks (`).\nFor example: *This* will not be in italics since I surrounded it with backticks.\nYou can also use backslash (\\) to escape other characters as:\n\nbackslash \\\nbacktick `\nasterisk *\nunderscore _\ncurly braces { }\nbrackets [ ]\nangle brackets &lt; &gt;\nparentheses ( )\nhashtag #\nplus sign +\ndash -\nperiod .\nexclamation mark !\npipe |\n\nMarkdown Syntax: \\# Without a backslash, this would be a heading.\nOutput: # Without a backslash, this would be a heading.\n\n\n\n\nIn order to insert a horizontal rule in Markdown, you must use three or more asterisks, dashes, or underscores on a line, like so:\n***\n---\n___\n\nA horizontal rule is below this.\n\n\n\n\n\nFor links, you can enclose link text in brackets and follow it with the URL in parentheses.\nFor instance: [Google](google.com)\nwill look like: Google\n\nYou can also link to other files by replacing the link address with a file name. Make sure the file is accessible in your current working directory.\n\n\n\n\n\nFor images, you can add an exclamation mark (!), followed by image alt text enclosed by brackets, and then followed by the image file path or URL enclosed by parentheses. You can also add a title to the image with quotations marks besides your path or URL. See below for an example.\nMarkdown Syntax:\n![Markdown Logo](/media/markdown.jpg \"Markdown\")\nOutput: \nFinally, you can link images to a URL by adding the link address enclosed by parentheses immediately following the image. You must also surround the image code with brackets. See the following example.\nMarkdown Syntax:\n[![Markdown Logo](/media/markdown.jpg \"Markdown\")](https://www.markdownguide.org/getting-started/)\nOutput: \n\n\n\n\nYou can strikethrough text by surrounding the text with two tildes.\nFor example: ~~This~~ is using strikethrough.\nOutput: This is using strikethrough.\n\n\n\n\nTo create a table, use three or more hyphens to create the column headers, and use pipes to create each column. See below for an example.\nMarkdown Syntax:\n| Header 1 | Header 2 |\n| -------- | -------- |\n| Element1 | Element2 |\n| Element3 | Element4 |\nOutput:\n\n\n\nHeader 1\nHeader 2\n\n\n\n\nElement1\nElement2\n\n\nElement3\nElement4\n\n\n\n\n\n\n\nPandoc understands an extended version of Markdown, so here are some useful extensions of the syntax which will work with Pandoc.\n\n\nYou can create headings in setext-style by following a heading with a line of equal signs (for level 1 headings) or dashes (for level 2 headings).\nFor example:\nThis is a level 1 heading\n=========================\nThis is a level 2 heading\n-------------------------\n\n\n\nPandoc allows for definition lists with the following syntax.\nTerm 1\n\n:   Definition 1\n\nTerm 2\n\n:   Definition 2\n\n    Second paragraph of definition 2.\n\n\n\nPandoc also supports multiline tables. Table rows may span multiple lines of text. See below for an example on how you may format one.\n-------------------------------------------------------------\n Centered   Default           Right Left\n  Header    Aligned         Aligned Aligned\n----------- ------- --------------- -------------------------\nFirst       row                12.0 Example of a row that\n                                    spans multiple lines.\n\nSecond      row                5.0 Here's another one. Note\n                                    the blank line between\n                                    rows.\n-------------------------------------------------------------\n\nTable: Here's the caption. It, too, may span\nmultiple lines.\n\n\n\nGrid Tables can also be created with Pandoc. A row of equal signs must separate the headers, and plus signs must indicate corners of cells. See below for an example.\n: Sample grid table.\n\n+---------------+---------------+--------------------+\n| Fruit         | Price         | Advantages         |\n+===============+===============+====================+\n| Bananas       | $1.34         | - built-in wrapper |\n|               |               | - bright color     |\n+---------------+---------------+--------------------+\n| Oranges       | $2.10         | - cures scurvy     |\n|               |               | - tasty            |\n+---------------+---------------+--------------------+\nCell elements may contain multiple paragraphs, code blocks, lists, etc. However, cannot span multiple columns or rows.\n\n\n\nSuperscripts can be created by surrounding the superscript with carets (^). Subscripts can be creating by surounding the subscript with tildes (~).\nFor example: H~2~O is a liquid. 2^10^ is 1024.\n\n\n\nAny text surrounded by two dollar signs ($) will be treated as TeX math. The output will depend on the output format. There must not be a whitespace after the first dollar sign or before the second one.\nExample: $2+2=4$\nFor example, in Docx format, the text will be rendered using OMML math markup.\n\n\n\n\n\nFor more information about Markdown syntax, a good resource is the official Markdown Guide.\nFor more information about Pandoc’s extended syntax, a good resource is the official Pandoc Manual."
  },
  {
    "objectID": "projects/pandoc-guide/index.html#how-to-publish-to-epub-html-word-and-pdf",
    "href": "projects/pandoc-guide/index.html#how-to-publish-to-epub-html-word-and-pdf",
    "title": "Pandoc Guide",
    "section": "",
    "text": "This section is a comprehensive guide on how to convert Markdown files to EPUB, HTML, Word, and PDF files. At the end of this process you will have the knowledge necessary to effortlessly make this conversion in the future.\n\n\nThe following are assumed to have been installed/obtained before beginning the guide\n\nPandoc\nMarkdown File\npdfLatex FOR PDF PUBLISHES ONLY\n\n\n\n\n\nOpen your File Explorer and locate the Markdown file that you would like to convert. Your screen should look similar to the image below (where the “convertMe” file is YOUR file).\n\nClick on the Path of your current directory in your file explorer. This can be located directly above the files shown, and is shown highlighted in the image below.\n\nType cmd over your highlighted path. Once it is highlighted from the previous step, all you must do is begin typing. Once this is complete, your screen should look similar to the image below.\n\nPress enter. This will open the command prompt at the directory that you were in during the File Explorer session. Once this is complete, your screen should look similar to the image below.\n\nType the appropriate command into the command prompt. The generalized formula for your command is the following\npandoc -s YOURFILE.md -o YOUROUTPUT.FILETYPE\nThe YOURFILE.md should be renamed based upon the name of the Markdown file you are attempting to convert. In our examples below, this file is named “convertMe”.\nThe YOUROUTPUT.FILETYPE should be renamed based upon the desired name of your output file and the type of file you are converting to. In our examples below, the file is named “convertedFile” and is being converted to Word.\nFor EPUB\npandoc -s convertMe.md -o convertedFile.epub\nFor HTML\npandoc -s convertMe.md -o convertedFile.html\nFor Word\npandoc -s convertMe.md -o convertedFile.docx\nFor PDF must have pdfLatex installed\npandoc -s convertMe.md -o convertedFile.pdf\nOnce this is complete, your screen’s command should look similar to the image below.\n\nPress enter. This will make Pandoc execute the command. The output file will now be present in the folder of your original Markdown file.\n\nIf you are receiving an error upon hitting enter you have made some small mistake in the previous steps. Carefully retrace your steps and be certain you are typing everything exactly as expected into the command prompt.\n\nOnce this is complete, your screen should look similar to the image below.\n\n\n\n\n\nYou have successfully converted your file."
  },
  {
    "objectID": "projects/welcome/index.html",
    "href": "projects/welcome/index.html",
    "title": "Welcome To My Blog",
    "section": "",
    "text": "This is the first post in a Quarto blog. Welcome!\n\nSince this post doesn’t specify an explicit image, the first image in the post will be used in the listing page of posts.\nI have now added to this too."
  }
]